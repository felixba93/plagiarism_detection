{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from xml.etree.ElementTree import *\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "import os\n",
    "import pprint\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import LdaMulticore\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from smart_open import open \n",
    "import spacy\n",
    "import de_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the XML-file\n",
    "xml_file = \"../Textmining/dewiki-20200920-pages-articles-multistream.xml\"\n",
    "\n",
    "# number of documents to parse \n",
    "num_documents = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the language model from spacy\n",
    "spacy_data = de_core_news_md.load()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # load and tokenize text with the spacy language model\n",
    "    prep_text = spacy_data(text)\n",
    "    # list for tokens\n",
    "    prep_tokens = []\n",
    "    # for every token in text\n",
    "    for token in prep_text:\n",
    "        # remove stopwords and punctuatiuon\n",
    "        if token.pos_ != 'PUNCT' and token.is_stop == False:\n",
    "            # lemmatize and transform to lowercase\n",
    "            lemma_token = token.lemma_.lower()\n",
    "            # remove non-alphabetic tokens\n",
    "            if lemma_token.isalpha() or lemma_token == '-PRON-':\n",
    "                prep_tokens.append(lemma_token)\n",
    "    # return preprocessed text \n",
    "    return prep_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the corpus as an object\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        # define the XML tree\n",
    "        for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):            \n",
    "            # Each document is represented as an object between <text> tags in the xml file\n",
    "            if event == 'end' and \"text\" in elem.tag:\n",
    "                # Transfom the corpus to vectors\n",
    "                yield dictionary.doc2bow(preprocess_text(elem.text))\n",
    "                # clear the node\n",
    "                elem.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a smaller corpus, containing only the first i documents:\n",
    "class MyCorpus_small:\n",
    "    def __iter__(self):\n",
    "        index = 0\n",
    "        # define the XML tree\n",
    "        for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):\n",
    "            if index < num_documents:\n",
    "                # Each document is represented as an object between <text> tags in the xml file\n",
    "                if event == 'end' and \"text\" in elem.tag:\n",
    "                    # Transfom the corpus to vectors\n",
    "                    yield dictionary.doc2bow(preprocess_text(elem.text))\n",
    "                    index+=1\n",
    "                    # clear the node\n",
    "                    elem.clear()\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = MyCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_small = MyCorpus_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(xml_file):\n",
    "    index = 0\n",
    "    first_elem = True\n",
    "    # loop through all nodes\n",
    "    for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):        \n",
    "        if index < num_documents:\n",
    "            # check if current node contains a document\n",
    "            if event == \"end\" and \"text\" in elem.tag:\n",
    "                # preprocess the text\n",
    "                text = preprocess_text(elem.text)\n",
    "                # if this is the first document found, create a new dictionary with it\n",
    "                if first_elem:\n",
    "                    dictionary = Dictionary([text])\n",
    "                    first_elem = False\n",
    "                    index += 1\n",
    "                # all documents after the first one get appended to the dictionary\n",
    "                else:\n",
    "                    dictionary.add_documents([text])\n",
    "                    index += 1\n",
    "                # clear the node\n",
    "                elem.clear()\n",
    "        else:\n",
    "            break\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nicht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 48s, sys: 13.2 s, total: 2min 1s\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build the dictionary:\n",
    "dictionary = build_dictionary(xml_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.4 ms, sys: 5.63 ms, total: 63 ms\n",
      "Wall time: 62.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# remove words that appear only once\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "dictionary.filter_tokens(once_ids)\n",
    "# remove gaps in id sequence after words that were removed\n",
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dictionary\n",
    "dictionary.save('../Textmining/wiki_200_new.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dictionary\n",
    "dictionary = Dictionary.load('../Textmining/wiki_200_new.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(20416 unique tokens: ['abc', 'abkehr', 'ablehnen', 'abrufen', 'abschluss']...)\n"
     ]
    }
   ],
   "source": [
    "# check if the dictionary has been loaded \n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 43s, sys: 27.9 s, total: 4min 11s\n",
      "Wall time: 4min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda = LdaMulticore(corpus_small, num_topics=200, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the trained model\n",
    "lda.save(\"../Textmining/lda_model_200.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the trained model\n",
    "lda = LdaModel.load(\"../Textmining/lda_model_200.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 11s, sys: 13.6 s, total: 4min 25s\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus_index = similarities.MatrixSimilarity(list(lda[corpus_small]), num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ab hier csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define document to use in similarity check\n",
    "#test_document = texts[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the document to vector space\n",
    "#test_vec = dictionary.doc2bow(preprocess_text(test_document))\n",
    "# convert to lda space\n",
    "#test_vec_lda = lda[test_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the similarities\n",
    "#sims = corpus_index[test_vec_lda]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pd.read_csv(\"../Textmining/beispieltext.csv\",sep=\";\") #test_document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv_file = pd.read_csv(\"../Textmining/beispieltext.csv\",sep=\";\") #test_document  xml_file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv_file[['Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_document = pd.DataFrame(csv_file)\n",
    "#test_document['Text'] = test_document['Text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_document = csv_file[['Text']]\n",
    "#print(test_document.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_document.Text.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_document[['Text']] = test_document[['Text']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%timeit test_document.astype(str) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document = open('wikibeispiele.txt', encoding='utf-8')\n",
    "#test_document.readlines()\n",
    "test_document = test_document.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the document to vector space\n",
    "test_vec = dictionary.doc2bow(preprocess_text(test_document)) #test_document\n",
    "# convert to lda space\n",
    "test_vec_lda = lda[test_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the similarities\n",
    "sims = corpus_index[test_vec_lda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Übereinstimmung von  83.88 % \n",
      " Document ID: 34   \n",
      " ------------------------------------\n",
      "Übereinstimmung von  78.72 % \n",
      " Document ID: 45 r \n",
      " ------------------------------------\n",
      "Übereinstimmung von  76.87 % \n",
      " Document ID: 85   \n",
      " ------------------------------------\n",
      "Übereinstimmung von  83.75 % \n",
      " Document ID: 97   \n",
      " ------------------------------------\n",
      "Übereinstimmung von  90.90 % \n",
      " Document ID: 123 . \n",
      " ------------------------------------\n",
      "Übereinstimmung von  83.91 % \n",
      " Document ID: 195 p \n",
      " ------------------------------------\n",
      "6 Plagiatsfälle gefunden\n"
     ]
    }
   ],
   "source": [
    "hits = 0\n",
    "for ids in list(enumerate(sims)):\n",
    "    if ids[1] >= 0.75:\n",
    "        hits += 1\n",
    "        print(\"Übereinstimmung von \",\"%.2f\" %(ids[1]*100),\"%\",\"\\n\",\"Document ID:\",ids[0],test_document[ids[0]],\"\\n\", \"------------------------------------\")\n",
    "print(hits, \"Plagiatsfälle gefunden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
