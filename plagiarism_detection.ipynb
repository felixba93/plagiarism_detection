{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plagiarism Detection Notebook\n",
    "## Notebook for the \"Textmining\" project in WS2020/2021\n",
    "\n",
    "Sources used for code: \n",
    "\n",
    "* https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html\n",
    "\n",
    "* https://radimrehurek.com/gensim/auto_examples/core/run_corpora_and_vector_spaces.html#corpus-streaming-tutorial\n",
    "\n",
    "* https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#12buildingthetopicmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pprint\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from smart_open import open \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some example documents. For the actual application we wouldn't load everything at once.\n",
    "\n",
    "documents = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Prepocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function for text preprocessing. Converts the text to lower case, removes stopwords and words with a minimum length of 2 and maximum length of 15\n",
    "def preprocessing (corpus):\n",
    "    \n",
    "    processed_corpus = []\n",
    "    \n",
    "    # load stopwords from NLTK\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # go through each document in the corpus\n",
    "    for document in corpus:\n",
    "        \n",
    "        # step1: convert to lowercase and remove words that do not match the min-max-length\n",
    "        step1 = gensim.utils.simple_preprocess(document, deacc=False, min_len=2, max_len=15)\n",
    "        \n",
    "        #step2: remove stopwords\n",
    "        step2 = [word for word in step1 if word not in stop_words]\n",
    "        \n",
    "        processed_corpus.append(step2)\n",
    "    return processed_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'],\n",
      " ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'management', 'system'],\n",
      " ['system', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
      " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
      " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
      " ['intersection', 'graph', 'paths', 'trees'],\n",
      " ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "processed_corpus = preprocessing(documents)\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe add a filter for min occurence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(35 unique tokens: ['abc', 'applications', 'computer', 'human', 'interface']...)\n",
      "{'abc': 0, 'applications': 1, 'computer': 2, 'human': 3, 'interface': 4, 'lab': 5, 'machine': 6, 'opinion': 7, 'response': 8, 'survey': 9, 'system': 10, 'time': 11, 'user': 12, 'eps': 13, 'management': 14, 'engineering': 15, 'testing': 16, 'error': 17, 'measurement': 18, 'perceived': 19, 'relation': 20, 'binary': 21, 'generation': 22, 'random': 23, 'trees': 24, 'unordered': 25, 'graph': 26, 'intersection': 27, 'paths': 28, 'iv': 29, 'minors': 30, 'ordering': 31, 'quasi': 32, 'well': 33, 'widths': 34}\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
      " [(2, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
      " [(4, 1), (10, 1), (12, 1), (13, 1), (14, 1)],\n",
      " [(3, 1), (10, 2), (13, 1), (15, 1), (16, 1)],\n",
      " [(8, 1), (11, 1), (12, 1), (17, 1), (18, 1), (19, 1), (20, 1)],\n",
      " [(21, 1), (22, 1), (23, 1), (24, 1), (25, 1)],\n",
      " [(24, 1), (26, 1), (27, 1), (28, 1)],\n",
      " [(24, 1), (26, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)],\n",
      " [(9, 1), (26, 1), (30, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# convert text to vectors using the dictionary function\n",
    "\n",
    "# define dictionary of our corpus. Contains the word frequency of each token in the whole corpus\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "\n",
    "# transform the corpus to vectors. Each vector consists of a token ID and the token frequency (taken from the dictionary)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "\n",
    "print(dictionary)\n",
    "print(dictionary.token2id)\n",
    "pprint.pprint(bow_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a TF-IDF model of the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# transform the corpus\n",
    "corpus_tfidf = tfidf[bow_corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.4301019571350565),\n",
      " (1, 0.4301019571350565),\n",
      " (2, 0.2944198962221451),\n",
      " (3, 0.2944198962221451),\n",
      " (4, 0.2944198962221451),\n",
      " (5, 0.4301019571350565),\n",
      " (6, 0.4301019571350565)]\n",
      "[(2, 0.3726494271826947),\n",
      " (7, 0.5443832091958983),\n",
      " (8, 0.3726494271826947),\n",
      " (9, 0.3726494271826947),\n",
      " (10, 0.27219160459794917),\n",
      " (11, 0.3726494271826947),\n",
      " (12, 0.27219160459794917)]\n",
      "[(4, 0.438482464916089),\n",
      " (10, 0.32027755044706185),\n",
      " (12, 0.32027755044706185),\n",
      " (13, 0.438482464916089),\n",
      " (14, 0.6405551008941237)]\n",
      "[(3, 0.3449874408519962),\n",
      " (10, 0.5039733231394895),\n",
      " (13, 0.3449874408519962),\n",
      " (15, 0.5039733231394895),\n",
      " (16, 0.5039733231394895)]\n",
      "[(8, 0.30055933182961736),\n",
      " (11, 0.30055933182961736),\n",
      " (12, 0.21953536176370683),\n",
      " (17, 0.43907072352741366),\n",
      " (18, 0.43907072352741366),\n",
      " (19, 0.43907072352741366),\n",
      " (20, 0.43907072352741366)]\n",
      "[(21, 0.48507125007266594),\n",
      " (22, 0.48507125007266594),\n",
      " (23, 0.48507125007266594),\n",
      " (24, 0.24253562503633297),\n",
      " (25, 0.48507125007266594)]\n",
      "[(24, 0.31622776601683794),\n",
      " (26, 0.31622776601683794),\n",
      " (27, 0.6324555320336759),\n",
      " (28, 0.6324555320336759)]\n",
      "[(24, 0.20466057569885868),\n",
      " (26, 0.20466057569885868),\n",
      " (29, 0.40932115139771735),\n",
      " (30, 0.2801947048062438),\n",
      " (31, 0.40932115139771735),\n",
      " (32, 0.40932115139771735),\n",
      " (33, 0.40932115139771735),\n",
      " (34, 0.40932115139771735)]\n",
      "[(9, 0.6282580468670046), (26, 0.45889394536615247), (30, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "# every word is now represented by a vector: Token-ID and token-weight\n",
    "for doc in corpus_tfidf:\n",
    "    pprint.pprint(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an index for the corpus\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0), (1, 0.12172779), (2, 0.14323246), (3, 0.67615116), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "# add a query document\n",
    "query_document = 'system engineering'.split()\n",
    "\n",
    "# transform the query document to a vector\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "\n",
    "# compare the query document to each document in the corpus\n",
    "sims = index[tfidf[query_bow]]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.67615116\n",
      "2 0.14323246\n",
      "1 0.12172779\n",
      "0 0.0\n",
      "4 0.0\n",
      "5 0.0\n",
      "6 0.0\n",
      "7 0.0\n",
      "8 0.0\n"
     ]
    }
   ],
   "source": [
    "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(document_number, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Streaming\n",
    "\n",
    "Since we do not want to load the whole corpus into memory every time, we need some changes in our approach, so we are able to stream the single documents when needed.\n",
    "Code is from: https://radimrehurek.com/gensim/auto_examples/core/run_corpora_and_vector_spaces.html#corpus-streaming-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stopwords from NLTK\n",
    "stop_words = set(stopwords.words('german'))\n",
    "\n",
    "# load all tokens as lowercase text\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in open('data/corpus/deu_mixed-typical_2011_10K/deu_mixed-typical_2011_10K-sentences.txt'))\n",
    "\n",
    "# find all stop words\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stop_words if stopword in dictionary.token2id]\n",
    "\n",
    "# find all words that only occur once\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "\n",
    "# Remove all stopwords and words that only occur once\n",
    "dictionary.filter_tokens(stop_ids + once_ids)\n",
    "\n",
    "# Since we filtered out some words, the ID count now has gaps. With .compactify we can remove those gaps\n",
    "dictionary.compactify()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the corpus as an object\n",
    "\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        # Each line in the corpus file represents one document, each token in a document is seperated by a whitespace\n",
    "        for line in open('data/corpus/deu_mixed-typical_2011_10K/deu_mixed-typical_2011_10K-sentences.txt'):\n",
    "            # Transfom the corpus to vectors\n",
    "            yield dictionary.doc2bow(line.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the corpus, without loading it into memory\n",
    "corpus = MyCorpus()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The corpus now exists only as an object, to work with it, the vectors inside have to be called. Only then they will be loaded into memory.\n",
    "Example: Print the first 10 document vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1)]\n",
      "[(4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)]\n",
      "[(13, 1), (14, 1), (15, 1), (16, 1)]\n",
      "[(7, 1), (17, 1), (18, 1), (19, 1)]\n",
      "[(7, 1), (20, 1), (21, 1)]\n",
      "[(7, 1), (22, 1), (23, 1), (24, 1), (25, 1)]\n",
      "[(26, 1), (27, 1), (28, 1), (29, 1)]\n",
      "[(30, 1), (31, 1), (32, 1), (33, 1)]\n",
      "[(7, 1), (19, 1), (34, 1), (35, 1), (36, 1)]\n"
     ]
    }
   ],
   "source": [
    "for index, vector in enumerate(corpus):  # load one vector into memory at a time\n",
    "    if index <10:\n",
    "        print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with our Corpus\n",
    "\n",
    "Now that we are able to load our corpus memory friendly we can transform the document vectors using a variety of functions.\n",
    "\n",
    "First we have to initialize a model, which will be used for the transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Model:\n",
    "tf_idf_model = models.TfidfModel(corpus, normalize=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
