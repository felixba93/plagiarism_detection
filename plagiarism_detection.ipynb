{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plagiarism Detection Notebook\n",
    "## Notebook for the \"Textmining\" project in WS2020/2021\n",
    "\n",
    "Sources used for code: \n",
    "\n",
    "* https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html\n",
    "\n",
    "* https://radimrehurek.com/gensim/auto_examples/core/run_corpora_and_vector_spaces.html#corpus-streaming-tutorial\n",
    "\n",
    "* https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#12buildingthetopicmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import pprint\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from smart_open import open \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some example documents. For the actual application we wouldn't load everything at once.\n",
    "\n",
    "documents = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Prepocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function for text preprocessing. Converts the text to lower case, removes stopwords and words with a minimum length of 2 and maximum length of 15\n",
    "def preprocessing (corpus):\n",
    "    \n",
    "    processed_corpus = []\n",
    "    \n",
    "    # load stopwords from NLTK\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # go through each document in the corpus\n",
    "    for document in corpus:\n",
    "        \n",
    "        # step1: convert to lowercase and remove words that do not match the min-max-length\n",
    "        step1 = gensim.utils.simple_preprocess(document, deacc=False, min_len=2, max_len=15)\n",
    "        \n",
    "        #step2: remove stopwords\n",
    "        step2 = [word for word in step1 if word not in stop_words]\n",
    "        \n",
    "        processed_corpus.append(step2)\n",
    "    return processed_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'],\n",
      " ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'management', 'system'],\n",
      " ['system', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
      " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
      " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
      " ['intersection', 'graph', 'paths', 'trees'],\n",
      " ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "processed_corpus = preprocessing(documents)\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe add a filter for min occurence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(35 unique tokens: ['abc', 'applications', 'computer', 'human', 'interface']...)\n",
      "{'abc': 0, 'applications': 1, 'computer': 2, 'human': 3, 'interface': 4, 'lab': 5, 'machine': 6, 'opinion': 7, 'response': 8, 'survey': 9, 'system': 10, 'time': 11, 'user': 12, 'eps': 13, 'management': 14, 'engineering': 15, 'testing': 16, 'error': 17, 'measurement': 18, 'perceived': 19, 'relation': 20, 'binary': 21, 'generation': 22, 'random': 23, 'trees': 24, 'unordered': 25, 'graph': 26, 'intersection': 27, 'paths': 28, 'iv': 29, 'minors': 30, 'ordering': 31, 'quasi': 32, 'well': 33, 'widths': 34}\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
      " [(2, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
      " [(4, 1), (10, 1), (12, 1), (13, 1), (14, 1)],\n",
      " [(3, 1), (10, 2), (13, 1), (15, 1), (16, 1)],\n",
      " [(8, 1), (11, 1), (12, 1), (17, 1), (18, 1), (19, 1), (20, 1)],\n",
      " [(21, 1), (22, 1), (23, 1), (24, 1), (25, 1)],\n",
      " [(24, 1), (26, 1), (27, 1), (28, 1)],\n",
      " [(24, 1), (26, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)],\n",
      " [(9, 1), (26, 1), (30, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# convert text to vectors using the dictionary function\n",
    "\n",
    "# define dictionary of our corpus. Contains the word frequency of each token in the whole corpus\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "\n",
    "# transform the corpus to vectors. Each vector consists of a token ID and the token frequency (taken from the dictionary)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "\n",
    "print(dictionary)\n",
    "print(dictionary.token2id)\n",
    "pprint.pprint(bow_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a TF-IDF model of the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# transform the corpus\n",
    "corpus_tfidf = tfidf[bow_corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.4301019571350565),\n",
      " (1, 0.4301019571350565),\n",
      " (2, 0.2944198962221451),\n",
      " (3, 0.2944198962221451),\n",
      " (4, 0.2944198962221451),\n",
      " (5, 0.4301019571350565),\n",
      " (6, 0.4301019571350565)]\n",
      "[(2, 0.3726494271826947),\n",
      " (7, 0.5443832091958983),\n",
      " (8, 0.3726494271826947),\n",
      " (9, 0.3726494271826947),\n",
      " (10, 0.27219160459794917),\n",
      " (11, 0.3726494271826947),\n",
      " (12, 0.27219160459794917)]\n",
      "[(4, 0.438482464916089),\n",
      " (10, 0.32027755044706185),\n",
      " (12, 0.32027755044706185),\n",
      " (13, 0.438482464916089),\n",
      " (14, 0.6405551008941237)]\n",
      "[(3, 0.3449874408519962),\n",
      " (10, 0.5039733231394895),\n",
      " (13, 0.3449874408519962),\n",
      " (15, 0.5039733231394895),\n",
      " (16, 0.5039733231394895)]\n",
      "[(8, 0.30055933182961736),\n",
      " (11, 0.30055933182961736),\n",
      " (12, 0.21953536176370683),\n",
      " (17, 0.43907072352741366),\n",
      " (18, 0.43907072352741366),\n",
      " (19, 0.43907072352741366),\n",
      " (20, 0.43907072352741366)]\n",
      "[(21, 0.48507125007266594),\n",
      " (22, 0.48507125007266594),\n",
      " (23, 0.48507125007266594),\n",
      " (24, 0.24253562503633297),\n",
      " (25, 0.48507125007266594)]\n",
      "[(24, 0.31622776601683794),\n",
      " (26, 0.31622776601683794),\n",
      " (27, 0.6324555320336759),\n",
      " (28, 0.6324555320336759)]\n",
      "[(24, 0.20466057569885868),\n",
      " (26, 0.20466057569885868),\n",
      " (29, 0.40932115139771735),\n",
      " (30, 0.2801947048062438),\n",
      " (31, 0.40932115139771735),\n",
      " (32, 0.40932115139771735),\n",
      " (33, 0.40932115139771735),\n",
      " (34, 0.40932115139771735)]\n",
      "[(9, 0.6282580468670046), (26, 0.45889394536615247), (30, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "# every word is now represented by a vector: Token-ID and token-weight\n",
    "for doc in corpus_tfidf:\n",
    "    pprint.pprint(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an index for the corpus\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0), (1, 0.12172779), (2, 0.14323246), (3, 0.67615116), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "# add a query document\n",
    "query_document = 'system engineering'.split()\n",
    "\n",
    "# transform the query document to a vector\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "\n",
    "# compare the query document to each document in the corpus\n",
    "sims = index[tfidf[query_bow]]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.67615116\n",
      "2 0.14323246\n",
      "1 0.12172779\n",
      "0 0.0\n",
      "4 0.0\n",
      "5 0.0\n",
      "6 0.0\n",
      "7 0.0\n",
      "8 0.0\n"
     ]
    }
   ],
   "source": [
    "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(document_number, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Streaming\n",
    "\n",
    "Since we do not want to load the whole corpus into memory every time, we need some changes in our approach, so we are able to stream the single documents when needed.\n",
    "Code is from: https://radimrehurek.com/gensim/auto_examples/core/run_corpora_and_vector_spaces.html#corpus-streaming-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the preprocessing file\n",
    "preprocessing_file = 'data/corpus/deu_mixed-typical_2011_10K/deu_mixed-typical_2011_10K-sentences.txt'\n",
    "# preprocessing_file = 'data/corpus/deu_mixed-typical_2011_1M/deu_mixed-typical_2011_1M-sentences.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove IDs from the corpus, since they do not belong in the content of the documents.\n",
    "temp_input_file = open(preprocessing_file, 'r', encoding = \"utf-8\")\n",
    "output_file = open(os.path.splitext(preprocessing_file)[0]+\"_only.txt\", 'w', encoding = \"utf-8\")\n",
    "for line in temp_input_file.readlines():\n",
    "    output_file.write(\" \".join(line.split()[1:])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input file for further preprocessing\n",
    "input_file = 'data/corpus/deu_mixed-typical_2011_10K/deu_mixed-typical_2011_10K-sentences_only.txt'\n",
    "# input_file = 'data/corpus/deu_mixed-typical_2011_1M/deu_mixed-typical_2011_1M-sentences_only.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save corpus as a list, so we can print out the document at the end. Not ideal for performance, but currently no other solution.\n",
    "corpus_list = []\n",
    "for line in open(input_file, encoding = \"utf-8\"):\n",
    "    corpus_list.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stopwords from NLTK\n",
    "stop_words = set(stopwords.words('german'))\n",
    "\n",
    "# load all tokens as lowercase text\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in open(input_file, encoding=\"utf-8\"))\n",
    "\n",
    "# find all stop words\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stop_words if stopword in dictionary.token2id]\n",
    "\n",
    "# find all words that only occur once\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "\n",
    "\n",
    "# Remove all stopwords and words that only occur once\n",
    "dictionary.filter_tokens(stop_ids + once_ids)\n",
    "\n",
    "# Since we filtered out some words, the ID count now has gaps. With .compactify we can remove those gaps\n",
    "dictionary.compactify()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the corpus as an object\n",
    "\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        # Each line in the corpus file represents one document, each token in a document is seperated by a whitespace\n",
    "        for line in open(input_file):\n",
    "            # Transfom the corpus to vectors\n",
    "            yield dictionary.doc2bow(line.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the corpus, without loading it into memory\n",
    "corpus = MyCorpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The corpus now exists only as an object, to work with it, the vectors inside have to be called. Only then they will be loaded into memory.\n",
    "Example: Print the first 10 document vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (499, 1)]\n",
      "[(2, 1), (3, 1), (701, 1)]\n",
      "[(3, 1), (4, 1), (5, 1), (57, 1)]\n",
      "[(7, 1), (8, 1), (1884, 1)]\n",
      "[(3, 1), (9, 1), (1072, 1)]\n",
      "[(3, 1), (174, 1)]\n",
      "[(3, 1), (10, 1), (11, 1), (2320, 1)]\n",
      "[(12, 1), (13, 1), (3009, 1)]\n",
      "[(14, 1), (15, 1), (16, 1), (3661, 1)]\n",
      "[(3, 1), (9, 1), (17, 1), (18, 1), (1693, 1)]\n"
     ]
    }
   ],
   "source": [
    "for index, vector in enumerate(corpus):\n",
    "    if index <10:\n",
    "        print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with our Corpus\n",
    "\n",
    "Now that we are able to load our corpus memory friendly we can transform the document vectors using a variety of functions.\n",
    "\n",
    "First we have to initialize a model, which will be used for the transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Model:\n",
    "tfidf_model = models.TfidfModel(corpus, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI (Latent Semantic Indexing) Model:\n",
    "tfidf_corpus = tfidf_model[corpus]\n",
    "lsi_model = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Model:\n",
    "lda_model = models.LdaModel(corpus, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_model.print_topics(num_topics=10, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = similarities.SparseMatrixSimilarity(tfidf_model[corpus], num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an example which should be queried against the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Sentence:\n",
    "example = \"Die bisherigen Bau-Planungen bezüglich des Klinikums sind sehr verändert.\"\n",
    "\n",
    "# Transform example to vector\n",
    "example_vec = dictionary.doc2bow(example.lower().split())\n",
    "\n",
    "# Convert it to our current model space\n",
    "example_lda = tfidf_model[example_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(954, 1), (1241, 1), (1730, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(example_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = index[example_vec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die bisherigen PCMs brachten es nur auf 128 MBit.\n",
      " 0.40824828\n",
      "Eine umfassende Chronik der bisherigen Ereignisse!\n",
      " 0.40824828\n",
      "Bei den bisherigen WM-Auktionen wurden insgesamt 170 000 Euro erzielt.\n",
      " 0.25779927\n"
     ]
    }
   ],
   "source": [
    "for document_number, score in sorted(enumerate(sim), key=lambda x: x[1], reverse=True):\n",
    "    if score != 0.0:\n",
    "        print(corpus_list[document_number], score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
