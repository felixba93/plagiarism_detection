{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plagiarism Detection Notebook\n",
    "## Notebook for the \"Textmining\" project in WS2020/2021\n",
    "\n",
    "Sources used for code: \n",
    "\n",
    "* https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html\n",
    "\n",
    "* https://radimrehurek.com/gensim/auto_examples/core/run_corpora_and_vector_spaces.html#corpus-streaming-tutorial\n",
    "\n",
    "* https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#12buildingthetopicmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import pprint\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from smart_open import open \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some example documents. For the actual application we wouldn't load everything at once.\n",
    "\n",
    "documents = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Prepocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function for text preprocessing. Converts the text to lower case, removes stopwords and words with a minimum length of 2 and maximum length of 15\n",
    "def preprocessing (corpus):\n",
    "    \n",
    "    processed_corpus = []\n",
    "    \n",
    "    # load stopwords from NLTK\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # go through each document in the corpus\n",
    "    for document in corpus:\n",
    "        \n",
    "        # step1: convert to lowercase and remove words that do not match the min-max-length\n",
    "        step1 = gensim.utils.simple_preprocess(document, deacc=False, min_len=2, max_len=15)\n",
    "        \n",
    "        #step2: remove stopwords\n",
    "        step2 = [word for word in step1 if word not in stop_words]\n",
    "        \n",
    "        processed_corpus.append(step2)\n",
    "    return processed_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'],\n",
      " ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'management', 'system'],\n",
      " ['system', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
      " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
      " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
      " ['intersection', 'graph', 'paths', 'trees'],\n",
      " ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "processed_corpus = preprocessing(documents)\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe add a filter for min occurence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(35 unique tokens: ['abc', 'applications', 'computer', 'human', 'interface']...)\n",
      "{'abc': 0, 'applications': 1, 'computer': 2, 'human': 3, 'interface': 4, 'lab': 5, 'machine': 6, 'opinion': 7, 'response': 8, 'survey': 9, 'system': 10, 'time': 11, 'user': 12, 'eps': 13, 'management': 14, 'engineering': 15, 'testing': 16, 'error': 17, 'measurement': 18, 'perceived': 19, 'relation': 20, 'binary': 21, 'generation': 22, 'random': 23, 'trees': 24, 'unordered': 25, 'graph': 26, 'intersection': 27, 'paths': 28, 'iv': 29, 'minors': 30, 'ordering': 31, 'quasi': 32, 'well': 33, 'widths': 34}\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
      " [(2, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
      " [(4, 1), (10, 1), (12, 1), (13, 1), (14, 1)],\n",
      " [(3, 1), (10, 2), (13, 1), (15, 1), (16, 1)],\n",
      " [(8, 1), (11, 1), (12, 1), (17, 1), (18, 1), (19, 1), (20, 1)],\n",
      " [(21, 1), (22, 1), (23, 1), (24, 1), (25, 1)],\n",
      " [(24, 1), (26, 1), (27, 1), (28, 1)],\n",
      " [(24, 1), (26, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)],\n",
      " [(9, 1), (26, 1), (30, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# convert text to vectors using the dictionary function\n",
    "\n",
    "# define dictionary of our corpus. Contains the word frequency of each token in the whole corpus\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "\n",
    "# transform the corpus to vectors. Each vector consists of a token ID and the token frequency (taken from the dictionary)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "\n",
    "print(dictionary)\n",
    "print(dictionary.token2id)\n",
    "pprint.pprint(bow_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a TF-IDF model of the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# transform the corpus\n",
    "corpus_tfidf = tfidf[bow_corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.4301019571350565),\n",
      " (1, 0.4301019571350565),\n",
      " (2, 0.2944198962221451),\n",
      " (3, 0.2944198962221451),\n",
      " (4, 0.2944198962221451),\n",
      " (5, 0.4301019571350565),\n",
      " (6, 0.4301019571350565)]\n",
      "[(2, 0.3726494271826947),\n",
      " (7, 0.5443832091958983),\n",
      " (8, 0.3726494271826947),\n",
      " (9, 0.3726494271826947),\n",
      " (10, 0.27219160459794917),\n",
      " (11, 0.3726494271826947),\n",
      " (12, 0.27219160459794917)]\n",
      "[(4, 0.438482464916089),\n",
      " (10, 0.32027755044706185),\n",
      " (12, 0.32027755044706185),\n",
      " (13, 0.438482464916089),\n",
      " (14, 0.6405551008941237)]\n",
      "[(3, 0.3449874408519962),\n",
      " (10, 0.5039733231394895),\n",
      " (13, 0.3449874408519962),\n",
      " (15, 0.5039733231394895),\n",
      " (16, 0.5039733231394895)]\n",
      "[(8, 0.30055933182961736),\n",
      " (11, 0.30055933182961736),\n",
      " (12, 0.21953536176370683),\n",
      " (17, 0.43907072352741366),\n",
      " (18, 0.43907072352741366),\n",
      " (19, 0.43907072352741366),\n",
      " (20, 0.43907072352741366)]\n",
      "[(21, 0.48507125007266594),\n",
      " (22, 0.48507125007266594),\n",
      " (23, 0.48507125007266594),\n",
      " (24, 0.24253562503633297),\n",
      " (25, 0.48507125007266594)]\n",
      "[(24, 0.31622776601683794),\n",
      " (26, 0.31622776601683794),\n",
      " (27, 0.6324555320336759),\n",
      " (28, 0.6324555320336759)]\n",
      "[(24, 0.20466057569885868),\n",
      " (26, 0.20466057569885868),\n",
      " (29, 0.40932115139771735),\n",
      " (30, 0.2801947048062438),\n",
      " (31, 0.40932115139771735),\n",
      " (32, 0.40932115139771735),\n",
      " (33, 0.40932115139771735),\n",
      " (34, 0.40932115139771735)]\n",
      "[(9, 0.6282580468670046), (26, 0.45889394536615247), (30, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "# every word is now represented by a vector: Token-ID and token-weight\n",
    "for doc in corpus_tfidf:\n",
    "    pprint.pprint(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an index for the corpus\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0), (1, 0.12172779), (2, 0.14323246), (3, 0.67615116), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "# add a query document\n",
    "query_document = 'system engineering'.split()\n",
    "\n",
    "# transform the query document to a vector\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "\n",
    "# compare the query document to each document in the corpus\n",
    "sims = index[tfidf[query_bow]]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.67615116\n",
      "2 0.14323246\n",
      "1 0.12172779\n",
      "0 0.0\n",
      "4 0.0\n",
      "5 0.0\n",
      "6 0.0\n",
      "7 0.0\n",
      "8 0.0\n"
     ]
    }
   ],
   "source": [
    "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(document_number, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Streaming\n",
    "\n",
    "Since we do not want to load the whole corpus into memory every time, we need some changes in our approach, so we are able to stream the single documents when needed.\n",
    "Code is from: https://radimrehurek.com/gensim/auto_examples/core/run_corpora_and_vector_spaces.html#corpus-streaming-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the preprocessing file\n",
    "preprocessing_file = 'data/corpus/deu_mixed-typical_2011_10K/deu_mixed-typical_2011_10K-sentences.txt'\n",
    "# preprocessing_file = 'data/corpus/deu_mixed-typical_2011_1M/deu_mixed-typical_2011_1M-sentences.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove IDs from the corpus, since they do not belong in the content of the documents.\n",
    "temp_input_file = open(preprocessing_file, 'r', encoding = \"utf-8\")\n",
    "output_file = open(os.path.splitext(preprocessing_file)[0]+\"_only.txt\", 'w', encoding = \"utf-8\")\n",
    "for line in temp_input_file.readlines():\n",
    "    output_file.write(\" \".join(line.split()[1:])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try do create an index\n",
    "temp_input_file = open(preprocessing_file, 'r', encoding = \"utf-8\")\n",
    "index_dic = {}\n",
    "for line in temp_input_file.readlines():\n",
    "    index_dic[int(line.split()[0])] = str(line.split()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input file for further preprocessing\n",
    "input_file = 'data/corpus/deu_mixed-typical_2011_10K/deu_mixed-typical_2011_10K-sentences_only.txt'\n",
    "# input_file = 'data/corpus/deu_mixed-typical_2011_1M/deu_mixed-typical_2011_1M-sentences_only.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save corpus as a list, so we can print out the document at the end. Not ideal for performance, but currently no other solution.\n",
    "corpus_list = []\n",
    "for line in open(input_file, encoding = \"utf-8\"):\n",
    "    corpus_list.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stopwords from NLTK\n",
    "stop_words = set(stopwords.words('german'))\n",
    "\n",
    "# load all tokens as lowercase text\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in open(input_file, encoding=\"utf-8\"))\n",
    "\n",
    "# find all stop words\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stop_words if stopword in dictionary.token2id]\n",
    "\n",
    "# find all words that only occur once\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "\n",
    "\n",
    "# Remove all stopwords and words that only occur once\n",
    "dictionary.filter_tokens(stop_ids + once_ids)\n",
    "\n",
    "# Since we filtered out some words, the ID count now has gaps. With .compactify we can remove those gaps\n",
    "dictionary.compactify()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the corpus as an object\n",
    "\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        # Each line in the corpus file represents one document, each token in a document is seperated by a whitespace\n",
    "        for line in open(input_file):\n",
    "            # Transfom the corpus to vectors\n",
    "            yield dictionary.doc2bow(line.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the corpus, without loading it into memory\n",
    "corpus = MyCorpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The corpus now exists only as an object, to work with it, the vectors inside have to be called. Only then they will be loaded into memory.\n",
    "Example: Print the first 10 document vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1)]\n",
      "[(2, 1), (3, 1)]\n",
      "[(3, 1), (4, 1), (5, 1)]\n",
      "[(7, 1), (8, 1)]\n",
      "[(3, 1), (9, 1)]\n",
      "[(3, 1)]\n",
      "[(3, 1), (10, 1), (11, 1)]\n",
      "[(12, 1), (13, 1)]\n",
      "[(14, 1), (15, 1), (16, 1)]\n",
      "[(3, 1), (9, 1), (17, 1), (18, 1)]\n"
     ]
    }
   ],
   "source": [
    "for index, vector in enumerate(corpus):\n",
    "    if index <10:\n",
    "        print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with our Corpus\n",
    "\n",
    "Now that we are able to load our corpus memory friendly we can transform the document vectors using a variety of functions.\n",
    "\n",
    "First we have to initialize a model, which will be used for the transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Model:\n",
    "tfidf_model = models.TfidfModel(corpus, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI (Latent Semantic Indexing) Model:\n",
    "tfidf_corpus = tfidf_model[corpus]\n",
    "lsi_model = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Model:\n",
    "lda_model = models.LdaModel(corpus, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_model.print_topics(num_topics=10, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = similarities.SparseMatrixSimilarity(tfidf_model[corpus], num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an example which should be queried against the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Sentence:\n",
    "example = \"Die bisherigen Bau-Planungen bezüglich des Klinikums sind sehr verändert.\"\n",
    "\n",
    "# Transform example to vector\n",
    "example_vec = dictionary.doc2bow(example.lower().split())\n",
    "\n",
    "# Convert it to our current model space\n",
    "example_lda = tfidf_model[example_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(954, 1), (1241, 1), (1730, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(example_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = index[example_vec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Die', 'Biografie', 'eines', 'Songs\".'] 0.40824828\n",
      "['Eine', 'Übersicht', 'der', 'Kindertageseinrichtungen', 'in', 'Rodgau.'] 0.40824828\n",
      "['Bei', 'den', 'beteiligten', 'Fahrzeugen', 'wurden', 'jeweils', 'die', 'Außenspiegel', 'beschädigt.'] 0.25779927\n"
     ]
    }
   ],
   "source": [
    "for document_number, score in sorted(enumerate(sim), key=lambda x: x[1], reverse=True):\n",
    "    if score != 0.0:\n",
    "        print(index_dic[document_number], score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die bisherigen PCMs brachten es nur auf 128 MBit.\n",
      " 0.40824828\n",
      "Eine umfassende Chronik der bisherigen Ereignisse!\n",
      " 0.40824828\n",
      "Bei den bisherigen WM-Auktionen wurden insgesamt 170 000 Euro erzielt.\n",
      " 0.25779927\n"
     ]
    }
   ],
   "source": [
    "for document_number, score in sorted(enumerate(sim), key=lambda x: x[1], reverse=True):\n",
    "    if score != 0.0:\n",
    "        print(corpus_list[document_number], score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different results for index_dic vs document list. document_number from gensim is not the same as the index number?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Corpus\n",
    "\n",
    "Corpus from: https://dumps.wikimedia.org/dewiki/20200820/\n",
    "\n",
    "Sentences for comparison from: https://github.com/t-systems-on-site-services-gmbh/german-wikipedia-text-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from xml.etree.ElementTree import *\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "import os\n",
    "import pprint\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import LdaMulticore\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from smart_open import open \n",
    "import spacy\n",
    "import de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the language model from spacy\n",
    "spacy_data = de_core_news_sm.load()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # load and tokenize text\n",
    "    prep_text = spacy_data(text)\n",
    "    # list for tokens\n",
    "    prep_tokens = []\n",
    "    # for every token in text\n",
    "    for token in prep_text:\n",
    "        # remove stopwords and punctuatiuon\n",
    "        if token.pos_ != 'PUNCT' and token.is_stop == False:\n",
    "            # lemmatize and transform to lowercase\n",
    "            lemma_token = token.lemma_.lower()\n",
    "            # remove non-alphabetic tokens\n",
    "            if lemma_token.isalpha() or lemma_token == '-PRON-':\n",
    "                prep_tokens.append(lemma_token)\n",
    "    # return preprocessed text \n",
    "    return prep_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a corpus from the text contents of the XML file:\n",
    "\n",
    "First test:\n",
    "\n",
    "Print text from \\<text>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_file = \"data/wiki_corpus/dewiki-20200820-pages-articles-multistream.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alan', 'smithee', 'stehen', 'pseudonym', 'fiktiv', 'regisseur', 'film', 'verantworten', 'eigentlich', 'regisseur', 'name', 'werk', 'verbindung', 'bringen', 'directors', 'guild', 'of', 'america', 'dga', 'situation', 'empfehlen', 'seither', 'thoma', 'angeles', 'name', 'of', 'director', 'smithee', 'what', 'it', 'used', 'to', 'be', 'zuletzt', 'prüfen', 'april', 'alan', 'smithee', 'weiterhin', 'gebrauch', 'alternative', 'schreibweise', 'ursprungsvariante', 'smithee', 'alan', 'smithee', 'teilweise', 'asiatisch', 'anmutend', 'schreibweise', 'alan', 'smi', 'thee', 'sumishii', 'aran', 'gehören', 'internet', 'movie', 'database', 'name', 'eintrag', 'alan', 'smithee', 'geschichte', 'entstehung', 'pseudonym', 'entstehen', 'infolge', 'arbeit', 'death', 'of', 'gunfighter', 'deutsch', 'titel', 'frank', 'patch', 'stunde', 'zählen', 'regisseur', 'robert', 'totten', 'hauptdarsteller', 'richard', 'widmark', 'geraten', 'streit', 'woraufhin', 'don', 'siegel', 'neu', 'regisseur', 'einsetzen', 'film', 'tragen', 'abschluss', 'arbeit', 'deutlich', 'tottens', 'manier', 'drehtage', 'siegel', 'arbeiten', 'weshalb', 'nennung', 'name', 'regisseur', 'ablehnen', 'totten', 'lehnen', 'ebenfalls', 'lösung', 'smithee', 'möglichst', 'einzigartig', 'name', 'name', 'bio', 'biography', 'for', 'alan', 'smithee', 'internet', 'movie', 'zeitgenössisch', 'kritik', 'regisseur', 'roger', 'ebert', 'worten', 'loben', 'smithee', 'name', 'i', 'm', 'not', 'familiar', 'with', 'allows', 'his', 'story', 'to', 'unfold', 'naturally', 'he', 'never', 'preaches', 'and', 'he', 'never', 'lingers', 'on', 'the', 'obvious', 'his', 'characters', 'do', 'what', 'they', 'have', 'to', 'death', 'of', 'gunfighter', 'zuletzt', 'prüfen', 'april', 'regisseur', 'alan', 'smithee', 'name', 'vertrauen', 'erlauben', 'handlung', 'entfalten', 'predigen', 'niemals', 'verweilen', 'offensichtliche', 'charakter', 'aufdeckung', 'abkehr', 'parodie', 'alan', 'smithee', 'film', 'burn', 'hollywood', 'burn', 'deutsch', 'titel', 'fahr', 'hölle', 'hollywood', 'kino', 'pseudonym', 'groß', 'publikum', 'zuletzt', 'arthur', 'hiller', 'hiller', 'eigentlich', 'regisseur', 'film', 'name', 'zurückziehen', 'analog', 'filmtitel', 'pseudonym', 'alan', 'smithee', 'benutzen', 'film', 'gelten', 'schlecht', 'film', 'gewinnen', 'goldene', 'film', 'supernova', 'führen', 'gewiß', 'thoma', 'lee', 'alias', 'walter', 'hill', 'regie', 'finden', 'siehe', 'kindermädchen', 'smithee', 'supernova', 'sichten', 'film', 'namens', 'the', 'guardian', 'verwendung', 'verwendung', 'pseudonyms', 'mitglied', 'dga', 'strengen', 'reglementiert', 'regisseur', 'gedreht', 'film', 'name', 'hergeben', 'sichtung', 'fertig', 'film', 'anzeigen', 'pseudonym', 'verwenden', 'rat', 'dga', 'entscheiden', 'binnen', 'anliegen', 'erhebt', 'produktionsfirma', 'einspruch', 'entscheiden', 'komitee', 'mitglied', 'dga', 'vereinigung', 'fernsehproduzenten', 'regisseur', 'pseudonym', 'angeben', 'beantragung', 'regisseur', 'stillschweigen', 'halten', 'fertig', 'film', 'öffentlich', 'kritisieren', 'dga', 'verwendung', 'pseudonyms', 'siehe', 'regelung', 'artikel', 'abschnitt', 'basic', 'agreement', 'pdf', 'dga', 'abrufen', 'april', 'antrag', 'regisseur', 'pseudonymisierung', 'ablehnen', 'tony', 'kaye', 'kaye', 'name', 'smithee', 'film', 'american', 'history', 'x', 'einsetzen', 'obwohl', 'antrag', 'stellen', 'produktion', 'name', 'verwenden', 'pilotfilm', 'fernsehserie', 'schulmädchen', 'senden', 'ard', 'august', 'zweiteilig', 'paparazzo', 'werk', 'erscheinen', 'anstatt', 'eigentlich', 'regisseur', 'stephan', 'wagner', 'wagner', 'alan', 'smithee', 'abspann', 'regisseur', 'pseudonym', 'benutzen', 'don', 'siegel', 'robert', 'totten', 'frank', 'patch', 'stunde', 'zählen', 'david', 'lynch', 'dreistündige', 'fernsehfassung', 'wüstenplanet', 'wüstenplanet', 'chris', 'christensen', 'the', 'omega', 'imperativ', 'gianni', 'bozzacchi', 'i', 'love', 'stuart', 'rosenberg', 'let', 's', 'get', 'harry', 'richard', 'sarafian', 'starfire', 'dennis', 'hopper', 'catchfire', 'arthur', 'hiller', 'hiller', 'fahr', 'hölle', 'hollywood', 'rick', 'rosenthal', 'vogel', 'ii', 'rückkehr', 'kevin', 'yagher', 'hellraiser', 'iv', 'bloodline', 'pilotfilm', 'serie', 'macgyver', 'folge', 'staffel', 'fahren', 'alan', 'smithee', 'regisseur', 'tv', 'rage', 'jerrold', 'freedman', 'regisseur', 'pilotfilms', 'angeben', 'regisseur', 'folge', 'unbekannt', 'drehbuchautoren', 'pseudonym', 'benutzen', 'gehören', 'sam', 'raimi', 'ivan', 'raimi', 'drehbuch', 'total', 'beknackt', 'nuß', 'alan', 'smithee', 'alan', 'smithee', 'sr', 'schreiben', 'computerspielen', 'pseudonym', 'angeben', 'abspann', 'marine', 'sharpshooter', 'iv', 'art', 'director', 'spiel', 'alan', 'smithee', 'external', 'showtopicalbumbackground', 'produzieren', 'new', 'york', 'yorker', 'big', 'dance', 'theater', 'alan', 'smithee', 'directed', 'this', 'play', 'august', 'jahr', 'berlin', 'tanz', 'august', 'aufführen', 'smithee', 'schuld', 'frankfurter', 'allgemeine', 'sonntagszeitung', 'august', 'seite', 'literatur', 'jeremy', 'braddock', 'stephen', 'hock', 'directed', 'by', 'smithee', 'foreword', 'by', 'andrew', 'sarris', 'university', 'of', 'minnesota', 'press', 'minneapolis', 'london', 'isbn', 'weblinks', 'artikel', 'smithee', 'abc', 'online', 'englisch', 'mann', 'niemals', 'leben', 'spiegel', 'online', 'einestages', 'alan', 'smithee', 'leben', 'dradio', 'wissen', 'einzelnachweise', 'references', 'kategorie', 'fiktiv', 'alan', 'kategorie', 'pseudonym', 'kategorie', 'alan', 'kategorie', 'werk', 'alan', 'smithee']\n",
      "['infobox', 'chemisches', 'element', 'periodensystem', 'name', 'actinium', 'symbol', 'ac', 'ordnungszahl', 'serie', 'üm', 'gruppe', 'periode', 'block', 'd', 'allgemein', 'aussehen', 'silbrig', 'cas', 'massenanteil', 'ref', 'binder', 'lexikon', 'chemisch', 'element', 'hirzel', 'verlag', 'stuttgart', 'isbn', 'atomar', 'hauptquelle', 'ref', 'wert', 'eigenschaft', 'infobox', 'angeben', 'actinium', 'atommasse', 'atomradius', 'atomradiusberechnet', 'kovalenterradius', 'vanderwaalsradius', 'elektronenkonfiguration', 'austrittsarbeit', 'ev', 'ref', 'kj', 'mol', 'ref', 'ev', 'ref', 'kj', 'mol', 'ref', 'ev', 'ref', 'kj', 'mol', 'ref', 'ev', 'ref', 'kj', 'mol', 'ref', 'physikalisch', 'aggregatzustand', 'fest', 'modifikation', 'kristallstruktur', 'kubisches', 'flächenzentriert', 'dicht', 'g', 'cm', 'mohshärte', 'magnetismus', 'molaresvolumen', 'verdampfungswärme', 'kj', 'mol', 'schmelzwärme', 'dampfdruck', 'schallgeschwindigkeit', 'spezifischewärmekapazität', 'elektrischeleitfähigkeit', 'wärmeleitfähigkeit', 'chemisch', 'oxidationszustände', 'normalpotential', 'ac', 'elektronegativität', 'quelle', 'nv', 'h', 'euh', 'p', 'quelle', 'p', 'radioaktiv', 'isotope', 'infobox', 'chemisches', 'element', 'isotop', 'massenzahl', 'symbol', 'ac', 'nh', 'halbwertszeit', 'anzahlzerfallstypen', 'ra', 'fr', 'th', 'infobox', 'chemisches', 'element', 'isotop', 'massenzahl', 'symbol', 'ac', 'nh', 'halbwertszeit', 'anzahlzerfallstypen', 'fr', 'infobox', 'chemisches', 'element', 'isotop', 'massenzahl', 'symbol', 'ac', 'nh', 'halbwertszeit', 'anzahlzerfallstypen', 'th', 'ra', 'fr', 'infobox', 'chemisches', 'element', 'isotop', 'massenzahl', 'symbol', 'ac', 'nh', 'halbwertszeit', 'anzahlzerfallstypen', 'th', 'fr', 'infobox', 'chemisches', 'element', 'isotop', 'massenzahl', 'symbol', 'ac', 'nh', 'halbwertszeit', 'h', 'anzahlzerfallstypen', 'th', 'nmreigenschaften', 'actinium', 'chemisch', 'element', 'elementsymbol', 'ac', 'ordnungszahl', 'periodensystem', 'element', 'stehen', 'scandiumgruppe', 'element', 'hören', 'block', 'namensgeber', 'gruppe', 'actinoide', 'folgend', 'element', 'geschichte', 'datei', 'periodensystem', 'periodensystem', 'lücke', 'actinium', 'unter', 'rand', 'thorium', 'actinium', 'französisch', 'chemiker', 'debierne', 'entdecken', 'pechblende', 'isolieren', 'ähnlichkeit', 'titan', 'debierne', 'sur', 'une', 'nouvelle', 'matière', 'comptes', 'rendus', 'gallica', 'debierne', 'sur', 'un', 'nouvel', 'élément', 'l', 'actinium', 'comptes', 'rendus', 'gallica', 'zuschreiben', 'bezeichnung', 'leiten', 'radioaktivität', 'griechisch', 'ἀκτίς', 'aktís', 'strahl', 'figurowski', 'entdeckung', 'chemisch', 'element', 'ursprung', 'name', 'deutsch', 'übersetzung', 'leo', 'korniljew', 'ernst', 'lemke', 'moskau', 'isbn', 'friedrich', 'giesel', 'entdecken', 'element', 'unabhängig', 'friedrich', 'oskar', 'giesel', 'ueber', 'radium', 'radioactive', 'stoff', 'bericht', 'deutsche', 'chemische', 'gesellschaft', 'beschreiben', 'ähnlichkeit', 'lanthan', 'name', 'friedrich', 'oskar', 'giesel', 'ueber', 'emanationskörper', 'emanium', 'bericht', 'deutsche', 'chemische', 'gesellschaft', 'bildung', 'lateinisch', 'emano', 'ausfließen', 'ebenfalls', 'bezug', 'abgegeben', 'actinium', 'emanium', 'identisch', 'erkennen', 'debiernes', 'namensgebung', 'vorzug', 'geben', 'entdecken', 'friedrich', 'oskar', 'giesel', 'ueber', 'emanium', 'bericht', 'deutsche', 'chemische', 'gesellschaft', 'geschichte', 'entdeckung', 'publikation', 'kirby', 'the', 'discovery', 'of', 'actinium', 'isis', 'zeitschrift', 'adloff', 'the', 'centenary', 'of', 'controversial', 'discovery', 'actinium', 'radiochimica', 'acta', 'fraglich', 'beschreiben', 'zeigen', 'publikation', 'einerseits', 'andererseits', 'widerspruch', 'aufweisen', 'gewinnung', 'darstellung', 'actinium', 'vorhanden', 'spielen', 'quelle', 'rolle', 'gewinnung', 'technisch', 'isotop', 'ac', 'bestrahlung', 'ra', 'herstellen', 'zeitangaben', 'schnell', 'zerfall', 'actiniums', 'stets', 'gering', 'menge', 'verfügbar', 'künstliche', 'herstellung', 'actinium', 'argonne', 'national', 'laboratory', 'chicago', 'durchführen', 'eigenschaft', 'physikalische', 'eigenschaft', 'metall', 'silberweiß', 'glänzen', 'ref', 'relativ', 'frederick', 'seitz', 'david', 'turnbull', 'solid', 'state', 'physics', 'advances', 'research', 'and', 'applications', 'academic', 'press', 'isbn', 'google', 'aufgrund', 'stark', 'radioaktivität', 'leuchten', 'actinium', 'dunkel', 'hellblauen', 'actinium', 'namensgebende', 'element', 'ähnlich', 'lanthan', 'gruppe', 'element', 'zeigen', 'deutlich', 'unterschied', 'lanthanoide', 'dauern', 'glenn', 'seaborg', 'wichtig', 'änderung', 'periodensystem', 'mendelejew', 'vorschlagen', 'einführung', 'glenn', 'seaborg', 'the', 'transuranium', 'element', 'science', 'pmid', 'chemisch', 'eigenschaft', 'reaktionsfähig', 'luft', 'wasser', 'angreifen', 'überziehen', 'schicht', 'actiniumoxid', 'wodurch', 'weit', 'oxidation', 'schützen', 'stites', 'murrell', 'salutsky', 'bob', 'stone', 'preparation', 'of', 'actinium', 'metal', 'chem', 'soc', 'ac', 'farblos', 'chemische', 'verhalten', 'actinium', 'ähneln', 'lanthan', 'actinium', 'bekannt', 'verbindung', 'katz', 'manning', 'chemistry', 'of', 'the', 'actinide', 'element', 'annual', 'review', 'of', 'nuclear', 'science', 'isotope', 'wovon', 'vorkommen', 'langlebig', 'isotop', 'ac', 'halbwertszeit', 'ac', 'zerfallsprodukt', 'uranisotops', 'u', 'uranerzen', 'lassen', 'wägbare', 'menge', 'ac', 'gewinnen', 'somit', 'verhältnismäßig', 'einfach', 'studium', 'element', 'ermöglichen', 'radioaktiv', 'zerfallsprodukten', 'befinden', 'aufwändig', 'strahlenschutzvorkehrungen', 'nötigen', 'verwendung', 'actinium', 'erzeugung', 'neutronen', 'einsetzen', 'rolle', 'spielen', 'energieumwandlung', 'nutzen', 'dual', 'zerfall', 'ac', 'groß', 'emission', 'th', 'zerfallen', 'francium', 'fr', 'lösung', 'ac', 'quelle', 'kurzlebig', 'fr', 'verwendbar', 'letzteres', 'regelmäßig', 'abtrennen', 'untersuchen', 'sicherheitshinweise', 'einstufung', 'verordnung', 'eg', 'nummer', 'liegen', 'chemische', 'gefährlichkeit', 'umfassen', 'völlig', 'untergeordnet', 'rolle', 'radioaktivität', 'beruhend', 'gefahr', 'spielen', 'letzteres', 'gelten', 'relevante', 'stoffmenge', 'handeln', 'verbindung', 'gering', 'anzahl', 'actiniumverbindungen', 'ausnahme', 'acpo', 'entsprechend', 'lanthanverbindungen', 'ähnlich', 'enthalten', 'actinium', 'oxidationsstufe', 'fried', 'french', 'hagemann', 'zachariasen', 'the', 'preparation', 'and', 'identification', 'of', 'some', 'pure', 'actinium', 'compounds', 'chem', 'soc', 'insbesondere', 'unterscheiden', 'gitterkonstanten', 'jeweilig', 'wenig', 'center', 'formel', 'farbe', 'symmetrie', 'raumgruppe', 'pm', 'b', 'pm', 'c', 'pm', 'z', 'cm', 'ac', 'silber', 'farr', 'giorgi', 'bowman', 'money', 'the', 'crystal', 'structure', 'of', 'actinium', 'metal', 'and', 'actinium', 'hydride', 'journal', 'of', 'inorganic', 'and', 'nuclear', 'chemistry', 'kubisch', 'ref', 'ac', 'o', 'weiß', 'ref', 'trigonal', 'ref', 'zachariasen', 'crystal', 'chemical', 'studies', 'of', 'the', 'of', 'element', 'xii', 'new', 'compounds', 'representing', 'known', 'structure', 'types', 'acta', 'crystallographica', 'ac', 's', 'kubisch', 'ref', 'zachariasen', 'crystal', 'chemical', 'studies', 'of', 'the', 'of', 'element', 'vi', 'the', 'ce', 's', 's', 'typ', 'of', 'structure', 'acta', 'crystallographica', 'acf', 'weiß', 'ref', 'meyer', 'lester', 'morss', 'synthesis', 'of', 'lanthanide', 'and', 'actinide', 'compounds', 'springer', 'isbn', 'google', 'hexagonal', 'ref', 'accl', 'hexagonal', 'ref', 'zachariasen', 'crystal', 'chemical', 'studies', 'of', 'the', 'of', 'element', 'new', 'structure', 'types', 'acta', 'crystallographica', 'acbr', 'weiß', 'ref', 'hexagonal', 'ref', 'acof', 'weiß', 'ref', 'meyer', 'lester', 'morss', 'synthesis', 'of', 'lanthanide', 'and', 'actinide', 'compounds', 'springer', 'isbn', 'google', 'kubisch', 'ref', 'acocl', 'tetragonal', 'ref', 'acobr', 'tetragonal', 'ref', 'acpo', 'h', 'o', 'hexagonal', 'ref', 'oxide', 'actinium', 'o', 'erhitzen', 'actinium', 'c', 'actinium', 'c', 'vakuum', 'erhalten', 'kristallgitter', 'isotyp', 'oxiden', 'meist', 'dreiwertigen', 'halogenide', 'actinium', 'lösung', 'feststoffreaktion', 'darstellen', 'fall', 'raumtemperatur', 'flusssäure', 'ac', 'fällen', 'produkt', 'fall', 'fluorwasserstoff', 'c', 'platinapparatur', 'behandeln', 'actinium', 'umsetzung', 'actiniumhydroxid', 'tetrachlormethan', 'temperatur', 'oberhalb', 'c', 'reaktion', 'aluminiumbromid', 'actinium', 'führen', 'actinium', 'behandlung', 'feucht', 'ammoniak', 'c', 'führen', 'oxibromid', 'verbindung', 'po', 'lösung', 'actinium', 'salzsäure', 'erhalten', 'weiß', 'gefärbt', 'o', 'erhitzen', 'actinium', 'schwefelwasserstoff', 'c', 'paar', 'minute', 'führen', 'schwarz', 'actinium', 's', 'literatur', 'harold', 'kirby', 'lester', 'morss', 'actinium', 'lester', 'morss', 'norman', 'edelstein', 'jean', 'fuger', 'the', 'chemistry', 'of', 'the', 'actinide', 'and', 'transactinide', 'element', 'springer', 'dordrecht', 'isbn', 'weblinks', 'wiktionary', 'commonscat', 'römpponline', 'actinium', 'januar', 'einzelnachweise', 'references', 'navigationsleiste', 'periodensystem', 'sh']\n",
      "['datei', 'ang', 'lee', 'festival', 'de', 'venise', 'mostra', 'lee', 'ang', 'lee', 'lǐ', 'ān', 'oktober', 'chaozhou', 'news', 'taiwan', 'archives', 'family', 'and', 'friends', 'praise', 'ang', 'quiet', 'dedication', 'taipei', 'times', 'landkreis', 'pingtung', 'republik', 'china', 'taiwan', 'taiwanischer', 'filmregisseur', 'drehbuchautor', 'watch', '戰爭人性與電影科技', 'chung', 't', 'ien', 'vielfach', 'ausgezeichnet', 'regisseur', 'unterschiedlich', 'film', 'eat', 'drink', 'woman', 'sinn', 'sinnlichkeit', 'sinnlichkeit', 'martial', 'tiger', 'and', 'dragon', 'film', 'brokeback', 'mountain', 'life', 'of', 'pi', 'schiffbruch', 'tiger', 'jeweils', 'oscar', 'kategorie', 'beste', 'regie', 'auszeichnen', 'leben', 'ang', 'lee', 'republik', 'china', 'gebären', 'eltern', 'emigrant', 'volksrepublik', 'lernen', 'taiwan', 'kennen', 'lee', 'alt', 'sohn', 'großeltern', 'mütterlicherseits', 'zug', 'kommunistisch', 'revolution', 'china', 'ums', 'leben', 'kommen', 'vater', 'lehrer', 'häufig', 'arbeitsstelle', 'wechseln', 'wachsen', 'ang', 'lee', 'verschieden', 'stadt', 'taiwan', 'entgegen', 'wunsch', 'eltern', 'vater', 'klassische', 'akademische', 'laufbahn', 'einschlagen', 'interessieren', 'lee', 'schauspiel', 'absolvieren', 'einverständnis', 'filmstudium', 'taipeh', 'anschluss', 'vereinigt', 'staat', 'university', 'of', 'illinois', 'urbana', 'theaterwissenschaft', 'studieren', 'erwerb', 'illinois', 'verlegen', 'studium', 'theaterproduktion', 'new', 'york', 'new', 'york', 'master', 'abschloss', 'entschloss', 'ebenfalls', 'taiwan', 'stammend', 'ehefrau', 'usa', 'bleiben', 'interesse', 'verschieben', 'trotzen', 'erfahrung', 'super', 'taiwan', 'ref', 'pekler', 'andrea', 'ungerböck', 'ang', 'lee', 'film', 'schüren', 'marburg', 'spät', 'filmregie', 'lee', 'berufswunsch', 'familie', 'insbesondere', 'vater', 'eingestehen', 'schneider', 'chronik', 'leben', 'werk', 'ang', 'lee', 'kino', 'poesie', 'grossformat', 'februar', 'studium', 'projekt', 'umsetzen', 'langfilm', 'fertigstellen', 'zeichnen', 'kontinuierliche', 'karriere', 'regisseur', 'groß', 'erfolg', 'sowohl', 'publikum', 'kritik', 'gelten', 'martial', 'tiger', 'and', 'dragon', 'starbesetzung', 'brokeback', 'mountain', 'heath', 'ledger', 'jake', 'gyllenhaal', 'letztere', 'bekommen', 'lee', 'regisseur', 'oscar', 'gut', 'regie', 'lee', 'film', 'preis', 'mittlerweile', 'goldener', 'bär', 'internationale', 'filmfestspiele', 'goldener', 'löwe', 'internationale', 'filmfestspiele', 'venedig', 'auszeichnen', 'lee', 'mikrobiologin', 'jane', 'lin', 'verheiraten', 'leben', 'white', 'plains', 'new', 'plains', 'westchester', 'county', 'bundesstaat', 'new', 'york', 'york', 'ehe', 'stammen', 'sohn', 'haan', 'mason', 'ang', 'lee', 'besitzen', 'united', 'states', 'permanent', 'resident', 'filmisches', 'werk', 'filmerfahrungen', 'taiwan', 'setzen', 'lee', 'studium', 'usa', 'ernsthaft', 'filmemachen', 'auseinander', 'rahmen', 'studium', 'new', 'york', 'drehen', 'kurzfilme', 'wirken', 'abschlussdreh', 'studienkollegen', 'spike', 'lee', 'regieassistent', 'abschlussfilm', 'fine', 'line', 'gewinnen', 'preis', 'renommiert', 'filmfest', 'gelingen', 'gewinn', 'hochdotierten', 'drehbuchwettbewerbs', 'taiwan', 'reihe', 'film', 'drehen', 'konflikt', 'taiwanischer', 'familie', 'thema', 'langfilme', 'lee', 'realisieren', 'begriff', 'father', 'knows', 'best', 'michael', 'pekler', 'andrea', 'ungerböck', 'merken', 'ironisch', 'trilogie', 'father', 'thinks', 'he', 'knows', 'best', 'titulieren', 'können', 'patriarchen', 'keineswegs', 'lösung', 'vergleiche', 'michael', 'pekler', 'andrea', 'ungerböck', 'ang', 'lee', 'film', 'schüren', 'marburg', 'bezeichnung', 'wiederkehrend', 'figur', 'chinesisch', 'familienoberhaupts', 'spielen', 'jeweils', 'taiwanischen', 'schauspieler', 'sihung', 'lung', 'film', 'thematisieren', 'öfter', 'ang', 'lee', 'familiär', 'problem', 'konflikt', 'selbstbestimmung', 'tradition', 'innen', 'außen', 'ost', 'west', 'generation', 'herrühren', 'film', 'allesamt', 'koproduktionen', 'bislang', 'folgend', 'projekt', 'handeln', 'film', 'lee', 'adaptionen', 'film', 'geschrieben', 'originaldrehbüchern', 'film', 'schiebende', 'hand', 'handeln', 'einzug', 'chinesisch', 'vater', 'erwachsen', 'sohn', 'schwiegertochter', 'new', 'york', 'interkulturell', 'problem', 'wohngemeinschaft', 'entstehen', 'zusammenarbeit', 'lee', 'drehbuchautor', 'produzent', 'james', 'schamus', 'bilden', 'film', 'lee', 'eng', 'arbeitsgemeinschaft', 'folgend', 'film', 'schreiben', 'gemeinsam', 'hochzeitsbankett', 'zeichnen', 'zusätzlich', 'neil', 'peng', 'eat', 'drink', 'woman', 'wang', 'film', 'lee', 'ausnahme', 'kurzfilms', 'chosen', 'hire', 'chosen', 'schamus', 'seither', 'entscheidend', 'funktion', 'ausüben', 'regelmäßig', 'zusammenarbeit', 'filmeditor', 'tim', 'squyres', 'lee', 'erstling', 'anfang', 'ausnahme', 'erfolgsfilms', 'brokeback', 'mountain', 'squires', 'film', 'ang', 'lee', 'drehen', 'schneiden', 'erfolg', 'erstlings', 'lee', 'nächstes', 'hochzeitsbankett', 'drehen', 'komödie', 'fingieren', 'eheschließung', 'homosexuell', 'usa', 'erneut', 'tauchen', 'figur', 'streng', 'weise', 'familienoberhaupts', 'schiebende', 'hand', 'taiwan', 'aufmerksamkeit', 'preis', 'sorgen', 'langfilm', 'lee', 'europa', 'aufstrebend', 'regisseur', 'film', 'erhalten', 'internationale', 'filmfestspiele', 'goldene', 'bär', 'bester', 'fremdsprachig', 'film', 'zudem', 'oscar', 'nominieren', 'gelten', 'hinaus', 'profitabel', 'jahr', 'million', 'produktionskosten', 'erzielen', 'einspielergebnis', 'isabell', 'gössele', 'kino', 'ang', 'lee', 'atem', 'verborgen', 'drache', 'tectum', 'marburg', 'sihung', 'lung', 'letzt', 'trilogie', 'eat', 'drink', 'woman', 'kongeniale', 'verkörperung', 'chinesisch', 'pekler', 'ungerböck', 'zentrum', 'maskeraden', 'alt', 'gesicht', 'wahr', 'lernen', 'verlieren', 'neu', 'lebenstauglicheres', 'christoph', 'schneider', 'chronik', 'leben', 'werk', 'ang', 'lee', 'kino', 'poesie', 'grossformat', 'februar', 'mal', 'verwitwete', 'vater', 'dreier', 'tochter', 'leben', 'liebe', 'unterschiedlich', 'art', 'angehen', 'ebenfalls', 'innerfamiliäre', 'konflikt', 'klären', 'eat', 'drink', 'woman', 'vorgänger', 'taipeh', 'drehen', 'mittelpunkt', 'film', 'stehen', 'titel', 'deuten', 'lieben', 'essen', 'ang', 'lee', 'privat', 'passioniert', 'koch', 'legen', 'hierbei', 'wert', 'kulinarisch', 'komponente', 'stilmittel', 'ref', 'pekler', 'ungerböck', 'konzipieren', 'hauptfigur', 'alt', 'witwers', 'berühmt', 'koch', 'dreimal', 'geschichte', 'angebot', 'produzentin', 'lindsay', 'doran', 'britisch', 'schauspielerin', 'emma', 'thompson', 'verfasste', 'adaption', 'roman', 'verstand', 'gefühl', 'jane', 'austen', 'großbritannien', 'drehen', 'eröffnen', 'lee', 'ersehnen', 'perspektive', 'jenseits', 'asiatisch', 'geprägt', 'stoff', 'trilogie', 'setzen', 'unterschiedlich', 'kultur', 'auseinander', 'sinn', 'sinnlichkeit', 'sinnlichkeit', 'verfilmung', 'roman', 'englisch', 'schriftstellerin', 'jane', 'austen', 'eissturm', 'spielen', 'usa', 'ride', 'with', 'the', 'devil', 'bürgerkrieg', 'ansiedeln', 'pendel', 'west', 'ost', 'tiger', 'and', 'dragon', 'hulk', 'unterschiedlich', 'tiger', 'and', 'dragon', 'gewinnen', 'lee', 'golden', 'werk', 'awards', 'oscar', 'prämieren', 'trophäe', 'fremdsprachig', 'film', 'film', 'chlotrudis', 'award', 'auszeichnen', 'chlotrudis', 'erhalten', 'brokeback', 'mountain', 'brokeback', 'mountain', 'lee', 'vielzahl', 'filmpreisen', 'ehren', 'oscar', 'gut', 'regie', 'goldener', 'löwe', 'filmfestspiele', 'venedig', 'venedig', 'auszeichnung', 'hollywood', 'foreign', 'press', 'association', 'gut', 'regisseur', 'jahr', 'verfilmen', 'gefahr', 'begierde', 'kurzgeschichte', 'eileen', 'chang', 'thriller', 'spielen', 'weltkriegs', 'shanghai', 'handeln', 'jung', 'chinesisch', 'agentin', 'spielen', 'tang', 'wei', 'beauftragen', 'hochrangigen', 'verräter', 'tony', 'leung', 'chiu', 'wai', 'liquidieren', 'lee', 'chinesischsprachige', 'tiger', 'and', 'dragon', 'offiziell', 'wettbewerb', 'internationale', 'filmfestspiele', 'venedig', 'venedig', 'vertreten', 'bringen', 'erneut', 'goldene', 'löwe', 'selben', 'gefahr', 'begierde', 'offiziell', 'taiwanischer', 'beitrag', 'nominierung', 'oscar', 'bester', 'fremdsprachig', 'fremdsprachig', 'film', 'douglas', 'tseng', 'shorter', 'version', 'of', 'lust', 'to', 'be', 'shown', 'here', 'the', 'straits', 'times', 'singapore', 'september', 'life', 'life', 'empfehlung', 'academy', 'of', 'motion', 'picture', 'arts', 'and', 'sciences', 'zurückziehen', 'chen', 'lian', 'xi', 'url', 'title', 'titel', 'lian', 'xi', 'qu', 'movie', 'database', 'ersetzen', 'februar', 'geben', 'lee', 'jury', 'internationale', 'filmfestspiele', 'venedig', 'venedig', 'leiten', 'kultur', 'venedig', 'ang', 'lee', 'frankfurter', 'februar', 'abrufen', 'juni', 'monat', 'erhalten', 'komödie', 'taking', 'woodstock', 'einladung', 'wettbewerb', 'internationale', 'filmfestspiele', 'cannes', 'filmfestspiele', 'cannes', 'wettbewerbsjury', 'internationale', 'filmfestspiele', 'cannes', 'cannes', 'berufen', 'stil', 'ang', 'lee', 'international', 'anerkannt', 'erfolgreich', 'regisseur', 'gelten', 'vielseitig', 'filmemacher', 'letzt', 'häufig', 'behandeln', 'lee', 'film', 'thema', 'familie', 'art', 'weise', 'autobiographische', 'zug', 'leben', 'tragen', 'lässt', 'umgebung', 'bewusst', 'einwirken', 'bringen', 'film', 'idee', 'manchmal', 'auge', 'schließen', 'bild', 'sehen', 'bild', 'finden', 'ang', 'kennzeichnend', 'meist', 'film', 'geradlinige', 'erzählstruktur', 'charakter', 'geschichte', 'verschieden', 'blickwinkeln', 'darstellen', 'verknüpfen', 'konflikt', 'menschlich', 'leben', 'traditionell', 'innovativ', 'stilelementen', 'ang', 'lee', 'erzählstrukturen', 'langweilig', 'kombinieren', 'verschieden', 'genre', 'epoche', 'hoffen', 'alt', 'filmgenres', 'ausprobieren', 'mixen', 'verdrehen', 'ang', 'filmografie', 'datei', 'ang', 'lee', 'festival', 'de', 'venise', 'lee', 'internationale', 'filmfestspiele', 'venedig', 'venedig', 'shades', 'of', 'the', 'lake', 'kurzfilm', 'fine', 'line', 'kurzfilm', 'schiebende', 'hand', 'pushing', 'hands', 'tui', 'shou', 'hochzeitsbankett', 'the', 'wedding', 'banquet', 'xi', 'yan', 'eat', 'drink', 'woman', 'yin', 'shi', 'nan', 'nu', 'sinn', 'sinnlichkeit', 'sinnlichkeit', 'sense', 'and', 'sensibility', 'eissturm', 'the', 'icestorm', 'ride', 'with', 'the', 'devil', 'tiger', 'and', 'dragon', 'crouching', 'tiger', 'hidden', 'dragon', 'hu', 'cang', 'long', 'chosen', 'hire', 'chosen', 'kurzwerbefilm', 'automarke', 'hulk', 'brokeback', 'mountain', 'gefahr', 'begierde', 'sè', 'jiè', 'taking', 'woodstock', 'life', 'of', 'pi', 'schiffbruch', 'zuschauer', 'frankfurter', 'allgemeine', 'dezember', 'irren', 'heldentour', 'billy', 'lynn', 'irren', 'heldentour', 'billy', 'lynn', 'billy', 'lynn', 's', 'long', 'halftime', 'walk', 'gemini', 'auszeichnung', 'auswahl', 'oscarverleihung', 'auszeichnung', 'kategorie', 'oscar', 'bester', 'fremdsprachig', 'fremdsprachig', 'film', 'tiger', 'and', 'dragon', 'oscarverleihung', 'nominierung', 'kategorie', 'oscar', 'beste', 'regie', 'tiger', 'and', 'dragon', 'oscarverleihung', 'nominierung', 'kategorie', 'oscar', 'bester', 'film', 'tiger', 'and', 'dragon', 'oscarverleihung', 'auszeichnung', 'kategorie', 'oscar', 'beste', 'regie', 'brokeback', 'mountain', 'oscarverleihung', 'auszeichnung', 'kategorie', 'oscar', 'beste', 'regie', 'life', 'of', 'pi', 'schiffbruch', 'tiger', 'oscarverleihung', 'nominierung', 'kategorie', 'oscar', 'bester', 'film', 'life', 'of', 'pi', 'schiffbruch', 'tiger', 'golden', 'globe', 'award', 'golden', 'globe', 'awards', 'nominierung', 'kategorie', 'golden', 'globe', 'award', 'beste', 'regie', 'sinn', 'sinnlichkeit', 'golden', 'globe', 'awards', 'auszeichnung', 'kategorie', 'golden', 'globe', 'award', 'beste', 'regie', 'tiger', 'and', 'dragon', 'golden', 'globe', 'awards', 'auszeichnung', 'kategorie', 'golden', 'globe', 'award', 'beste', 'regie', 'brokeback', 'mountain', 'golden', 'globe', 'awards', 'nominierung', 'kategorie', 'golden', 'globe', 'award', 'beste', 'regie', 'life', 'of', 'pi', 'schiffbruch', 'tiger', 'directors', 'guild', 'of', 'america', 'award', 'nominierung', 'kategorie', 'beste', 'spielfilmregie', 'sinn', 'sinnlichkeit', 'auszeichnung', 'kategorie', 'beste', 'spielfilmregie', 'tiger', 'and', 'dragon', 'auszeichnung', 'kategorie', 'beste', 'spielfilmregie', 'brokeback', 'mountain', 'nominierung', 'kategorie', 'beste', 'spielfilmregie', 'life', 'of', 'pi', 'schiffbruch', 'tiger', 'auszeichnung', 'goldener', 'bär', 'berliner', 'filmfestspiele', 'hochzeitsbankett', 'golden', 'horse', 'film', 'horse', 'beste', 'regie', 'hochzeitsbankett', 'goldener', 'bär', 'berliner', 'filmfestspiele', 'sinn', 'sinnlichkeit', 'deutscher', 'deutscher', 'filmpreis', 'bester', 'ausländisch', 'ausländisch', 'film', 'sinn', 'sinnlichkeit', 'golden', 'horse', 'film', 'horse', 'bester', 'film', 'tiger', 'and', 'dragon', 'hong', 'kong', 'film', 'award', 'tiger', 'and', 'dragon', 'aufnahme', 'american', 'academy', 'of', 'arts', 'and', 'sciences', 'goldener', 'löwe', 'filmfestivals', 'venedig', 'brokeback', 'mountain', 'golden', 'horse', 'film', 'horse', 'beste', 'regie', 'gefahr', 'begierde', 'goldener', 'löwe', 'filmfestivals', 'venedig', 'gefahr', 'begierde', 'siehe', 'taiwanischer', 'film', 'film', 'literatur', 'chronologisch', 'tanja', 'hanhart', 'redaktorin', 'ang', 'lee', 'kino', 'poesie', 'grossformat', 'februar', 'isbn', 'thoma', 'koebner', 'artikel', 'ang', 'lee', 'ders', 'filmregisseure', 'biographie', 'werkbeschreibungen', 'filmographien', 'abbildung', 'aktualisieren', 'erweitern', 'auflage', 'reclam', 'stuttgart', 'aufl', 'isbn', 'qin', 'hu', 'kino', 'ang', 'lee', 'chinesisch', 'philosophie', 'kunstauffassung', 'kultur', 'filmästhetischen', 'aspekt', 'gardez', 'verlag', 'michael', 'pekler', 'andrea', 'ungerböck', 'ang', 'lee', 'film', 'schüren', 'marburg', 'isbn', 'isabell', 'gössele', 'kino', 'ang', 'lee', 'atem', 'verborgen', 'drache', 'tectum', 'marburg', 'isbn', 'weblinks', 'commonscat', 'lee', 'ang', 'lee', 'biografie', 'who', 's', 'who', 'erfolg', 'missbrauche', 'spiegel', 'online', 'oktober', 'interview', 'subtext', 'western', 'interview', 'ralph', 'geisenhanslüke', 'regisseur', 'ang', 'lee', 'vater', 'stolz', 'januar', 'interview', 'andrea', 'kilb', 'ang', 'lee', 'sechzigsten', 'zerbrechlichkeit', 'welt', 'frankfurter', 'allgemeine', 'oktober', 'einzelnachweise', 'references', 'no', 'sortierung', 'lee', 'ang', 'kategorie', 'ang', 'kategorie', 'drehbuchautor', 'kategorie', 'filmregisseur', 'kategorie', 'oscarpreisträger', 'kategorie', 'kategorie', 'person', 'namensgeber', 'asteroid', 'kategorie', 'mitglied', 'american', 'academy', 'of', 'arts', 'and', 'sciences', 'kategorie', 'taiwaner', 'kategorie', 'geboren', 'kategorie', 'mann', 'personendaten', 'lee', 'ang', '李安', 'chinesisch', 'lǐ', 'ān', 'chinesisch', 'taiwanischer', 'regisseur', 'drehbuchautor', 'oktober', 'landkreis', 'pingtung', 'taiwan']\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):\n",
    "    if index < 3:\n",
    "        if event == 'end' and \"text\" in elem.tag:\n",
    "            prep_tokens = preprocess_text(elem.text)\n",
    "            print(prep_tokens)\n",
    "            elem.clear()\n",
    "            index += 1    \n",
    "    else:\n",
    "        break       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the corpus as an object\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        # define the XML tree\n",
    "        for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):            \n",
    "            # Each document is represented as an object between <text> tags in the xml file\n",
    "            if event == 'end' and \"text\" in elem.tag:\n",
    "                # Transfom the corpus to vectors\n",
    "                yield dictionary.doc2bow(preprocess_text(elem.text))\n",
    "                elem.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a smaller corpus, containing only the first 200 documents:\n",
    "class MyCorpus_small:\n",
    "    def __iter__(self):\n",
    "        index = 0\n",
    "        # define the XML tree\n",
    "        for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):\n",
    "            if index < 200:\n",
    "                # Each document is represented as an object between <text> tags in the xml file\n",
    "                if event == 'end' and \"text\" in elem.tag:\n",
    "                    # Transfom the corpus to vectors\n",
    "                    yield dictionary.doc2bow(preprocess_text(elem.text))\n",
    "                    index+=1\n",
    "                    elem.clear()\n",
    "                else:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the corpus, without loading it into memory\n",
    "corpus = MyCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_small = MyCorpus_small()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Somehow we have to save which text matches which ID, so we can later on return the text and not only it's vector representation. To match every text with an id and store it would render the following approaches, to stream the data instead of storing it in memory, obsolete/useless. To counter that i propose we create a dictionary which only contains the title of every article, instead of the full text.\n",
    "Currently WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_ids = {}\n",
    "index = 0\n",
    "for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):        \n",
    "    if index < 200:\n",
    "        if event == 'end' and \"title\" in elem.tag:\n",
    "            title_ids[index]=str(elem.text)\n",
    "            index += 1    \n",
    "            elem.clear()\n",
    "    else:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Alan Smithee'}\n"
     ]
    }
   ],
   "source": [
    "print(title_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for a specific entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "title_ids = {}\n",
    "index = 0\n",
    "for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):        \n",
    "    if event == 'end' and \"title\" in elem.tag:\n",
    "        if \"British Airways i360\" in elem.text:\n",
    "            title_ids[index]=str(elem.text)\n",
    "            print(elem.text)\n",
    "            break\n",
    "        elem.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get texts from the XML File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 47 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_ids = {}\n",
    "index = 0\n",
    "for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):        \n",
    "    if index < 200:\n",
    "        if event == 'end' and \"text\" in elem.tag:\n",
    "            text_ids[index]=str(elem.text)\n",
    "            index += 1    \n",
    "            elem.clear()\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(xml_file):\n",
    "    index = 0\n",
    "    first_elem = True\n",
    "    for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):        \n",
    "        if index < 200:\n",
    "            if event == \"end\" and \"text\" in elem.tag:\n",
    "                text = preprocess_text(elem.text)\n",
    "                if first_elem:\n",
    "                    dictionary = Dictionary([text])\n",
    "                    first_elem = False\n",
    "                    index += 1\n",
    "                else:\n",
    "                    dictionary.add_documents([text])\n",
    "                    index += 1\n",
    "                elem.clear()\n",
    "        else:\n",
    "            break\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build the dictionary:\n",
    "dictionary = build_dictionary(xml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 50 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# remove words that appear only once\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "dictionary.filter_tokens(once_ids)\n",
    "# remove gaps in id sequence after words that were removed\n",
    "dictionary.compactify() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(20309 unique tokens: ['abc', 'abkehr', 'ablehnen', 'abrufen', 'abschluss']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dictionary\n",
    "dictionary.save('data/wiki_200.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dictionary\n",
    "dictionary = Dictionary.load('data/wiki_200.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(20309 unique tokens: ['abc', 'abkehr', 'ablehnen', 'abrufen', 'abschluss']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "for vector in corpus:\n",
    "    if index <2:\n",
    "        print(vector)\n",
    "        index += 1\n",
    "    else:\n",
    "        print(\"finished\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity with LDA (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "* corpus: the corpus\n",
    "* num_topics: topics to be extracted from the training corpus\n",
    "* id2word: id to word mapping, the dictionary\n",
    "* workers: number of cpu cores used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda = LdaMulticore(corpus_small, num_topics=10, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.033490263), (1, 0.033490896), (2, 0.03348978), (3, 0.033489995), (4, 0.033489734), (5, 0.0334891), (6, 0.69858706), (7, 0.03348973), (8, 0.03349116), (9, 0.0334923)]\n"
     ]
    }
   ],
   "source": [
    "doc_bow = [(0, 1), (1, 1)]\n",
    "print(lda[doc_bow]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = similarities.SparseMatrixSimilarity(lda[corpus_small], num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 1), (43, 1), (65, 1), (84, 1), (85, 1), (157, 1), (182, 1), (189, 2), (217, 1), (244, 1), (245, 1), (256, 1)]\n",
      "[(3, 0.93544364)]\n"
     ]
    }
   ],
   "source": [
    "test_doc_raw = \"'''Alan Smithee''' steht als [[Pseudonym]] für einen fiktiven Regisseur, der Filme verantwortet, bei denen der eigentliche [[Regisseur]] seinen Namen nicht mit dem Werk in Verbindung gebracht haben möchte.\"\n",
    "test_vec = dictionary.doc2bow(preprocess_text(test_doc_raw))\n",
    "print(test_vec)\n",
    "# convert to lda space\n",
    "test_vec_lda = lda[test_vec]\n",
    "print(test_vec_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "sims = index[test_vec_lda]\n",
    "print(sims)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
