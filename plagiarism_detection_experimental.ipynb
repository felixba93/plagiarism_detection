{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plagiarism Detection Notebook\n",
    "## Notebook for the \"Textmining\" project in WS2020/2021\n",
    "\n",
    "Sources used for code: \n",
    "\n",
    "* https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html\n",
    "\n",
    "* https://radimrehurek.com/gensim/auto_examples/core/run_corpora_and_vector_spaces.html#corpus-streaming-tutorial\n",
    "\n",
    "* https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#12buildingthetopicmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import pprint\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from smart_open import open \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some example documents. For the actual application we wouldn't load everything at once.\n",
    "\n",
    "documents = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Prepocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function for text preprocessing. Converts the text to lower case, removes stopwords and words with a minimum length of 2 and maximum length of 15\n",
    "def preprocessing (corpus):\n",
    "    \n",
    "    processed_corpus = []\n",
    "    \n",
    "    # load stopwords from NLTK\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # go through each document in the corpus\n",
    "    for document in corpus:\n",
    "        \n",
    "        # step1: convert to lowercase and remove words that do not match the min-max-length\n",
    "        step1 = gensim.utils.simple_preprocess(document, deacc=False, min_len=2, max_len=15)\n",
    "        \n",
    "        #step2: remove stopwords\n",
    "        step2 = [word for word in step1 if word not in stop_words]\n",
    "        \n",
    "        processed_corpus.append(step2)\n",
    "    return processed_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'],\n",
      " ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'management', 'system'],\n",
      " ['system', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
      " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
      " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
      " ['intersection', 'graph', 'paths', 'trees'],\n",
      " ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "processed_corpus = preprocessing(documents)\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe add a filter for min occurence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(35 unique tokens: ['abc', 'applications', 'computer', 'human', 'interface']...)\n",
      "{'abc': 0, 'applications': 1, 'computer': 2, 'human': 3, 'interface': 4, 'lab': 5, 'machine': 6, 'opinion': 7, 'response': 8, 'survey': 9, 'system': 10, 'time': 11, 'user': 12, 'eps': 13, 'management': 14, 'engineering': 15, 'testing': 16, 'error': 17, 'measurement': 18, 'perceived': 19, 'relation': 20, 'binary': 21, 'generation': 22, 'random': 23, 'trees': 24, 'unordered': 25, 'graph': 26, 'intersection': 27, 'paths': 28, 'iv': 29, 'minors': 30, 'ordering': 31, 'quasi': 32, 'well': 33, 'widths': 34}\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
      " [(2, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
      " [(4, 1), (10, 1), (12, 1), (13, 1), (14, 1)],\n",
      " [(3, 1), (10, 2), (13, 1), (15, 1), (16, 1)],\n",
      " [(8, 1), (11, 1), (12, 1), (17, 1), (18, 1), (19, 1), (20, 1)],\n",
      " [(21, 1), (22, 1), (23, 1), (24, 1), (25, 1)],\n",
      " [(24, 1), (26, 1), (27, 1), (28, 1)],\n",
      " [(24, 1), (26, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)],\n",
      " [(9, 1), (26, 1), (30, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# convert text to vectors using the dictionary function\n",
    "\n",
    "# define dictionary of our corpus. Contains the word frequency of each token in the whole corpus\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "\n",
    "# transform the corpus to vectors. Each vector consists of a token ID and the token frequency (taken from the dictionary)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "\n",
    "print(dictionary)\n",
    "print(dictionary.token2id)\n",
    "pprint.pprint(bow_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a TF-IDF model of the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# transform the corpus\n",
    "corpus_tfidf = tfidf[bow_corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.4301019571350565),\n",
      " (1, 0.4301019571350565),\n",
      " (2, 0.2944198962221451),\n",
      " (3, 0.2944198962221451),\n",
      " (4, 0.2944198962221451),\n",
      " (5, 0.4301019571350565),\n",
      " (6, 0.4301019571350565)]\n",
      "[(2, 0.3726494271826947),\n",
      " (7, 0.5443832091958983),\n",
      " (8, 0.3726494271826947),\n",
      " (9, 0.3726494271826947),\n",
      " (10, 0.27219160459794917),\n",
      " (11, 0.3726494271826947),\n",
      " (12, 0.27219160459794917)]\n",
      "[(4, 0.438482464916089),\n",
      " (10, 0.32027755044706185),\n",
      " (12, 0.32027755044706185),\n",
      " (13, 0.438482464916089),\n",
      " (14, 0.6405551008941237)]\n",
      "[(3, 0.3449874408519962),\n",
      " (10, 0.5039733231394895),\n",
      " (13, 0.3449874408519962),\n",
      " (15, 0.5039733231394895),\n",
      " (16, 0.5039733231394895)]\n",
      "[(8, 0.30055933182961736),\n",
      " (11, 0.30055933182961736),\n",
      " (12, 0.21953536176370683),\n",
      " (17, 0.43907072352741366),\n",
      " (18, 0.43907072352741366),\n",
      " (19, 0.43907072352741366),\n",
      " (20, 0.43907072352741366)]\n",
      "[(21, 0.48507125007266594),\n",
      " (22, 0.48507125007266594),\n",
      " (23, 0.48507125007266594),\n",
      " (24, 0.24253562503633297),\n",
      " (25, 0.48507125007266594)]\n",
      "[(24, 0.31622776601683794),\n",
      " (26, 0.31622776601683794),\n",
      " (27, 0.6324555320336759),\n",
      " (28, 0.6324555320336759)]\n",
      "[(24, 0.20466057569885868),\n",
      " (26, 0.20466057569885868),\n",
      " (29, 0.40932115139771735),\n",
      " (30, 0.2801947048062438),\n",
      " (31, 0.40932115139771735),\n",
      " (32, 0.40932115139771735),\n",
      " (33, 0.40932115139771735),\n",
      " (34, 0.40932115139771735)]\n",
      "[(9, 0.6282580468670046), (26, 0.45889394536615247), (30, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "# every word is now represented by a vector: Token-ID and token-weight\n",
    "for doc in corpus_tfidf:\n",
    "    pprint.pprint(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an index for the corpus\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0), (1, 0.12172779), (2, 0.14323246), (3, 0.67615116), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "# add a query document\n",
    "query_document = 'system engineering'.split()\n",
    "\n",
    "# transform the query document to a vector\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "\n",
    "# compare the query document to each document in the corpus\n",
    "sims = index[tfidf[query_bow]]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.67615116\n",
      "2 0.14323246\n",
      "1 0.12172779\n",
      "0 0.0\n",
      "4 0.0\n",
      "5 0.0\n",
      "6 0.0\n",
      "7 0.0\n",
      "8 0.0\n"
     ]
    }
   ],
   "source": [
    "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(document_number, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Streaming\n",
    "\n",
    "Since we do not want to load the whole corpus into memory every time, we need some changes in our approach, so we are able to stream the single documents when needed.\n",
    "Code is from: https://radimrehurek.com/gensim/auto_examples/core/run_corpora_and_vector_spaces.html#corpus-streaming-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the preprocessing file\n",
    "preprocessing_file = 'data/corpus/deu_mixed-typical_2011_10K/deu_mixed-typical_2011_10K-sentences.txt'\n",
    "# preprocessing_file = 'data/corpus/deu_mixed-typical_2011_1M/deu_mixed-typical_2011_1M-sentences.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove IDs from the corpus, since they do not belong in the content of the documents.\n",
    "temp_input_file = open(preprocessing_file, 'r', encoding = \"utf-8\")\n",
    "output_file = open(os.path.splitext(preprocessing_file)[0]+\"_only.txt\", 'w', encoding = \"utf-8\")\n",
    "for line in temp_input_file.readlines():\n",
    "    output_file.write(\" \".join(line.split()[1:])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try do create an index\n",
    "temp_input_file = open(preprocessing_file, 'r', encoding = \"utf-8\")\n",
    "index_dic = {}\n",
    "for line in temp_input_file.readlines():\n",
    "    index_dic[int(line.split()[0])] = str(line.split()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input file for further preprocessing\n",
    "input_file = 'data/corpus/deu_mixed-typical_2011_10K/deu_mixed-typical_2011_10K-sentences_only.txt'\n",
    "# input_file = 'data/corpus/deu_mixed-typical_2011_1M/deu_mixed-typical_2011_1M-sentences_only.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save corpus as a list, so we can print out the document at the end. Not ideal for performance, but currently no other solution.\n",
    "corpus_list = []\n",
    "for line in open(input_file, encoding = \"utf-8\"):\n",
    "    corpus_list.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stopwords from NLTK\n",
    "stop_words = set(stopwords.words('german'))\n",
    "\n",
    "# load all tokens as lowercase text\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in open(input_file, encoding=\"utf-8\"))\n",
    "\n",
    "# find all stop words\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stop_words if stopword in dictionary.token2id]\n",
    "\n",
    "# find all words that only occur once\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "\n",
    "\n",
    "# Remove all stopwords and words that only occur once\n",
    "dictionary.filter_tokens(stop_ids + once_ids)\n",
    "\n",
    "# Since we filtered out some words, the ID count now has gaps. With .compactify we can remove those gaps\n",
    "dictionary.compactify()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the corpus as an object\n",
    "\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        # Each line in the corpus file represents one document, each token in a document is seperated by a whitespace\n",
    "        for line in open(input_file):\n",
    "            # Transfom the corpus to vectors\n",
    "            yield dictionary.doc2bow(line.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the corpus, without loading it into memory\n",
    "corpus = MyCorpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The corpus now exists only as an object, to work with it, the vectors inside have to be called. Only then they will be loaded into memory.\n",
    "Example: Print the first 10 document vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1)]\n",
      "[(2, 1), (3, 1)]\n",
      "[(3, 1), (4, 1), (5, 1)]\n",
      "[(7, 1), (8, 1)]\n",
      "[(3, 1), (9, 1)]\n",
      "[(3, 1)]\n",
      "[(3, 1), (10, 1), (11, 1)]\n",
      "[(12, 1), (13, 1)]\n",
      "[(14, 1), (15, 1), (16, 1)]\n",
      "[(3, 1), (9, 1), (17, 1), (18, 1)]\n"
     ]
    }
   ],
   "source": [
    "for index, vector in enumerate(corpus):\n",
    "    if index <10:\n",
    "        print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with our Corpus\n",
    "\n",
    "Now that we are able to load our corpus memory friendly we can transform the document vectors using a variety of functions.\n",
    "\n",
    "First we have to initialize a model, which will be used for the transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Model:\n",
    "tfidf_model = models.TfidfModel(corpus, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI (Latent Semantic Indexing) Model:\n",
    "tfidf_corpus = tfidf_model[corpus]\n",
    "lsi_model = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Model:\n",
    "lda_model = models.LdaModel(corpus, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_model.print_topics(num_topics=10, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = similarities.SparseMatrixSimilarity(tfidf_model[corpus], num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an example which should be queried against the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Sentence:\n",
    "example = \"Die bisherigen Bau-Planungen bezüglich des Klinikums sind sehr verändert.\"\n",
    "\n",
    "# Transform example to vector\n",
    "example_vec = dictionary.doc2bow(example.lower().split())\n",
    "\n",
    "# Convert it to our current model space\n",
    "example_lda = tfidf_model[example_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(954, 1), (1241, 1), (1730, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(example_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = index[example_vec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Die', 'Biografie', 'eines', 'Songs\".'] 0.40824828\n",
      "['Eine', 'Übersicht', 'der', 'Kindertageseinrichtungen', 'in', 'Rodgau.'] 0.40824828\n",
      "['Bei', 'den', 'beteiligten', 'Fahrzeugen', 'wurden', 'jeweils', 'die', 'Außenspiegel', 'beschädigt.'] 0.25779927\n"
     ]
    }
   ],
   "source": [
    "for document_number, score in sorted(enumerate(sim), key=lambda x: x[1], reverse=True):\n",
    "    if score != 0.0:\n",
    "        print(index_dic[document_number], score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die bisherigen PCMs brachten es nur auf 128 MBit.\n",
      " 0.40824828\n",
      "Eine umfassende Chronik der bisherigen Ereignisse!\n",
      " 0.40824828\n",
      "Bei den bisherigen WM-Auktionen wurden insgesamt 170 000 Euro erzielt.\n",
      " 0.25779927\n"
     ]
    }
   ],
   "source": [
    "for document_number, score in sorted(enumerate(sim), key=lambda x: x[1], reverse=True):\n",
    "    if score != 0.0:\n",
    "        print(corpus_list[document_number], score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different results for index_dic vs document list. document_number from gensim is not the same as the index number?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Corpus\n",
    "\n",
    "Corpus from: https://dumps.wikimedia.org/dewiki/20200820/\n",
    "\n",
    "Sentences for comparison from: https://github.com/t-systems-on-site-services-gmbh/german-wikipedia-text-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from xml.etree.ElementTree import *\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "import os\n",
    "import pprint\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import LdaMulticore\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from smart_open import open \n",
    "import spacy\n",
    "import de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the language model from spacy\n",
    "spacy_data = de_core_news_sm.load()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # load and tokenize text\n",
    "    prep_text = spacy_data(text)\n",
    "    # list for tokens\n",
    "    prep_tokens = []\n",
    "    # for every token in text\n",
    "    for token in prep_text:\n",
    "        # remove stopwords and punctuatiuon\n",
    "        if token.pos_ != 'PUNCT' and token.is_stop == False:\n",
    "            # lemmatize and transform to lowercase\n",
    "            lemma_token = token.lemma_.lower()\n",
    "            # remove non-alphabetic tokens\n",
    "            if lemma_token.isalpha() or lemma_token == '-PRON-':\n",
    "                prep_tokens.append(lemma_token)\n",
    "    # return preprocessed text \n",
    "    return prep_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a corpus from the text contents of the XML file:\n",
    "\n",
    "First test:\n",
    "\n",
    "Print text from \\<text>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_file = \"data/wiki_corpus/dewiki-20200820-pages-articles-multistream.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alan', 'smithee', 'stehen', 'pseudonym', 'fiktiv', 'regisseur', 'film', 'verantworten', 'eigentlich', 'regisseur', 'name', 'werk', 'verbindung', 'bringen', 'directors', 'guild', 'of', 'america', 'dga', 'situation', 'empfehlen', 'seither', 'thoma', 'angeles', 'name', 'of', 'director', 'smithee', 'what', 'it', 'used', 'to', 'be', 'zuletzt', 'prüfen', 'april', 'alan', 'smithee', 'weiterhin', 'gebrauch', 'alternative', 'schreibweise', 'ursprungsvariante', 'smithee', 'alan', 'smithee', 'teilweise', 'asiatisch', 'anmutend', 'schreibweise', 'alan', 'smi', 'thee', 'sumishii', 'aran', 'gehören', 'internet', 'movie', 'database', 'name', 'eintrag', 'alan', 'smithee', 'geschichte', 'entstehung', 'pseudonym', 'entstehen', 'infolge', 'arbeit', 'death', 'of', 'gunfighter', 'deutsch', 'titel', 'frank', 'patch', 'stunde', 'zählen', 'regisseur', 'robert', 'totten', 'hauptdarsteller', 'richard', 'widmark', 'geraten', 'streit', 'woraufhin', 'don', 'siegel', 'neu', 'regisseur', 'einsetzen', 'film', 'tragen', 'abschluss', 'arbeit', 'deutlich', 'tottens', 'manier', 'drehtage', 'siegel', 'arbeiten', 'weshalb', 'nennung', 'name', 'regisseur', 'ablehnen', 'totten', 'lehnen', 'ebenfalls', 'lösung', 'smithee', 'möglichst', 'einzigartig', 'name', 'name', 'bio', 'biography', 'for', 'alan', 'smithee', 'internet', 'movie', 'zeitgenössisch', 'kritik', 'regisseur', 'roger', 'ebert', 'worten', 'loben', 'smithee', 'name', 'i', 'm', 'not', 'familiar', 'with', 'allows', 'his', 'story', 'to', 'unfold', 'naturally', 'he', 'never', 'preaches', 'and', 'he', 'never', 'lingers', 'on', 'the', 'obvious', 'his', 'characters', 'do', 'what', 'they', 'have', 'to', 'death', 'of', 'gunfighter', 'zuletzt', 'prüfen', 'april', 'regisseur', 'alan', 'smithee', 'name', 'vertrauen', 'erlauben', 'handlung', 'entfalten', 'predigen', 'niemals', 'verweilen', 'offensichtliche', 'charakter', 'aufdeckung', 'abkehr', 'parodie', 'alan', 'smithee', 'film', 'burn', 'hollywood', 'burn', 'deutsch', 'titel', 'fahr', 'hölle', 'hollywood', 'kino', 'pseudonym', 'groß', 'publikum', 'zuletzt', 'arthur', 'hiller', 'hiller', 'eigentlich', 'regisseur', 'film', 'name', 'zurückziehen', 'analog', 'filmtitel', 'pseudonym', 'alan', 'smithee', 'benutzen', 'film', 'gelten', 'schlecht', 'film', 'gewinnen', 'goldene', 'film', 'supernova', 'führen', 'gewiß', 'thoma', 'lee', 'alias', 'walter', 'hill', 'regie', 'finden', 'siehe', 'kindermädchen', 'smithee', 'supernova', 'sichten', 'film', 'namens', 'the', 'guardian', 'verwendung', 'verwendung', 'pseudonyms', 'mitglied', 'dga', 'strengen', 'reglementiert', 'regisseur', 'gedreht', 'film', 'name', 'hergeben', 'sichtung', 'fertig', 'film', 'anzeigen', 'pseudonym', 'verwenden', 'rat', 'dga', 'entscheiden', 'binnen', 'anliegen', 'erhebt', 'produktionsfirma', 'einspruch', 'entscheiden', 'komitee', 'mitglied', 'dga', 'vereinigung', 'fernsehproduzenten', 'regisseur', 'pseudonym', 'angeben', 'beantragung', 'regisseur', 'stillschweigen', 'halten', 'fertig', 'film', 'öffentlich', 'kritisieren', 'dga', 'verwendung', 'pseudonyms', 'siehe', 'regelung', 'artikel', 'abschnitt', 'basic', 'agreement', 'pdf', 'dga', 'abrufen', 'april', 'antrag', 'regisseur', 'pseudonymisierung', 'ablehnen', 'tony', 'kaye', 'kaye', 'name', 'smithee', 'film', 'american', 'history', 'x', 'einsetzen', 'obwohl', 'antrag', 'stellen', 'produktion', 'name', 'verwenden', 'pilotfilm', 'fernsehserie', 'schulmädchen', 'senden', 'ard', 'august', 'zweiteilig', 'paparazzo', 'werk', 'erscheinen', 'anstatt', 'eigentlich', 'regisseur', 'stephan', 'wagner', 'wagner', 'alan', 'smithee', 'abspann', 'regisseur', 'pseudonym', 'benutzen', 'don', 'siegel', 'robert', 'totten', 'frank', 'patch', 'stunde', 'zählen', 'david', 'lynch', 'dreistündige', 'fernsehfassung', 'wüstenplanet', 'wüstenplanet', 'chris', 'christensen', 'the', 'omega', 'imperativ', 'gianni', 'bozzacchi', 'i', 'love', 'stuart', 'rosenberg', 'let', 's', 'get', 'harry', 'richard', 'sarafian', 'starfire', 'dennis', 'hopper', 'catchfire', 'arthur', 'hiller', 'hiller', 'fahr', 'hölle', 'hollywood', 'rick', 'rosenthal', 'vogel', 'ii', 'rückkehr', 'kevin', 'yagher', 'hellraiser', 'iv', 'bloodline', 'pilotfilm', 'serie', 'macgyver', 'folge', 'staffel', 'fahren', 'alan', 'smithee', 'regisseur', 'tv', 'rage', 'jerrold', 'freedman', 'regisseur', 'pilotfilms', 'angeben', 'regisseur', 'folge', 'unbekannt', 'drehbuchautoren', 'pseudonym', 'benutzen', 'gehören', 'sam', 'raimi', 'ivan', 'raimi', 'drehbuch', 'total', 'beknackt', 'nuß', 'alan', 'smithee', 'alan', 'smithee', 'sr', 'schreiben', 'computerspielen', 'pseudonym', 'angeben', 'abspann', 'marine', 'sharpshooter', 'iv', 'art', 'director', 'spiel', 'alan', 'smithee', 'external', 'showtopicalbumbackground', 'produzieren', 'new', 'york', 'yorker', 'big', 'dance', 'theater', 'alan', 'smithee', 'directed', 'this', 'play', 'august', 'jahr', 'berlin', 'tanz', 'august', 'aufführen', 'smithee', 'schuld', 'frankfurter', 'allgemeine', 'sonntagszeitung', 'august', 'seite', 'literatur', 'jeremy', 'braddock', 'stephen', 'hock', 'directed', 'by', 'smithee', 'foreword', 'by', 'andrew', 'sarris', 'university', 'of', 'minnesota', 'press', 'minneapolis', 'london', 'isbn', 'weblinks', 'artikel', 'smithee', 'abc', 'online', 'englisch', 'mann', 'niemals', 'leben', 'spiegel', 'online', 'einestages', 'alan', 'smithee', 'leben', 'dradio', 'wissen', 'einzelnachweise', 'references', 'kategorie', 'fiktiv', 'alan', 'kategorie', 'pseudonym', 'kategorie', 'alan', 'kategorie', 'werk', 'alan', 'smithee']\n",
      "['infobox', 'chemisches', 'element', 'periodensystem', 'name', 'actinium', 'symbol', 'ac', 'ordnungszahl', 'serie', 'üm', 'gruppe', 'periode', 'block', 'd', 'allgemein', 'aussehen', 'silbrig', 'cas', 'massenanteil', 'ref', 'binder', 'lexikon', 'chemisch', 'element', 'hirzel', 'verlag', 'stuttgart', 'isbn', 'atomar', 'hauptquelle', 'ref', 'wert', 'eigenschaft', 'infobox', 'angeben', 'actinium', 'atommasse', 'atomradius', 'atomradiusberechnet', 'kovalenterradius', 'vanderwaalsradius', 'elektronenkonfiguration', 'austrittsarbeit', 'ev', 'ref', 'kj', 'mol', 'ref', 'ev', 'ref', 'kj', 'mol', 'ref', 'ev', 'ref', 'kj', 'mol', 'ref', 'ev', 'ref', 'kj', 'mol', 'ref', 'physikalisch', 'aggregatzustand', 'fest', 'modifikation', 'kristallstruktur', 'kubisches', 'flächenzentriert', 'dicht', 'g', 'cm', 'mohshärte', 'magnetismus', 'molaresvolumen', 'verdampfungswärme', 'kj', 'mol', 'schmelzwärme', 'dampfdruck', 'schallgeschwindigkeit', 'spezifischewärmekapazität', 'elektrischeleitfähigkeit', 'wärmeleitfähigkeit', 'chemisch', 'oxidationszustände', 'normalpotential', 'ac', 'elektronegativität', 'quelle', 'nv', 'h', 'euh', 'p', 'quelle', 'p', 'radioaktiv', 'isotope', 'infobox', 'chemisches', 'element', 'isotop', 'massenzahl', 'symbol', 'ac', 'nh', 'halbwertszeit', 'anzahlzerfallstypen', 'ra', 'fr', 'th', 'infobox', 'chemisches', 'element', 'isotop', 'massenzahl', 'symbol', 'ac', 'nh', 'halbwertszeit', 'anzahlzerfallstypen', 'fr', 'infobox', 'chemisches', 'element', 'isotop', 'massenzahl', 'symbol', 'ac', 'nh', 'halbwertszeit', 'anzahlzerfallstypen', 'th', 'ra', 'fr', 'infobox', 'chemisches', 'element', 'isotop', 'massenzahl', 'symbol', 'ac', 'nh', 'halbwertszeit', 'anzahlzerfallstypen', 'th', 'fr', 'infobox', 'chemisches', 'element', 'isotop', 'massenzahl', 'symbol', 'ac', 'nh', 'halbwertszeit', 'h', 'anzahlzerfallstypen', 'th', 'nmreigenschaften', 'actinium', 'chemisch', 'element', 'elementsymbol', 'ac', 'ordnungszahl', 'periodensystem', 'element', 'stehen', 'scandiumgruppe', 'element', 'hören', 'block', 'namensgeber', 'gruppe', 'actinoide', 'folgend', 'element', 'geschichte', 'datei', 'periodensystem', 'periodensystem', 'lücke', 'actinium', 'unter', 'rand', 'thorium', 'actinium', 'französisch', 'chemiker', 'debierne', 'entdecken', 'pechblende', 'isolieren', 'ähnlichkeit', 'titan', 'debierne', 'sur', 'une', 'nouvelle', 'matière', 'comptes', 'rendus', 'gallica', 'debierne', 'sur', 'un', 'nouvel', 'élément', 'l', 'actinium', 'comptes', 'rendus', 'gallica', 'zuschreiben', 'bezeichnung', 'leiten', 'radioaktivität', 'griechisch', 'ἀκτίς', 'aktís', 'strahl', 'figurowski', 'entdeckung', 'chemisch', 'element', 'ursprung', 'name', 'deutsch', 'übersetzung', 'leo', 'korniljew', 'ernst', 'lemke', 'moskau', 'isbn', 'friedrich', 'giesel', 'entdecken', 'element', 'unabhängig', 'friedrich', 'oskar', 'giesel', 'ueber', 'radium', 'radioactive', 'stoff', 'bericht', 'deutsche', 'chemische', 'gesellschaft', 'beschreiben', 'ähnlichkeit', 'lanthan', 'name', 'friedrich', 'oskar', 'giesel', 'ueber', 'emanationskörper', 'emanium', 'bericht', 'deutsche', 'chemische', 'gesellschaft', 'bildung', 'lateinisch', 'emano', 'ausfließen', 'ebenfalls', 'bezug', 'abgegeben', 'actinium', 'emanium', 'identisch', 'erkennen', 'debiernes', 'namensgebung', 'vorzug', 'geben', 'entdecken', 'friedrich', 'oskar', 'giesel', 'ueber', 'emanium', 'bericht', 'deutsche', 'chemische', 'gesellschaft', 'geschichte', 'entdeckung', 'publikation', 'kirby', 'the', 'discovery', 'of', 'actinium', 'isis', 'zeitschrift', 'adloff', 'the', 'centenary', 'of', 'controversial', 'discovery', 'actinium', 'radiochimica', 'acta', 'fraglich', 'beschreiben', 'zeigen', 'publikation', 'einerseits', 'andererseits', 'widerspruch', 'aufweisen', 'gewinnung', 'darstellung', 'actinium', 'vorhanden', 'spielen', 'quelle', 'rolle', 'gewinnung', 'technisch', 'isotop', 'ac', 'bestrahlung', 'ra', 'herstellen', 'zeitangaben', 'schnell', 'zerfall', 'actiniums', 'stets', 'gering', 'menge', 'verfügbar', 'künstliche', 'herstellung', 'actinium', 'argonne', 'national', 'laboratory', 'chicago', 'durchführen', 'eigenschaft', 'physikalische', 'eigenschaft', 'metall', 'silberweiß', 'glänzen', 'ref', 'relativ', 'frederick', 'seitz', 'david', 'turnbull', 'solid', 'state', 'physics', 'advances', 'research', 'and', 'applications', 'academic', 'press', 'isbn', 'google', 'aufgrund', 'stark', 'radioaktivität', 'leuchten', 'actinium', 'dunkel', 'hellblauen', 'actinium', 'namensgebende', 'element', 'ähnlich', 'lanthan', 'gruppe', 'element', 'zeigen', 'deutlich', 'unterschied', 'lanthanoide', 'dauern', 'glenn', 'seaborg', 'wichtig', 'änderung', 'periodensystem', 'mendelejew', 'vorschlagen', 'einführung', 'glenn', 'seaborg', 'the', 'transuranium', 'element', 'science', 'pmid', 'chemisch', 'eigenschaft', 'reaktionsfähig', 'luft', 'wasser', 'angreifen', 'überziehen', 'schicht', 'actiniumoxid', 'wodurch', 'weit', 'oxidation', 'schützen', 'stites', 'murrell', 'salutsky', 'bob', 'stone', 'preparation', 'of', 'actinium', 'metal', 'chem', 'soc', 'ac', 'farblos', 'chemische', 'verhalten', 'actinium', 'ähneln', 'lanthan', 'actinium', 'bekannt', 'verbindung', 'katz', 'manning', 'chemistry', 'of', 'the', 'actinide', 'element', 'annual', 'review', 'of', 'nuclear', 'science', 'isotope', 'wovon', 'vorkommen', 'langlebig', 'isotop', 'ac', 'halbwertszeit', 'ac', 'zerfallsprodukt', 'uranisotops', 'u', 'uranerzen', 'lassen', 'wägbare', 'menge', 'ac', 'gewinnen', 'somit', 'verhältnismäßig', 'einfach', 'studium', 'element', 'ermöglichen', 'radioaktiv', 'zerfallsprodukten', 'befinden', 'aufwändig', 'strahlenschutzvorkehrungen', 'nötigen', 'verwendung', 'actinium', 'erzeugung', 'neutronen', 'einsetzen', 'rolle', 'spielen', 'energieumwandlung', 'nutzen', 'dual', 'zerfall', 'ac', 'groß', 'emission', 'th', 'zerfallen', 'francium', 'fr', 'lösung', 'ac', 'quelle', 'kurzlebig', 'fr', 'verwendbar', 'letzteres', 'regelmäßig', 'abtrennen', 'untersuchen', 'sicherheitshinweise', 'einstufung', 'verordnung', 'eg', 'nummer', 'liegen', 'chemische', 'gefährlichkeit', 'umfassen', 'völlig', 'untergeordnet', 'rolle', 'radioaktivität', 'beruhend', 'gefahr', 'spielen', 'letzteres', 'gelten', 'relevante', 'stoffmenge', 'handeln', 'verbindung', 'gering', 'anzahl', 'actiniumverbindungen', 'ausnahme', 'acpo', 'entsprechend', 'lanthanverbindungen', 'ähnlich', 'enthalten', 'actinium', 'oxidationsstufe', 'fried', 'french', 'hagemann', 'zachariasen', 'the', 'preparation', 'and', 'identification', 'of', 'some', 'pure', 'actinium', 'compounds', 'chem', 'soc', 'insbesondere', 'unterscheiden', 'gitterkonstanten', 'jeweilig', 'wenig', 'center', 'formel', 'farbe', 'symmetrie', 'raumgruppe', 'pm', 'b', 'pm', 'c', 'pm', 'z', 'cm', 'ac', 'silber', 'farr', 'giorgi', 'bowman', 'money', 'the', 'crystal', 'structure', 'of', 'actinium', 'metal', 'and', 'actinium', 'hydride', 'journal', 'of', 'inorganic', 'and', 'nuclear', 'chemistry', 'kubisch', 'ref', 'ac', 'o', 'weiß', 'ref', 'trigonal', 'ref', 'zachariasen', 'crystal', 'chemical', 'studies', 'of', 'the', 'of', 'element', 'xii', 'new', 'compounds', 'representing', 'known', 'structure', 'types', 'acta', 'crystallographica', 'ac', 's', 'kubisch', 'ref', 'zachariasen', 'crystal', 'chemical', 'studies', 'of', 'the', 'of', 'element', 'vi', 'the', 'ce', 's', 's', 'typ', 'of', 'structure', 'acta', 'crystallographica', 'acf', 'weiß', 'ref', 'meyer', 'lester', 'morss', 'synthesis', 'of', 'lanthanide', 'and', 'actinide', 'compounds', 'springer', 'isbn', 'google', 'hexagonal', 'ref', 'accl', 'hexagonal', 'ref', 'zachariasen', 'crystal', 'chemical', 'studies', 'of', 'the', 'of', 'element', 'new', 'structure', 'types', 'acta', 'crystallographica', 'acbr', 'weiß', 'ref', 'hexagonal', 'ref', 'acof', 'weiß', 'ref', 'meyer', 'lester', 'morss', 'synthesis', 'of', 'lanthanide', 'and', 'actinide', 'compounds', 'springer', 'isbn', 'google', 'kubisch', 'ref', 'acocl', 'tetragonal', 'ref', 'acobr', 'tetragonal', 'ref', 'acpo', 'h', 'o', 'hexagonal', 'ref', 'oxide', 'actinium', 'o', 'erhitzen', 'actinium', 'c', 'actinium', 'c', 'vakuum', 'erhalten', 'kristallgitter', 'isotyp', 'oxiden', 'meist', 'dreiwertigen', 'halogenide', 'actinium', 'lösung', 'feststoffreaktion', 'darstellen', 'fall', 'raumtemperatur', 'flusssäure', 'ac', 'fällen', 'produkt', 'fall', 'fluorwasserstoff', 'c', 'platinapparatur', 'behandeln', 'actinium', 'umsetzung', 'actiniumhydroxid', 'tetrachlormethan', 'temperatur', 'oberhalb', 'c', 'reaktion', 'aluminiumbromid', 'actinium', 'führen', 'actinium', 'behandlung', 'feucht', 'ammoniak', 'c', 'führen', 'oxibromid', 'verbindung', 'po', 'lösung', 'actinium', 'salzsäure', 'erhalten', 'weiß', 'gefärbt', 'o', 'erhitzen', 'actinium', 'schwefelwasserstoff', 'c', 'paar', 'minute', 'führen', 'schwarz', 'actinium', 's', 'literatur', 'harold', 'kirby', 'lester', 'morss', 'actinium', 'lester', 'morss', 'norman', 'edelstein', 'jean', 'fuger', 'the', 'chemistry', 'of', 'the', 'actinide', 'and', 'transactinide', 'element', 'springer', 'dordrecht', 'isbn', 'weblinks', 'wiktionary', 'commonscat', 'römpponline', 'actinium', 'januar', 'einzelnachweise', 'references', 'navigationsleiste', 'periodensystem', 'sh']\n",
      "['datei', 'ang', 'lee', 'festival', 'de', 'venise', 'mostra', 'lee', 'ang', 'lee', 'lǐ', 'ān', 'oktober', 'chaozhou', 'news', 'taiwan', 'archives', 'family', 'and', 'friends', 'praise', 'ang', 'quiet', 'dedication', 'taipei', 'times', 'landkreis', 'pingtung', 'republik', 'china', 'taiwan', 'taiwanischer', 'filmregisseur', 'drehbuchautor', 'watch', '戰爭人性與電影科技', 'chung', 't', 'ien', 'vielfach', 'ausgezeichnet', 'regisseur', 'unterschiedlich', 'film', 'eat', 'drink', 'woman', 'sinn', 'sinnlichkeit', 'sinnlichkeit', 'martial', 'tiger', 'and', 'dragon', 'film', 'brokeback', 'mountain', 'life', 'of', 'pi', 'schiffbruch', 'tiger', 'jeweils', 'oscar', 'kategorie', 'beste', 'regie', 'auszeichnen', 'leben', 'ang', 'lee', 'republik', 'china', 'gebären', 'eltern', 'emigrant', 'volksrepublik', 'lernen', 'taiwan', 'kennen', 'lee', 'alt', 'sohn', 'großeltern', 'mütterlicherseits', 'zug', 'kommunistisch', 'revolution', 'china', 'ums', 'leben', 'kommen', 'vater', 'lehrer', 'häufig', 'arbeitsstelle', 'wechseln', 'wachsen', 'ang', 'lee', 'verschieden', 'stadt', 'taiwan', 'entgegen', 'wunsch', 'eltern', 'vater', 'klassische', 'akademische', 'laufbahn', 'einschlagen', 'interessieren', 'lee', 'schauspiel', 'absolvieren', 'einverständnis', 'filmstudium', 'taipeh', 'anschluss', 'vereinigt', 'staat', 'university', 'of', 'illinois', 'urbana', 'theaterwissenschaft', 'studieren', 'erwerb', 'illinois', 'verlegen', 'studium', 'theaterproduktion', 'new', 'york', 'new', 'york', 'master', 'abschloss', 'entschloss', 'ebenfalls', 'taiwan', 'stammend', 'ehefrau', 'usa', 'bleiben', 'interesse', 'verschieben', 'trotzen', 'erfahrung', 'super', 'taiwan', 'ref', 'pekler', 'andrea', 'ungerböck', 'ang', 'lee', 'film', 'schüren', 'marburg', 'spät', 'filmregie', 'lee', 'berufswunsch', 'familie', 'insbesondere', 'vater', 'eingestehen', 'schneider', 'chronik', 'leben', 'werk', 'ang', 'lee', 'kino', 'poesie', 'grossformat', 'februar', 'studium', 'projekt', 'umsetzen', 'langfilm', 'fertigstellen', 'zeichnen', 'kontinuierliche', 'karriere', 'regisseur', 'groß', 'erfolg', 'sowohl', 'publikum', 'kritik', 'gelten', 'martial', 'tiger', 'and', 'dragon', 'starbesetzung', 'brokeback', 'mountain', 'heath', 'ledger', 'jake', 'gyllenhaal', 'letztere', 'bekommen', 'lee', 'regisseur', 'oscar', 'gut', 'regie', 'lee', 'film', 'preis', 'mittlerweile', 'goldener', 'bär', 'internationale', 'filmfestspiele', 'goldener', 'löwe', 'internationale', 'filmfestspiele', 'venedig', 'auszeichnen', 'lee', 'mikrobiologin', 'jane', 'lin', 'verheiraten', 'leben', 'white', 'plains', 'new', 'plains', 'westchester', 'county', 'bundesstaat', 'new', 'york', 'york', 'ehe', 'stammen', 'sohn', 'haan', 'mason', 'ang', 'lee', 'besitzen', 'united', 'states', 'permanent', 'resident', 'filmisches', 'werk', 'filmerfahrungen', 'taiwan', 'setzen', 'lee', 'studium', 'usa', 'ernsthaft', 'filmemachen', 'auseinander', 'rahmen', 'studium', 'new', 'york', 'drehen', 'kurzfilme', 'wirken', 'abschlussdreh', 'studienkollegen', 'spike', 'lee', 'regieassistent', 'abschlussfilm', 'fine', 'line', 'gewinnen', 'preis', 'renommiert', 'filmfest', 'gelingen', 'gewinn', 'hochdotierten', 'drehbuchwettbewerbs', 'taiwan', 'reihe', 'film', 'drehen', 'konflikt', 'taiwanischer', 'familie', 'thema', 'langfilme', 'lee', 'realisieren', 'begriff', 'father', 'knows', 'best', 'michael', 'pekler', 'andrea', 'ungerböck', 'merken', 'ironisch', 'trilogie', 'father', 'thinks', 'he', 'knows', 'best', 'titulieren', 'können', 'patriarchen', 'keineswegs', 'lösung', 'vergleiche', 'michael', 'pekler', 'andrea', 'ungerböck', 'ang', 'lee', 'film', 'schüren', 'marburg', 'bezeichnung', 'wiederkehrend', 'figur', 'chinesisch', 'familienoberhaupts', 'spielen', 'jeweils', 'taiwanischen', 'schauspieler', 'sihung', 'lung', 'film', 'thematisieren', 'öfter', 'ang', 'lee', 'familiär', 'problem', 'konflikt', 'selbstbestimmung', 'tradition', 'innen', 'außen', 'ost', 'west', 'generation', 'herrühren', 'film', 'allesamt', 'koproduktionen', 'bislang', 'folgend', 'projekt', 'handeln', 'film', 'lee', 'adaptionen', 'film', 'geschrieben', 'originaldrehbüchern', 'film', 'schiebende', 'hand', 'handeln', 'einzug', 'chinesisch', 'vater', 'erwachsen', 'sohn', 'schwiegertochter', 'new', 'york', 'interkulturell', 'problem', 'wohngemeinschaft', 'entstehen', 'zusammenarbeit', 'lee', 'drehbuchautor', 'produzent', 'james', 'schamus', 'bilden', 'film', 'lee', 'eng', 'arbeitsgemeinschaft', 'folgend', 'film', 'schreiben', 'gemeinsam', 'hochzeitsbankett', 'zeichnen', 'zusätzlich', 'neil', 'peng', 'eat', 'drink', 'woman', 'wang', 'film', 'lee', 'ausnahme', 'kurzfilms', 'chosen', 'hire', 'chosen', 'schamus', 'seither', 'entscheidend', 'funktion', 'ausüben', 'regelmäßig', 'zusammenarbeit', 'filmeditor', 'tim', 'squyres', 'lee', 'erstling', 'anfang', 'ausnahme', 'erfolgsfilms', 'brokeback', 'mountain', 'squires', 'film', 'ang', 'lee', 'drehen', 'schneiden', 'erfolg', 'erstlings', 'lee', 'nächstes', 'hochzeitsbankett', 'drehen', 'komödie', 'fingieren', 'eheschließung', 'homosexuell', 'usa', 'erneut', 'tauchen', 'figur', 'streng', 'weise', 'familienoberhaupts', 'schiebende', 'hand', 'taiwan', 'aufmerksamkeit', 'preis', 'sorgen', 'langfilm', 'lee', 'europa', 'aufstrebend', 'regisseur', 'film', 'erhalten', 'internationale', 'filmfestspiele', 'goldene', 'bär', 'bester', 'fremdsprachig', 'film', 'zudem', 'oscar', 'nominieren', 'gelten', 'hinaus', 'profitabel', 'jahr', 'million', 'produktionskosten', 'erzielen', 'einspielergebnis', 'isabell', 'gössele', 'kino', 'ang', 'lee', 'atem', 'verborgen', 'drache', 'tectum', 'marburg', 'sihung', 'lung', 'letzt', 'trilogie', 'eat', 'drink', 'woman', 'kongeniale', 'verkörperung', 'chinesisch', 'pekler', 'ungerböck', 'zentrum', 'maskeraden', 'alt', 'gesicht', 'wahr', 'lernen', 'verlieren', 'neu', 'lebenstauglicheres', 'christoph', 'schneider', 'chronik', 'leben', 'werk', 'ang', 'lee', 'kino', 'poesie', 'grossformat', 'februar', 'mal', 'verwitwete', 'vater', 'dreier', 'tochter', 'leben', 'liebe', 'unterschiedlich', 'art', 'angehen', 'ebenfalls', 'innerfamiliäre', 'konflikt', 'klären', 'eat', 'drink', 'woman', 'vorgänger', 'taipeh', 'drehen', 'mittelpunkt', 'film', 'stehen', 'titel', 'deuten', 'lieben', 'essen', 'ang', 'lee', 'privat', 'passioniert', 'koch', 'legen', 'hierbei', 'wert', 'kulinarisch', 'komponente', 'stilmittel', 'ref', 'pekler', 'ungerböck', 'konzipieren', 'hauptfigur', 'alt', 'witwers', 'berühmt', 'koch', 'dreimal', 'geschichte', 'angebot', 'produzentin', 'lindsay', 'doran', 'britisch', 'schauspielerin', 'emma', 'thompson', 'verfasste', 'adaption', 'roman', 'verstand', 'gefühl', 'jane', 'austen', 'großbritannien', 'drehen', 'eröffnen', 'lee', 'ersehnen', 'perspektive', 'jenseits', 'asiatisch', 'geprägt', 'stoff', 'trilogie', 'setzen', 'unterschiedlich', 'kultur', 'auseinander', 'sinn', 'sinnlichkeit', 'sinnlichkeit', 'verfilmung', 'roman', 'englisch', 'schriftstellerin', 'jane', 'austen', 'eissturm', 'spielen', 'usa', 'ride', 'with', 'the', 'devil', 'bürgerkrieg', 'ansiedeln', 'pendel', 'west', 'ost', 'tiger', 'and', 'dragon', 'hulk', 'unterschiedlich', 'tiger', 'and', 'dragon', 'gewinnen', 'lee', 'golden', 'werk', 'awards', 'oscar', 'prämieren', 'trophäe', 'fremdsprachig', 'film', 'film', 'chlotrudis', 'award', 'auszeichnen', 'chlotrudis', 'erhalten', 'brokeback', 'mountain', 'brokeback', 'mountain', 'lee', 'vielzahl', 'filmpreisen', 'ehren', 'oscar', 'gut', 'regie', 'goldener', 'löwe', 'filmfestspiele', 'venedig', 'venedig', 'auszeichnung', 'hollywood', 'foreign', 'press', 'association', 'gut', 'regisseur', 'jahr', 'verfilmen', 'gefahr', 'begierde', 'kurzgeschichte', 'eileen', 'chang', 'thriller', 'spielen', 'weltkriegs', 'shanghai', 'handeln', 'jung', 'chinesisch', 'agentin', 'spielen', 'tang', 'wei', 'beauftragen', 'hochrangigen', 'verräter', 'tony', 'leung', 'chiu', 'wai', 'liquidieren', 'lee', 'chinesischsprachige', 'tiger', 'and', 'dragon', 'offiziell', 'wettbewerb', 'internationale', 'filmfestspiele', 'venedig', 'venedig', 'vertreten', 'bringen', 'erneut', 'goldene', 'löwe', 'selben', 'gefahr', 'begierde', 'offiziell', 'taiwanischer', 'beitrag', 'nominierung', 'oscar', 'bester', 'fremdsprachig', 'fremdsprachig', 'film', 'douglas', 'tseng', 'shorter', 'version', 'of', 'lust', 'to', 'be', 'shown', 'here', 'the', 'straits', 'times', 'singapore', 'september', 'life', 'life', 'empfehlung', 'academy', 'of', 'motion', 'picture', 'arts', 'and', 'sciences', 'zurückziehen', 'chen', 'lian', 'xi', 'url', 'title', 'titel', 'lian', 'xi', 'qu', 'movie', 'database', 'ersetzen', 'februar', 'geben', 'lee', 'jury', 'internationale', 'filmfestspiele', 'venedig', 'venedig', 'leiten', 'kultur', 'venedig', 'ang', 'lee', 'frankfurter', 'februar', 'abrufen', 'juni', 'monat', 'erhalten', 'komödie', 'taking', 'woodstock', 'einladung', 'wettbewerb', 'internationale', 'filmfestspiele', 'cannes', 'filmfestspiele', 'cannes', 'wettbewerbsjury', 'internationale', 'filmfestspiele', 'cannes', 'cannes', 'berufen', 'stil', 'ang', 'lee', 'international', 'anerkannt', 'erfolgreich', 'regisseur', 'gelten', 'vielseitig', 'filmemacher', 'letzt', 'häufig', 'behandeln', 'lee', 'film', 'thema', 'familie', 'art', 'weise', 'autobiographische', 'zug', 'leben', 'tragen', 'lässt', 'umgebung', 'bewusst', 'einwirken', 'bringen', 'film', 'idee', 'manchmal', 'auge', 'schließen', 'bild', 'sehen', 'bild', 'finden', 'ang', 'kennzeichnend', 'meist', 'film', 'geradlinige', 'erzählstruktur', 'charakter', 'geschichte', 'verschieden', 'blickwinkeln', 'darstellen', 'verknüpfen', 'konflikt', 'menschlich', 'leben', 'traditionell', 'innovativ', 'stilelementen', 'ang', 'lee', 'erzählstrukturen', 'langweilig', 'kombinieren', 'verschieden', 'genre', 'epoche', 'hoffen', 'alt', 'filmgenres', 'ausprobieren', 'mixen', 'verdrehen', 'ang', 'filmografie', 'datei', 'ang', 'lee', 'festival', 'de', 'venise', 'lee', 'internationale', 'filmfestspiele', 'venedig', 'venedig', 'shades', 'of', 'the', 'lake', 'kurzfilm', 'fine', 'line', 'kurzfilm', 'schiebende', 'hand', 'pushing', 'hands', 'tui', 'shou', 'hochzeitsbankett', 'the', 'wedding', 'banquet', 'xi', 'yan', 'eat', 'drink', 'woman', 'yin', 'shi', 'nan', 'nu', 'sinn', 'sinnlichkeit', 'sinnlichkeit', 'sense', 'and', 'sensibility', 'eissturm', 'the', 'icestorm', 'ride', 'with', 'the', 'devil', 'tiger', 'and', 'dragon', 'crouching', 'tiger', 'hidden', 'dragon', 'hu', 'cang', 'long', 'chosen', 'hire', 'chosen', 'kurzwerbefilm', 'automarke', 'hulk', 'brokeback', 'mountain', 'gefahr', 'begierde', 'sè', 'jiè', 'taking', 'woodstock', 'life', 'of', 'pi', 'schiffbruch', 'zuschauer', 'frankfurter', 'allgemeine', 'dezember', 'irren', 'heldentour', 'billy', 'lynn', 'irren', 'heldentour', 'billy', 'lynn', 'billy', 'lynn', 's', 'long', 'halftime', 'walk', 'gemini', 'auszeichnung', 'auswahl', 'oscarverleihung', 'auszeichnung', 'kategorie', 'oscar', 'bester', 'fremdsprachig', 'fremdsprachig', 'film', 'tiger', 'and', 'dragon', 'oscarverleihung', 'nominierung', 'kategorie', 'oscar', 'beste', 'regie', 'tiger', 'and', 'dragon', 'oscarverleihung', 'nominierung', 'kategorie', 'oscar', 'bester', 'film', 'tiger', 'and', 'dragon', 'oscarverleihung', 'auszeichnung', 'kategorie', 'oscar', 'beste', 'regie', 'brokeback', 'mountain', 'oscarverleihung', 'auszeichnung', 'kategorie', 'oscar', 'beste', 'regie', 'life', 'of', 'pi', 'schiffbruch', 'tiger', 'oscarverleihung', 'nominierung', 'kategorie', 'oscar', 'bester', 'film', 'life', 'of', 'pi', 'schiffbruch', 'tiger', 'golden', 'globe', 'award', 'golden', 'globe', 'awards', 'nominierung', 'kategorie', 'golden', 'globe', 'award', 'beste', 'regie', 'sinn', 'sinnlichkeit', 'golden', 'globe', 'awards', 'auszeichnung', 'kategorie', 'golden', 'globe', 'award', 'beste', 'regie', 'tiger', 'and', 'dragon', 'golden', 'globe', 'awards', 'auszeichnung', 'kategorie', 'golden', 'globe', 'award', 'beste', 'regie', 'brokeback', 'mountain', 'golden', 'globe', 'awards', 'nominierung', 'kategorie', 'golden', 'globe', 'award', 'beste', 'regie', 'life', 'of', 'pi', 'schiffbruch', 'tiger', 'directors', 'guild', 'of', 'america', 'award', 'nominierung', 'kategorie', 'beste', 'spielfilmregie', 'sinn', 'sinnlichkeit', 'auszeichnung', 'kategorie', 'beste', 'spielfilmregie', 'tiger', 'and', 'dragon', 'auszeichnung', 'kategorie', 'beste', 'spielfilmregie', 'brokeback', 'mountain', 'nominierung', 'kategorie', 'beste', 'spielfilmregie', 'life', 'of', 'pi', 'schiffbruch', 'tiger', 'auszeichnung', 'goldener', 'bär', 'berliner', 'filmfestspiele', 'hochzeitsbankett', 'golden', 'horse', 'film', 'horse', 'beste', 'regie', 'hochzeitsbankett', 'goldener', 'bär', 'berliner', 'filmfestspiele', 'sinn', 'sinnlichkeit', 'deutscher', 'deutscher', 'filmpreis', 'bester', 'ausländisch', 'ausländisch', 'film', 'sinn', 'sinnlichkeit', 'golden', 'horse', 'film', 'horse', 'bester', 'film', 'tiger', 'and', 'dragon', 'hong', 'kong', 'film', 'award', 'tiger', 'and', 'dragon', 'aufnahme', 'american', 'academy', 'of', 'arts', 'and', 'sciences', 'goldener', 'löwe', 'filmfestivals', 'venedig', 'brokeback', 'mountain', 'golden', 'horse', 'film', 'horse', 'beste', 'regie', 'gefahr', 'begierde', 'goldener', 'löwe', 'filmfestivals', 'venedig', 'gefahr', 'begierde', 'siehe', 'taiwanischer', 'film', 'film', 'literatur', 'chronologisch', 'tanja', 'hanhart', 'redaktorin', 'ang', 'lee', 'kino', 'poesie', 'grossformat', 'februar', 'isbn', 'thoma', 'koebner', 'artikel', 'ang', 'lee', 'ders', 'filmregisseure', 'biographie', 'werkbeschreibungen', 'filmographien', 'abbildung', 'aktualisieren', 'erweitern', 'auflage', 'reclam', 'stuttgart', 'aufl', 'isbn', 'qin', 'hu', 'kino', 'ang', 'lee', 'chinesisch', 'philosophie', 'kunstauffassung', 'kultur', 'filmästhetischen', 'aspekt', 'gardez', 'verlag', 'michael', 'pekler', 'andrea', 'ungerböck', 'ang', 'lee', 'film', 'schüren', 'marburg', 'isbn', 'isabell', 'gössele', 'kino', 'ang', 'lee', 'atem', 'verborgen', 'drache', 'tectum', 'marburg', 'isbn', 'weblinks', 'commonscat', 'lee', 'ang', 'lee', 'biografie', 'who', 's', 'who', 'erfolg', 'missbrauche', 'spiegel', 'online', 'oktober', 'interview', 'subtext', 'western', 'interview', 'ralph', 'geisenhanslüke', 'regisseur', 'ang', 'lee', 'vater', 'stolz', 'januar', 'interview', 'andrea', 'kilb', 'ang', 'lee', 'sechzigsten', 'zerbrechlichkeit', 'welt', 'frankfurter', 'allgemeine', 'oktober', 'einzelnachweise', 'references', 'no', 'sortierung', 'lee', 'ang', 'kategorie', 'ang', 'kategorie', 'drehbuchautor', 'kategorie', 'filmregisseur', 'kategorie', 'oscarpreisträger', 'kategorie', 'kategorie', 'person', 'namensgeber', 'asteroid', 'kategorie', 'mitglied', 'american', 'academy', 'of', 'arts', 'and', 'sciences', 'kategorie', 'taiwaner', 'kategorie', 'geboren', 'kategorie', 'mann', 'personendaten', 'lee', 'ang', '李安', 'chinesisch', 'lǐ', 'ān', 'chinesisch', 'taiwanischer', 'regisseur', 'drehbuchautor', 'oktober', 'landkreis', 'pingtung', 'taiwan']\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):\n",
    "    if index < 3:\n",
    "        if event == 'end' and \"text\" in elem.tag:\n",
    "            prep_tokens = preprocess_text(elem.text)\n",
    "            print(prep_tokens)\n",
    "            elem.clear()\n",
    "            index += 1    \n",
    "    else:\n",
    "        break       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the corpus as an object\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        # define the XML tree\n",
    "        for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):            \n",
    "            # Each document is represented as an object between <text> tags in the xml file\n",
    "            if event == 'end' and \"text\" in elem.tag:\n",
    "                # Transfom the corpus to vectors\n",
    "                yield dictionary.doc2bow(preprocess_text(elem.text))\n",
    "                elem.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a smaller corpus, containing only the first 200 documents:\n",
    "class MyCorpus_small:\n",
    "    def __iter__(self):\n",
    "        index = 0\n",
    "        # define the XML tree\n",
    "        for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):\n",
    "            if index < 20:\n",
    "                # Each document is represented as an object between <text> tags in the xml file\n",
    "                if event == 'end' and \"text\" in elem.tag:\n",
    "                    # Transfom the corpus to vectors\n",
    "                    yield dictionary.doc2bow(preprocess_text(elem.text))\n",
    "                    index+=1\n",
    "                    elem.clear()\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the corpus, without loading it into memory\n",
    "corpus = MyCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_small = MyCorpus_small()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Somehow we have to save which text matches which ID, so we can later on return the text and not only it's vector representation. To match every text with an id and store it would render the following approaches, to stream the data instead of storing it in memory, obsolete/useless. To counter that i propose we create a dictionary which only contains the title of every article, instead of the full text.\n",
    "Currently WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_ids = {}\n",
    "index = 0\n",
    "for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):        \n",
    "    if index < 200:\n",
    "        if event == 'end' and \"title\" in elem.tag:\n",
    "            title_ids[index]=str(elem.text)\n",
    "            index += 1    \n",
    "            elem.clear()\n",
    "    else:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Alan Smithee'}\n"
     ]
    }
   ],
   "source": [
    "print(title_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for a specific entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "title_ids = {}\n",
    "index = 0\n",
    "for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):        \n",
    "    if event == 'end' and \"title\" in elem.tag:\n",
    "        if \"British Airways i360\" in elem.text:\n",
    "            title_ids[index]=str(elem.text)\n",
    "            print(elem.text)\n",
    "            break\n",
    "        elem.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get texts from the XML File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_ids = {}\n",
    "texts = []\n",
    "index = 0\n",
    "for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):        \n",
    "    if index < 200:\n",
    "        if event == 'end' and \"text\" in elem.tag:\n",
    "            text_ids[index]=str(elem.text)\n",
    "            index += 1  \n",
    "            texts.append(str(elem.text))\n",
    "            elem.clear()\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = [dictionary.doc2bow(preprocess_text(text)) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(xml_file):\n",
    "    index = 0\n",
    "    first_elem = True\n",
    "    for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):        \n",
    "        if index < 200:\n",
    "            if event == \"end\" and \"text\" in elem.tag:\n",
    "                text = preprocess_text(elem.text)\n",
    "                if first_elem:\n",
    "                    dictionary = Dictionary([text])\n",
    "                    first_elem = False\n",
    "                    index += 1\n",
    "                else:\n",
    "                    dictionary.add_documents([text])\n",
    "                    index += 1\n",
    "                elem.clear()\n",
    "        else:\n",
    "            break\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build the dictionary:\n",
    "dictionary = build_dictionary(xml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 50 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# remove words that appear only once\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "dictionary.filter_tokens(once_ids)\n",
    "# remove gaps in id sequence after words that were removed\n",
    "dictionary.compactify() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(20309 unique tokens: ['abc', 'abkehr', 'ablehnen', 'abrufen', 'abschluss']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dictionary\n",
    "dictionary.save('data/wiki_200.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dictionary\n",
    "dictionary = Dictionary.load('data/wiki_200.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(20309 unique tokens: ['abc', 'abkehr', 'ablehnen', 'abrufen', 'abschluss']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "for vector in corpus:\n",
    "    if index <2:\n",
    "        print(vector)\n",
    "        index += 1\n",
    "    else:\n",
    "        print(\"finished\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity with LDA (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "* corpus: the corpus\n",
    "* num_topics: topics to be extracted from the training corpus\n",
    "* id2word: id to word mapping, the dictionary\n",
    "* workers: number of cpu cores used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda = LdaMulticore(corpus_small, num_topics=200, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First experiments have shown that a topic number of 10 (default) is too low. 100 resulted in better disctinction between the different articles.\n",
    "__Further fine tuning needed here__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.033490263), (1, 0.033490896), (2, 0.03348978), (3, 0.033489995), (4, 0.033489734), (5, 0.0334891), (6, 0.69858706), (7, 0.03348973), (8, 0.03349116), (9, 0.0334923)]\n"
     ]
    }
   ],
   "source": [
    "doc_bow = [(0, 1), (1, 1)]\n",
    "print(lda[doc_bow]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'MyCorpus_small' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32me:\\python3_8\\lib\\site-packages\\gensim\\similarities\\docsim.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_best, dtype, num_features, chunksize, corpus_len)\u001b[0m\n\u001b[0;32m    807\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus_len\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m             \u001b[0mcorpus_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\python3_8\\lib\\site-packages\\gensim\\interfaces.py\u001b[0m in \u001b[0;36m__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;34m\"\"\"Get corpus size.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'MyCorpus_small' has no len()"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus_index = similarities.MatrixSimilarity(lda[corpus_small], num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 1), (6, 2), (7, 1), (8, 19), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 3), (18, 1), (19, 1), (20, 1), (21, 1), (22, 2), (23, 1), (24, 3), (25, 2), (26, 1), (27, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 1), (33, 1), (34, 4), (35, 1), (36, 1), (37, 3), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 2), (45, 2), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 2), (54, 1), (55, 1), (56, 2), (57, 2), (58, 1), (59, 1), (60, 2), (61, 1), (62, 1), (63, 1), (64, 1), (65, 3), (66, 1), (67, 2), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 2), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 2), (84, 2), (85, 12), (86, 1), (87, 1), (88, 2), (89, 1), (90, 1), (91, 2), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 2), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 2), (115, 1), (116, 4), (117, 2), (118, 1), (119, 3), (120, 2), (121, 2), (122, 1), (123, 1), (124, 1), (125, 2), (126, 1), (127, 1), (128, 2), (129, 1), (130, 1), (131, 1), (132, 4), (133, 2), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 2), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 1), (148, 1), (149, 1), (150, 1), (151, 1), (152, 1), (153, 1), (154, 2), (155, 2), (156, 1), (157, 12), (158, 1), (159, 1), (160, 1), (161, 2), (162, 1), (163, 2), (164, 1), (165, 1), (166, 1), (167, 1), (168, 5), (169, 1), (170, 1), (171, 2), (172, 1), (173, 1), (174, 2), (175, 1), (176, 1), (177, 1), (178, 1), (179, 1), (180, 1), (181, 2), (182, 10), (183, 1), (184, 1), (185, 1), (186, 1), (187, 1), (188, 1), (189, 17), (190, 1), (191, 2), (192, 1), (193, 2), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), (199, 1), (200, 1), (201, 1), (202, 2), (203, 1), (204, 1), (205, 1), (206, 1), (207, 1), (208, 1), (209, 3), (210, 2), (211, 1), (212, 1), (213, 1), (214, 1), (215, 1), (216, 1), (217, 1), (218, 1), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 1), (225, 1), (226, 2), (227, 1), (228, 1), (229, 3), (230, 1), (231, 1), (232, 1), (233, 2), (234, 2), (235, 3), (236, 1), (237, 1), (238, 3), (239, 1), (240, 1), (241, 1), (242, 1), (243, 1), (244, 1), (245, 1), (246, 1), (247, 1), (248, 1), (249, 2), (250, 3), (251, 1), (252, 2), (253, 1), (254, 1), (255, 1), (256, 3), (257, 1), (258, 2), (259, 1), (260, 1), (261, 1), (262, 1), (263, 1), (264, 1), (265, 1), (266, 1), (267, 3), (268, 1), (269, 1), (270, 2), (271, 1)]\n",
      "[(73, 0.99762106)]\n"
     ]
    }
   ],
   "source": [
    "test_doc_raw = texts[0]\n",
    "test_vec = dictionary.doc2bow(preprocess_text(test_doc_raw))\n",
    "print(test_vec)\n",
    "# convert to lda space\n",
    "test_vec_lda = lda[test_vec]\n",
    "print(test_vec_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0), (9, 0.0), (10, 0.0), (11, 0.0), (12, 0.0), (13, 0.0), (14, 0.0), (15, 0.0), (16, 0.0), (17, 0.0), (18, 0.0), (19, 0.0), (20, 0.0), (21, 0.0), (22, 0.0), (23, 0.0), (24, 0.0), (25, 0.0), (26, 0.0), (27, 0.0), (28, 0.0), (29, 0.0), (30, 0.0), (31, 0.0), (32, 0.0), (33, 0.0), (34, 0.0), (35, 0.0), (36, 0.0), (37, 0.0), (38, 0.0), (39, 0.0), (40, 0.0), (41, 0.0), (42, 0.0), (43, 0.0), (44, 0.0), (45, 0.0), (46, 0.0), (47, 0.0), (48, 0.0), (49, 0.0), (50, 0.0), (51, 0.0), (52, 0.0), (53, 0.0), (54, 0.0), (55, 0.0), (56, 0.0), (57, 0.0), (58, 0.0), (59, 0.0), (60, 0.0), (61, 0.0), (62, 0.0), (63, 0.0), (64, 0.0), (65, 0.0), (66, 0.0), (67, 0.0), (68, 0.0), (69, 0.0), (70, 0.0), (71, 0.0), (72, 0.0), (73, 0.0), (74, 0.0), (75, 0.0), (76, 0.0), (77, 0.0), (78, 0.0), (79, 0.0), (80, 0.0), (81, 0.0), (82, 0.0), (83, 0.0), (84, 0.0), (85, 0.0), (86, 0.0), (87, 0.0), (88, 0.0), (89, 0.0), (90, 0.0), (91, 0.0), (92, 0.0), (93, 0.0), (94, 0.0), (95, 0.0), (96, 0.0), (97, 0.0), (98, 0.0), (99, 0.0), (100, 0.0), (101, 0.0), (102, 0.0), (103, 0.0), (104, 0.0), (105, 0.0), (106, 0.0), (107, 0.0), (108, 0.0), (109, 0.0), (110, 0.0), (111, 0.0), (112, 0.0), (113, 0.0), (114, 0.0), (115, 0.0), (116, 0.0), (117, 0.0), (118, 0.0), (119, 0.0), (120, 0.0), (121, 0.017386837), (122, 0.0), (123, 0.0), (124, 0.0), (125, 0.0), (126, 0.0), (127, 0.0), (128, 0.0), (129, 0.0), (130, 0.0), (131, 0.0), (132, 0.0), (133, 0.0), (134, 0.0), (135, 0.0), (136, 0.0), (137, 0.0), (138, 0.0), (139, 0.0), (140, 0.0), (141, 0.0), (142, 0.0), (143, 0.0), (144, 0.0), (145, 0.0), (146, 0.0), (147, 0.0), (148, 0.0), (149, 0.0), (150, 0.0), (151, 0.0), (152, 0.0), (153, 0.0), (154, 0.0), (155, 0.0), (156, 0.0), (157, 0.0), (158, 0.0), (159, 0.0), (160, 0.0), (161, 0.0), (162, 0.0), (163, 0.0), (164, 0.0), (165, 0.0), (166, 0.0), (167, 0.0), (168, 0.0), (169, 0.0), (170, 0.0), (171, 0.0), (172, 0.0), (173, 0.0), (174, 0.0), (175, 0.0), (176, 0.0), (177, 0.0), (178, 0.0), (179, 0.0), (180, 0.0), (181, 0.0), (182, 0.0), (183, 0.0), (184, 0.0), (185, 0.0), (186, 0.0), (187, 0.0), (188, 0.0), (189, 0.0), (190, 0.0), (191, 0.0), (192, 0.0), (193, 0.0), (194, 0.0), (195, 0.0), (196, 0.0), (197, 0.0), (198, 0.0), (199, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "sims = corpus_index[test_vec_lda]\n",
    "print(list(enumerate(sims)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
