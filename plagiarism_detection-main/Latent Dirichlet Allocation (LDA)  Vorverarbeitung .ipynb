{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample documents\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for tokenized documents in loop\n",
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 1), (5, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.068*\"brother\" + 0.068*\"mother\" + 0.068*\"drive\" + 0.041*\"pressur\"'), (1, '0.086*\"health\" + 0.086*\"good\" + 0.086*\"brocolli\" + 0.061*\"eat\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=2, num_words=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beispiele für die Vorverarbeitung. Noch eine Möglichkeit wäre TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Das ist ein Beispielsatz für eine Vorverarbeitung.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Does the substring match a tokenizer exception rule? For example, “don’t” does not contain whitespace, but should be split into two tokens, “do” and “n’t”, while “U.K.” should always remain one token.\n",
    "2. Can a prefix, suffix or infix be split off? For example punctuation like commas, periods, hyphens or quotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Doc = A container for accessing linguistic annotations.\n",
    "- Tokenizer: Segment text, and create Doc objects with the discovered segment boundaries.\n",
    "- Lemmatizer: Determine the base forms of words. For example, the lemma of “was” is “be”, and the lemma of “rats” is “rat”.\n",
    "- Tagger: Annotate part-of-speech tags on Doc objects.\n",
    "- Dependency Parsing: Assigning syntactic dependency labels, describing the relations between individual tokens, like subject or object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Text: The original word text.\n",
    "- Lemma: The base form of the word.\n",
    "- POS: The simple UPOS part-of-speech tag.\n",
    "- Tag: The detailed part-of-speech tag.\n",
    "- Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "- Shape: The word shape – capitalization, punctuation, digits.\n",
    "- is alpha: Is the token an alpha character?\n",
    "- is stop: Is the token part of a stop list, i.e. the most common words of the language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das der PRON PDS sb Xxx True True\n",
      "ist sein AUX VAFIN ROOT xxx True True\n",
      "ein einen DET ART nk xxx True True\n",
      "Beispielsatz Beispielsatz NOUN NN pd Xxxxx True False\n",
      "für für ADP APPR mnr xxx True True\n",
      "eine einen DET ART nk xxxx True True\n",
      "Vorverarbeitung Vorverarbeitung NOUN NN nk Xxxxx True False\n",
      ". . PUNCT $. punct . False False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from germalemma import GermaLemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('german'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = '''Das ist ein Beispielsatz für eine Vorverarbeitung. Und noch ein zweiter Satz, weil es so schön ist. Dieser Satz ist für viele Corpi und mehrere Anzahlen.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Das', 'ist', 'ein', 'Beispielsatz', 'für', 'eine', 'Vorverarbeitung', '.', 'Und', 'noch', 'ein', 'zweiter', 'Satz', ',', 'weil', 'es', 'so', 'schön', 'ist', '.', 'Dieser', 'Satz', 'ist', 'für', 'viele', 'Corpi', 'und', 'mehrere', 'Anzahlen', '.']\n",
      "['Das', 'Beispielsatz', 'Vorverarbeitung', '.', 'Und', 'zweiter', 'Satz', ',', 'schön', '.', 'Dieser', 'Satz', 'viele', 'Corpi', 'mehrere', 'Anzahlen', '.']\n",
      "[('Das', 'NNP'), ('ist', 'NN'), ('ein', 'NN'), ('Beispielsatz', 'NNP'), ('für', 'NN'), ('eine', 'NN'), ('Vorverarbeitung', 'NNP'), ('.', '.')]\n",
      "[('Und', 'NNP'), ('noch', 'CC'), ('ein', 'RB'), ('zweiter', 'NN'), ('Satz', 'NNP'), (',', ','), ('weil', 'VBP'), ('es', 'RB'), ('so', 'RB'), ('schön', 'JJ'), ('ist', 'NN'), ('.', '.')]\n",
      "[('Dieser', 'NNP'), ('Satz', 'NNP'), ('ist', 'NN'), ('für', 'NN'), ('viele', 'NN'), ('Corpi', 'NNP'), ('und', 'NN'), ('mehrere', 'RB'), ('Anzahlen', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(doc) \n",
    "tokenized = sent_tokenize(doc)\n",
    "\n",
    "\n",
    "wordsList = [w for w in word_tokens if not w in stop_words] \n",
    "wordsList = [] \n",
    "\n",
    "for w in word_tokens:  \n",
    "    if w not in stop_words:  \n",
    "        wordsList.append(w) \n",
    "        \n",
    "print(word_tokens)  \n",
    "print(wordsList)  \n",
    "\n",
    "\n",
    "for i in tokenized:\n",
    "    wordsList = nltk.word_tokenize(i) \n",
    "    tagged = nltk.pos_tag(wordsList) \n",
    "  \n",
    "    print(tagged) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei NLTK ist es etwas schwieriger mit der Lammatizierung. Da müssten wir nochmal schauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.stem import GermanWortschatzLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizer = GermaLemma()\n",
    "\n",
    "# passing the word and the POS tag (\"N\" for noun)\n",
    "#lemma = lemmatizer.find_lemma(doc)\n",
    "#print(lemma)\n",
    "# -> lemma is \"Feinstaubbelastung\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
