{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Corpus\n",
    "\n",
    "Corpus from: https://dumps.wikimedia.org/dewiki/20200820/\n",
    "\n",
    "Sentences for comparison from: https://github.com/t-systems-on-site-services-gmbh/german-wikipedia-text-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from xml.etree.ElementTree import *\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "import os\n",
    "import pprint\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import LdaMulticore\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from smart_open import open \n",
    "import spacy\n",
    "import de_core_news_md\n",
    "import pickle\n",
    "\n",
    "from ipywidgets import FileUpload\n",
    "from IPython.display import display\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the XML-file\n",
    "xml_file = \"/Volumes/SSD/dewiki-20200820-pages-articles-multistream.xml\"\n",
    "\n",
    "# number of documents to parse \n",
    "num_documents = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to return the title of a given article later on, we need to store those in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_ids = get_titles(xml_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a corpus from the text contents of the XML file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Corpus is defined as a class object, so it can be called when needed.\n",
    "2. Loops through the XML-file, searching for closing \"text\" tags.\n",
    "3. Returns the text contents from these nodes in preprocessed form.\n",
    "4. Then clears the current node from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the corpus as an object\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        # define the XML tree\n",
    "        for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):            \n",
    "            # Each document is represented as an object between <text> tags in the xml file\n",
    "            if event == 'end' and \"text\" in elem.tag:\n",
    "                # Transfom the corpus to vectors\n",
    "                yield dictionary.doc2bow(preprocess_text(elem.text))\n",
    "                # clear the node\n",
    "                elem.clear()                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the corpus, without loading it into memory, this is not needed when working with the smaller corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = MyCorpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole corpus is too big for this experiment and takes too long to parse through. For our proof-of-concept approach we therefore propose a function which only loops through the first i documents (text nodes) in the XML tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a smaller corpus, containing only the first i documents:\n",
    "class MyCorpus_small:\n",
    "    def __iter__(self):\n",
    "        index = 0\n",
    "        # define the XML tree\n",
    "        for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):\n",
    "            if index < num_documents:\n",
    "                # Each document is represented as an object between <text> tags in the xml file\n",
    "                if event == 'end' and \"text\" in elem.tag:\n",
    "                    # Transfom the corpus to vectors\n",
    "                    yield dictionary.doc2bow(preprocess_text(elem.text))\n",
    "                    index+=1\n",
    "                    # clear the node\n",
    "                    elem.clear()\n",
    "            else:\n",
    "                break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the smaller corpus, again without loading it into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_small = MyCorpus_small()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Dictionary\n",
    "\n",
    "To further work with the corpus in vector form, we need to build a dictionary. \n",
    "\n",
    "This function needs to be called only once, since we are able to save the dictionary created by it and load it in future use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DO NOT RUN THE FOLLOWING CODE IF THE DICTIONARY CAN BE LOADED FROM A FILE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "build_dictionary() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: build_dictionary() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build the dictionary:\n",
    "dictionary = build_dictionary(xml_file, num_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# remove words that appear only once\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "dictionary.filter_tokens(once_ids)\n",
    "# remove gaps in id sequence after words that were removed\n",
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dictionary\n",
    "dictionary.save('data/wiki_200_new.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CONTINUE HERE TO LOAD THE DICTIONARY__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dictionary\n",
    "dictionary = Dictionary.load('data/wiki_200_new.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the dictionary has been loaded \n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity with LDA (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "* corpus: the corpus\n",
    "* num_topics: topics to be extracted from the training corpus\n",
    "* id2word: id to word mapping, the dictionary\n",
    "* workers: number of cpu cores used\n",
    "\n",
    "The trained model can be stored and loaded, as same as the dictionary before."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "lda = LdaMulticore(corpus_small, num_topics=300, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First experiments have shown that a topic number of 10 (default) is too low. 100 resulted in better disctinction between the different articles.\n",
    "__Further fine tuning needed here__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save the trained model\n",
    "lda.save(\"data/lda_model_200_t300.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "lda = LdaModel.load(\"data/lda_model_200_t300.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index the corpus with the trained model:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "corpus_index = similarities.MatrixSimilarity(list(lda[corpus_small]), num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save the index\n",
    "pickle_out = open(\"data/corpus_index.pickle\", \"wb\")\n",
    "pickle.dump(corpus_index, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the index from disk\n",
    "corpus_index = pickle.load(open(\"data/corpus_index.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Check\n",
    "\n",
    "Now that we have a LDA model and an index we can check the similarity of an input document against all documents in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# upload text file for testing\n",
    "upload = FileUpload(accept='.txt', multiple=False)\n",
    "print(\"upload the text you want to check for plagiarism\")\n",
    "display(upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(upload.value) == 0:\n",
    "    print(\"no file uploaded! try again\")\n",
    "else :\n",
    "    print(\"File uploaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save text file for testing\n",
    "with open('import/tobetested.txt', 'wb') as output_file: \n",
    "    for uploaded_filename in upload.value:\n",
    "        content = upload.value[uploaded_filename]['content']   \n",
    "        output_file.write(content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define document to use in similarity check\n",
    "test_document = open('import/tobetested.txt', encoding='utf-8')\n",
    "document_name = '\"'+os.path.basename(test_document.name)+'\"'\n",
    "test_document = test_document.read()\n",
    "\n",
    "# delete imported data after correct usage \n",
    "os.remove(\"import/tobetested.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transform the document to vector space\n",
    "test_vec = dictionary.doc2bow(preprocess_text(test_document))\n",
    "# convert to lda space\n",
    "test_vec_lda = lda[test_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the similarities\n",
    "sims = corpus_index[test_vec_lda]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creates result tags for html output\n",
    "result_html = \"\"\n",
    "vis = []\n",
    "hits = 0\n",
    "for ids in list(enumerate(sims)):\n",
    "    if ids[1] >= 0.1:\n",
    "        hits += 1\n",
    "        title = title_ids.get(ids[0])\n",
    "        if ids[1] < 0.4:\n",
    "            cr_level=\"zero\"\n",
    "        if ids[1] >= 0.4:\n",
    "            cr_level=\"low\"\n",
    "        if ids[1] >= 0.5:\n",
    "            cr_level=\"medium\"\n",
    "        if ids[1] >= 0.6:\n",
    "            cr_level=\"higher\"\n",
    "        if ids[1] >= 0.75:\n",
    "            cr_level=\"high\"\n",
    "        result_html = result_html+\" <tr class='\"+cr_level+\"'><td><a href='https://de.wikipedia.org/wiki/\"+title+\"'>\"+title+\"</a></td> \"+\"<td>\"+str(round(ids[1],2))+\"</td> \"+\"<td>\"+str(ids[0])+\"</td> </tr> \"\n",
    "        vis.append(cr_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#werte für visualisierung\n",
    "prczero = 0\n",
    "prclow = 0\n",
    "prcmed = 0\n",
    "prcher = 0\n",
    "prchig = 0\n",
    "\n",
    "for elm in vis:\n",
    "    if elm==\"zero\": \n",
    "        prczero+=1\n",
    "    if elm==\"low\": \n",
    "        prclow+=1\n",
    "    if elm==\"medium\": \n",
    "        prcmed+=1\n",
    "    if elm==\"higher\": \n",
    "        prcher+=1\n",
    "    if elm==\"high\": \n",
    "        prchig+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# html output of all results\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    ".r_table {\n",
    "  font-family: Arial;\n",
    "  border-collapse: collapse;\n",
    "  width: 100%;}\n",
    "  \n",
    ".r_table th {border: 1px solid #ddd;padding: 8px;}\n",
    "\n",
    ".r_table th {\n",
    "  font-size: 16px;\n",
    "  padding-top: 12px;\n",
    "  padding-bottom: 12px;\n",
    "  text-align: left;\n",
    "  background-color: steelblue;\n",
    "  color: white;\n",
    "  border: 1px solid #ddd;}\n",
    "  \n",
    ".r_table td {border: 1px solid #ddd;font-size: 14px; text-align:left;}\n",
    "\n",
    ".high td{background-color: #F8E0E0;}\n",
    ".higher td{background-color: #F8ECE0;}\n",
    ".medium td{background-color: #F7F8E0;}\n",
    ".low td{background-color: #E0F8E0;}\n",
    ".zero td{background-color: white;}\n",
    "</style>\n",
    "\n",
    "<h3> The tested input \"\"\"+document_name+\"\"\" has the following similarity results </h3> \n",
    "<table class=\"r_table\">\n",
    "  <tr>\n",
    "    <th>Document Title</th>\n",
    "    <th>Similarity Score</th> \n",
    "    <th>Document-ID</th>\n",
    "  </tr>\n",
    "  \"\"\"+result_html+\"\"\"\n",
    "\n",
    "</table>\n",
    "<h4>\"\"\"+str(hits)+\"\"\" wikipedia documents with higher similarity found</h4> \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "#piechart {\n",
    "  position: relative;\n",
    "  width: 250px;\n",
    "  height: 250px;\n",
    "  margin-left: 100px;\n",
    "  margin-top: 100px;\n",
    "}\n",
    " \n",
    ".piece {\n",
    "  position: absolute;\n",
    "  width: 250px;\n",
    "  height: 250px;\n",
    "  clip: rect(0px, 250px, 250px, 125px);\n",
    "  border-radius: 125px;\n",
    "  transition: all 0.8s ease-out;\n",
    "}\n",
    "\n",
    ".piece-inner {\n",
    "    position: absolute;\n",
    "\twidth: 250px;\n",
    "\theight: 250px;\n",
    "\tclip: rect(0px, 125px, 250px, 0px);\n",
    "\tborder-radius: 125px;\n",
    "\t-webkit-backface-visibility: hidden;\n",
    "    transition: all 0.8s ease-out;\n",
    "}\n",
    "/* Spezifische Einstellungen */\n",
    "#piece1 > .piece-inner {\n",
    "    background: green; \n",
    "}\n",
    "#piece2 > .piece-inner {\n",
    "    background: yellow; \n",
    "}\n",
    "#piece3 > .piece-inner {\n",
    "    background: orange;\n",
    "}\n",
    "#piece4 > .piece-inner {\n",
    "    background: red;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<script>\n",
    "var wert1 = \"\"\"+str(prclow/len(vis)*1200)+\"\"\";\n",
    "var wert2 = \"\"\"+str(prcmed/len(vis)*1200)+\"\"\";\n",
    "var wert3 = \"\"\"+str(prcher/len(vis)*1200)+\"\"\";\n",
    "var wert4 = \"\"\"+str(prchig/len(vis)*1200)+\"\"\";\n",
    "\n",
    "document.querySelector(\"#piece1\").style.webkitTransform = \"rotate(0deg)\";\n",
    "document.querySelector(\"#piece1\").style.transform = \"rotate(0deg)\";\n",
    "document.querySelector(\"#piece1 > .piece-inner\").style.webkitTransform =\n",
    "    \"rotate(\" + wert1 + \"deg)\";\n",
    "document.querySelector(\"#piece1 > .piece-inner\").style.transform = \"rotate(\" + wert1 + \"deg)\";\n",
    "\n",
    "document.querySelector(\"#piece2\").style.webkitTransform = \"rotate(\" + wert1 + \"deg)\";\n",
    "document.querySelector(\"#piece2\").style.transform = \"rotate(\" + wert1 + \"deg)\";\n",
    "document.querySelector(\"#piece2 > .piece-inner\").style.webkitTransform = \"rotate(\" + wert2 + \"deg)\";\n",
    "document.querySelector(\"#piece2 > .piece-inner\").style.transform = \"rotate(\" + wert2 + \"deg)\";\n",
    "\n",
    "document.querySelector(\"#piece3\").style.webkitTransform = \"rotate(\" + (wert1 + wert2) + \"deg)\";\n",
    "document.querySelector(\"#piece3\").style.transform = \"rotate(\" + (wert1 + wert2) + \"deg)\";\n",
    "document.querySelector(\"#piece3 > .piece-inner\").style.webkitTransform = \"rotate(\" + wert3 + \"deg)\";\n",
    "document.querySelector(\"#piece3 > .piece-inner\").style.transform = \"rotate(\" + wert3 + \"deg)\";\n",
    "\n",
    "document.querySelector(\"#piece4\").style.webkitTransform = \"rotate(\" + (wert1 + wert2 + wert3) + \"deg)\";\n",
    "document.querySelector(\"#piece4\").style.transform = \"rotate(\" + (wert1 + wert2 + wert3) + \"deg)\";\n",
    "document.querySelector(\"#piece4 > .piece-inner\").style.webkitTransform = \"rotate(\" + wert4 + \"deg)\";\n",
    "document.querySelector(\"#piece4 > .piece-inner\").style.transform = \"rotate(\" + wert4 + \"deg)\";\n",
    "</script>\n",
    "\n",
    "<div id=\"piechart\">\n",
    "  <div id=\"piece1\" class=\"piece\">\n",
    "      <div class=\"piece-inner\"></div>      \n",
    "  </div>\n",
    "  <div id=\"piece2\" class=\"piece\">\n",
    "      <div class=\"piece-inner\"></div>\n",
    "  </div>\n",
    "  <div id=\"piece3\" class=\"piece\">\n",
    "      <div class=\"piece-inner\"></div>\n",
    "  </div>\n",
    "  <div id=\"piece4\" class=\"piece\">\n",
    "      <div class=\"piece-inner\"></div>\n",
    "  </div>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "text_ids = {}\n",
    "texts = []\n",
    "index = 0\n",
    "for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):        \n",
    "    if index < 200:\n",
    "        if event == 'end' and \"text\" in elem.tag:\n",
    "            text_ids[index]=str(elem.text)\n",
    "            index += 1  \n",
    "            texts.append(str(elem.text))\n",
    "            elem.clear()\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hit-corpus\n",
    "# takes all plagiarism documents with similarity over 0.75\n",
    "# split into sentences with spacy\n",
    "class MyCorpus_hits:\n",
    "    def __iter__(self):\n",
    "          for ids in list(enumerate(sims)):\n",
    "            if ids[1] >= 0.7:\n",
    "                for split in spacy_data(texts[ids[0]]).sents:\n",
    "                    yield dictionary.doc2bow(preprocess_text(str(split)))\n",
    "                    elem.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_corpus = MyCorpus_hits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hit_lda = LdaMulticore(hit_corpus, num_topics=300, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "corpus_hit_index = similarities.MatrixSimilarity(list(hit_lda[hit_corpus]), num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slice test document to sentences\n",
    "test_doc_raw_slice = []\n",
    "for split in spacy_data(test_document).sents:\n",
    "    test_doc_raw_slice.append(preprocess_text(str(split)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for sentence in test_doc_raw_slice:\n",
    "    # test doc Sätze vs hit_corpus \n",
    "    test_vec = dictionary.doc2bow(sentence)\n",
    "    # convert to lda space\n",
    "    test_vec_lda = lda[test_vec]\n",
    "    sims_hits = corpus_hit_index[test_vec_lda]\n",
    "    print(np.amax(sims_hits))\n",
    "    print(test_vec)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#hits for all sentences\n",
    "sims_hits = corpus_hit_index[test_vec_lda]\n",
    "print(list(enumerate(sims_hits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html output of all results\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "\n",
    ".high {background-color: #F8E0E0;}\n",
    ".higher {background-color: #F8ECE0;}\n",
    ".medium {background-color: #F7F8E0;}\n",
    ".low {background-color: #E0F8E0;}\n",
    "</style style>\n",
    "\n",
    "<h3>color schema\n",
    "<span class='high'>1.00-0.70</span>&nbsp;&nbsp;\n",
    "<span class='higher'>0.69-0.60</span>&nbsp;&nbsp; \n",
    "<span class='medium'>0.59-0.50</span>&nbsp;&nbsp; \n",
    "<span class='low'>0.49-0.40</span>&nbsp;&nbsp; \n",
    "<br><br></h3>\n",
    "<div style=\"font-size:12pt;line-height:150%;\">\n",
    "<span class='medium'>Der Kleinspecht (Dryobates minor, Syn.: Dendrocopos minor) ist eine Vogelart aus der Gattung der Buntspechte (Dendrocopos). Diese gehören zur Unterfamilie der Echten Spechte in der Familie der Spechte (Picidae).\n",
    "Die Art zählt mit einer Körperlänge von rund 15 cm zu den kleinsten Echten Spechten.<b> (Aristoteles)</b></span> Sie ist in 11 Unterarten über die gesamte westliche und nördliche Paläarktis bis an die asiatische Pazifikküste verbreitet.\n",
    "<span class='higher'>Der Kleinspecht ist ein typischer Vertreter der Buntspechte mit schwarz-weiß kontrastierendem Gefieder, trotzdem ist er in der West- und Zentralpaläarktis auf Grund seiner Kleinheit unverwechselbar\n",
    "Beide Geschlechter des Kleinspechtes sind fast während des gesamten Jahres sehr ruffreudig<b> (Abraham Lincoln)</b></span>\n",
    "Der Höhepunkt der gesanglichen Aktivität liegt jedoch im Spätwinter und im zeitigen Frühjahr\n",
    "Die dichteste Verbreitung liegt in der planaren und collinen Stufe. Bedeutend seltener brüten Kleinspechte in Mitteleuropa in höhergelegenen Gebieten.\n",
    "Er bevorzugt Waldgebiete und Gehölze mit einem guten Bestand an alten, grobborkigen Laubbäumen. \n",
    "Die Nahrung des Kleinspechtes besteht fast während des gesamten Jahres aus kleinen baumbewohnenden Insekten\n",
    "<span class='medium'>Wie alle Spechte ist auch der Kleinspecht tagaktiv; seine Aktivität beginnt kurz vor Sonnenaufgang und endet kurz nach Sonnenuntergang\n",
    "Alfredo James „Al“ Pacino (* 25. April 1940 in New York) ist ein US-amerikanischer Schauspieler, Filmregisseur und Filmproduzent.<b> (Abraham Lincoln)</b></span> Er gilt für viele Kritiker und Zuschauer als einer der herausragenden Charakterdarsteller des zeitgenössischen amerikanischen Films und Theaters. So ist er seit den 1970er Jahren in zahlreichen Filmklassikern zu sehen.\n",
    "m Laufe seiner Karriere wurde er unter anderem mit dem Oscar, dem Golden Globe Award, dem Tony Award und der National Medal of Arts ausgezeichnet. <span class='high'>Seine bekanntesten Rollen sind die des Michael Corleone in der von Francis Ford Coppola inszenierten Der Pate-Trilogie und als Gangster Tony Montana in Scarface.<b> (Aldi)</b></span> \n",
    "</div>\n",
    "\"\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
