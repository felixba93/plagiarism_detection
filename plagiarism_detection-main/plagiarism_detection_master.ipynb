{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Corpus\n",
    "\n",
    "Corpus from: https://dumps.wikimedia.org/dewiki/20200820/\n",
    "\n",
    "Sentences for comparison from: https://github.com/t-systems-on-site-services-gmbh/german-wikipedia-text-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from xml.etree.ElementTree import *\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "import os\n",
    "import pprint\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import LdaMulticore\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from smart_open import open \n",
    "import spacy\n",
    "import de_core_news_md\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import FileUpload\n",
    "from IPython.display import display\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the XML-file\n",
    "xml_file = \"data/dewiki-20200820-pages-articles-multistream.xml\"\n",
    "\n",
    "# number of documents to parse \n",
    "num_documents = 200\n",
    "\n",
    "# similarity threshold, when does a document count as plagiarism\n",
    "sim_threshold = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to return the title of a given article later on, we need to store those in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_ids = get_titles(xml_file, num_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the index\n",
    "pickle_out = open(\"data/title_ids200.pickle\", \"wb\")\n",
    "pickle.dump(title_ids, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the index from disk\n",
    "title_ids = pickle.load(open(\"data/title_ids200.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a corpus from the text contents of the XML file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Corpus is defined as a class object, so it can be called when needed.\n",
    "2. Loops through the XML-file, searching for closing \"text\" tags.\n",
    "3. Returns the text contents from these nodes in preprocessed form.\n",
    "4. Then clears the current node from memory"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Define the corpus as an object\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        # define the XML tree\n",
    "        for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):            \n",
    "            # Each document is represented as an object between <text> tags in the xml file\n",
    "            if event == 'end' and \"text\" in elem.tag:\n",
    "                # Transfom the corpus to vectors\n",
    "                yield dictionary.doc2bow(preprocess_text(elem.text))\n",
    "                # clear the node\n",
    "                elem.clear()                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the corpus, without loading it into memory, this is not needed when working with the smaller corpus."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corpus = MyCorpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole corpus is too big for this experiment and takes too long to parse through. For our proof-of-concept approach we therefore propose a function which only loops through the first i documents (text nodes) in the XML tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a smaller corpus, containing only the first i documents:\n",
    "class MyCorpus_small:\n",
    "    def __iter__(self):\n",
    "        index = 0\n",
    "        # define the XML tree\n",
    "        for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):\n",
    "            if index < num_documents:\n",
    "                # Each document is represented as an object between <text> tags in the xml file\n",
    "                if event == 'end' and \"text\" in elem.tag:\n",
    "                    # Transfom the corpus to vectors\n",
    "                    yield dictionary.doc2bow(preprocess_text(elem.text))\n",
    "                    index+=1\n",
    "                    # clear the node\n",
    "                    elem.clear()\n",
    "            else:\n",
    "                break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the smaller corpus, again without loading it into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_small = MyCorpus_small()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Dictionary\n",
    "\n",
    "To further work with the corpus in vector form, we need to build a dictionary. \n",
    "\n",
    "This function needs to be called only once, since we are able to save the dictionary created by it and load it in future use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DO NOT RUN THE FOLLOWING CODE IF THE DICTIONARY CAN BE LOADED FROM A FILE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# build the dictionary:\n",
    "dictionary = build_dictionary(xml_file, num_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# remove words that appear only once\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "dictionary.filter_tokens(once_ids)\n",
    "# remove gaps in id sequence after words that were removed\n",
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dictionary\n",
    "dictionary.save('data/wiki_200_new.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CONTINUE HERE TO LOAD THE DICTIONARY__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dictionary\n",
    "dictionary = Dictionary.load('data/wiki_200_new.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the dictionary has been loaded \n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity with LDA (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "* corpus: the corpus\n",
    "* num_topics: topics to be extracted from the training corpus\n",
    "* id2word: id to word mapping, the dictionary\n",
    "* workers: number of cpu cores used\n",
    "\n",
    "The trained model can be stored and loaded, as same as the dictionary before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lda = LdaMulticore(corpus_small, num_topics=300, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First experiments have shown that a topic number of 10 (default) is too low. 100 resulted in better disctinction between the different articles.\n",
    "__Further fine tuning needed here__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the trained model\n",
    "lda.save(\"data/lda_model_200_t300.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the trained model\n",
    "lda = LdaModel.load(\"data/lda_model_200_t300.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index the corpus with the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "corpus_index = similarities.MatrixSimilarity(list(lda[corpus_small]), num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the index\n",
    "pickle_out = open(\"data/lda_index_200_t300.pickle\", \"wb\")\n",
    "pickle.dump(corpus_index, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the index from disk\n",
    "corpus_index = pickle.load(open(\"data/lda_index_200_t300.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Check\n",
    "\n",
    "Now that we have a LDA model and an index we can check the similarity of an input document against all documents in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define document to use in similarity check\n",
    "test_document = open('beispieltexte/wikibeispiele.txt', encoding='utf-8')\n",
    "test_document = test_document.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(test_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transform the document to vector space\n",
    "test_vec = dictionary.doc2bow(preprocess_text(test_document))\n",
    "# convert to lda space\n",
    "test_vec_lda = lda[test_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the similarities\n",
    "sims = corpus_index[test_vec_lda]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hits = 0\n",
    "hit_title =[]\n",
    "for ids in list(enumerate(sims)):\n",
    "    if ids[1] >= sim_threshold and \"Liste von Autoren\" not in title_ids.get(ids[0]):\n",
    "        hits += 1\n",
    "        title = title_ids.get(ids[0])\n",
    "        hit_title.append(title)\n",
    "        print(\"Similarity Score: \",ids[1],\"\\n\",\"Document ID:\",ids[0],\"\\n\",\"Title:\", title,\"\\n\", \"------------------------------------\")\n",
    "print(hits, \"cases of possible plagiarism detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_ids = {}\n",
    "for ids in list(enumerate(sims)):\n",
    "    if ids[1] >= sim_threshold and \"Liste von Autoren\" not in title_ids.get(ids[0]):\n",
    "        hit_ids[ids[0]] = ids[1]\n",
    "hit_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Similarity\n",
    "\n",
    "The next step would be to define all documents that were found to have a specific similarity score as a new corpus. Then we can check the similarty score for each sentence from the input document in relation to the sentences from the \"new\" corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build new dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "index = 0\n",
    "first_elem = True\n",
    "# loop through all nodes\n",
    "for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):        \n",
    "    if index < num_documents:\n",
    "        # check if current node contains a document\n",
    "        if event == \"end\" and \"text\" in elem.tag:\n",
    "            if index in hit_ids.keys():\n",
    "                # preprocess the text\n",
    "                text = preprocess_text(elem.text)\n",
    "                # if this is the first document found, create a new dictionary with it\n",
    "                if first_elem:\n",
    "                    dictionary_hits = Dictionary([text])\n",
    "                    first_elem = False\n",
    "                    index += 1\n",
    "                # all documents after the first one get appended to the dictionary\n",
    "                else:\n",
    "                    dictionary_hits.add_documents([text])\n",
    "                    index += 1\n",
    "                # clear the node\n",
    "                elem.clear()\n",
    "                \n",
    "            else:\n",
    "                index += 1\n",
    "                elem.clear()\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(dictionary_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a smaller corpus, containing only the first i documents:\n",
    "class MyCorpus_small_hits:\n",
    "    def __iter__(self):\n",
    "        index = 0\n",
    "        # define the XML tree\n",
    "        for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):\n",
    "            if index < num_documents:\n",
    "                if index in hit_ids.keys():\n",
    "                    # Each document is represented as an object between <text> tags in the xml file\n",
    "                    if event == 'end' and \"text\" in elem.tag:\n",
    "                        # Transfom the corpus to vectors\n",
    "                        yield dictionary_hits.doc2bow(preprocess_text(elem.text))\n",
    "                        index+=1\n",
    "                        # clear the node\n",
    "                        elem.clear()\n",
    "                else:\n",
    "                    index+=1  \n",
    "            else:\n",
    "                break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_small_hits = MyCorpus_small_hits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "hit_lda = LdaMulticore(corpus_small_hits, num_topics=300, id2word=dictionary_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hit_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "corpus_hit_index = similarities.MatrixSimilarity(list(hit_lda[corpus_small_hits]), num_features=len(dictionary_hits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus_hit_index)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#use spacy to slice sentences\n",
    "#slice test document to sentences\n",
    "test_doc_raw_slice = []\n",
    "for split in spacy_data(test_document).sents:\n",
    "    test_doc_raw_slice.append(preprocess_text(str(split)))\n",
    "\n",
    "test_doc_raw_sentence = []\n",
    "for split in spacy_data(test_document).sents:\n",
    "    test_doc_raw_sentence.append(str(split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use nltk tokenize to slice sentences\n",
    "from nltk import tokenize\n",
    "\n",
    "#slice test document to sentences\n",
    "test_doc_raw_slice = []\n",
    "for split in tokenize.sent_tokenize(test_document):\n",
    "    test_doc_raw_slice.append(preprocess_text(str(split)))\n",
    "\n",
    "test_doc_raw_sentence = []\n",
    "for split in tokenize.sent_tokenize(test_document):\n",
    "    test_doc_raw_sentence.append(str(split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_hits = []\n",
    "for sentence in test_doc_raw_slice:\n",
    "    # test doc Sätze vs hit_corpus \n",
    "    test_vec = dictionary_hits.doc2bow(sentence)\n",
    "    # convert to lda space\n",
    "    test_vec_lda = hit_lda[test_vec]\n",
    "    sim_hits.append(corpus_hit_index[test_vec_lda])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elm in list(enumerate(sim_hits)):\n",
    "    title = hit_title[np.argmax(elm[1])]\n",
    "    \n",
    "    if elm[1][np.argmax(elm[1])] > 0.80:\n",
    "        print(test_doc_raw_sentence[elm[0]])\n",
    "        print(\"aus Dokument: \", title)\n",
    "        print(\"Übereinstimmung: \", elm[1][np.argmax(elm[1])])\n",
    "        print(\"  \")\n",
    "        print(\"Mehr Infos:\")\n",
    "        print(str(elm[1]).replace(\"         \", \" \").replace(\"        \", \"\"))\n",
    "        print(\"max: \", elm[1][np.argmax(elm[1])], \"position: \", np.argmax(elm[1]))\n",
    "        print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates result tags for html output\n",
    "hit_result_html = \"\"\n",
    "hit_vis = []\n",
    "hits = 0\n",
    "for elm in list(enumerate(sim_hits)):\n",
    "    title = hit_title[np.argmax(elm[1])]\n",
    "    if elm[1][np.argmax(elm[1])] < 0.60:\n",
    "        cr_level=\"zero\"\n",
    "    if elm[1][np.argmax(elm[1])] >= 0.70:\n",
    "        cr_level=\"low\"\n",
    "    if elm[1][np.argmax(elm[1])] >= 0.80:\n",
    "        cr_level=\"medium\"\n",
    "    if elm[1][np.argmax(elm[1])] >= 0.90:\n",
    "        cr_level=\"higher\"\n",
    "    if elm[1][np.argmax(elm[1])] >= 0.99:\n",
    "        cr_level=\"high\"\n",
    "    \n",
    "    if cr_level==\"zero\":\n",
    "        hit_result_html = hit_result_html+\" <t class='\"+cr_level+\"'>\"+test_doc_raw_sentence[elm[0]]+\"</t> \"\n",
    "    else:\n",
    "        hit_result_html = hit_result_html+\" <t class='\"+cr_level+\"'>\"+test_doc_raw_sentence[elm[0]]+\"<b> <a href='https://de.wikipedia.org/wiki/\"+title+\"'>\"+title+\"</a></b></t>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html output of all results\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    ".high {background-color: #F8E0E0;}\n",
    ".higher {background-color: #F8ECE0;}\n",
    ".medium {background-color: #F7F8E0;}\n",
    ".low {background-color: #E0F8E0;}\n",
    ".zero {background-color: white;}\n",
    "</style>\n",
    "\n",
    "  \"\"\"+hit_result_html+\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
