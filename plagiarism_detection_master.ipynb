{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Corpus\n",
    "\n",
    "Corpus from: https://dumps.wikimedia.org/dewiki/20200820/\n",
    "\n",
    "Sentences for comparison from: https://github.com/t-systems-on-site-services-gmbh/german-wikipedia-text-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from xml.etree.ElementTree import *\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "import os\n",
    "import pprint\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import LdaMulticore\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from smart_open import open \n",
    "import spacy\n",
    "import de_core_news_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the XML-file\n",
    "xml_file = \"data/wiki_corpus/dewiki-20200820-pages-articles-multistream.xml\"\n",
    "\n",
    "# number of documents to parse \n",
    "num_documents = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the language model from spacy\n",
    "2. Function preprocess_text(text) transforms text to preprocessed tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the language model from spacy\n",
    "spacy_data = de_core_news_md.load()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # load and tokenize text with the spacy language model\n",
    "    prep_text = spacy_data(text)\n",
    "    # list for tokens\n",
    "    prep_tokens = []\n",
    "    # for every token in text\n",
    "    for token in prep_text:\n",
    "        # remove stopwords and punctuatiuon\n",
    "        if token.pos_ != 'PUNCT' and token.is_stop == False:\n",
    "            # lemmatize and transform to lowercase\n",
    "            lemma_token = token.lemma_.lower()\n",
    "            # remove non-alphabetic tokens\n",
    "            if lemma_token.isalpha() or lemma_token == '-PRON-':\n",
    "                prep_tokens.append(lemma_token)\n",
    "    # return preprocessed text \n",
    "    return prep_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 47 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "texts = []\n",
    "index = 0\n",
    "for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):        \n",
    "    if index < 200:\n",
    "        if event == 'end' and \"text\" in elem.tag:\n",
    "            index += 1  \n",
    "            texts.append(str(elem.text))\n",
    "            elem.clear()\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_ids = {}\n",
    "index = 0\n",
    "for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):        \n",
    "    if index < 200:\n",
    "        if event == 'end' and \"title\" in elem.tag:\n",
    "            title_ids[index]=str(elem.text)\n",
    "            index += 1    \n",
    "            elem.clear()\n",
    "    else:\n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a corpus from the text contents of the XML file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Corpus is defined as a class object, so it can be called when needed.\n",
    "2. Loops through the XML-file, searching for closing \"text\" tags.\n",
    "3. Returns the text contents from these nodes in preprocessed form.\n",
    "4. Then clears the current node from memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the corpus as an object\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        # define the XML tree\n",
    "        for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):            \n",
    "            # Each document is represented as an object between <text> tags in the xml file\n",
    "            if event == 'end' and \"text\" in elem.tag:\n",
    "                # Transfom the corpus to vectors\n",
    "                yield dictionary.doc2bow(preprocess_text(elem.text))\n",
    "                # clear the node\n",
    "                elem.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole corpus is too big for this experiment and takes too long to parse through. For our proof-of-concept approach we therefore propose a function which only loops through the first i documents (text nodes) in the XML tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a smaller corpus, containing only the first i documents:\n",
    "class MyCorpus_small:\n",
    "    def __iter__(self):\n",
    "        index = 0\n",
    "        # define the XML tree\n",
    "        for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):\n",
    "            if index < num_documents:\n",
    "                # Each document is represented as an object between <text> tags in the xml file\n",
    "                if event == 'end' and \"text\" in elem.tag:\n",
    "                    # Transfom the corpus to vectors\n",
    "                    yield dictionary.doc2bow(preprocess_text(elem.text))\n",
    "                    index+=1\n",
    "                    # clear the node\n",
    "                    elem.clear()\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the corpus, without loading it into memory, this is not needed when working with the smaller corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = MyCorpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the smaller corpus, again without loading it into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_small = MyCorpus_small()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Dictionary\n",
    "\n",
    "To further work with the corpus in vector form, we need to build a dictionary. \n",
    "\n",
    "This function needs to be called only once, since we are able to save the dictionary created by it and load it in future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(xml_file):\n",
    "    index = 0\n",
    "    first_elem = True\n",
    "    # loop through all nodes\n",
    "    for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):        \n",
    "        if index < num_documents:\n",
    "            # check if current node contains a document\n",
    "            if event == \"end\" and \"text\" in elem.tag:\n",
    "                # preprocess the text\n",
    "                text = preprocess_text(elem.text)\n",
    "                # if this is the first document found, create a new dictionary with it\n",
    "                if first_elem:\n",
    "                    dictionary = Dictionary([text])\n",
    "                    first_elem = False\n",
    "                    index += 1\n",
    "                # all documents after the first one get appended to the dictionary\n",
    "                else:\n",
    "                    dictionary.add_documents([text])\n",
    "                    index += 1\n",
    "                # clear the node\n",
    "                elem.clear()\n",
    "        else:\n",
    "            break\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DO NOT RUN THE FOLLOWING CODE IF THE DICTIONARY CAN BE LOADED FROM A FILE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build the dictionary:\n",
    "dictionary = build_dictionary(xml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 43 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# remove words that appear only once\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "dictionary.filter_tokens(once_ids)\n",
    "# remove gaps in id sequence after words that were removed\n",
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dictionary\n",
    "dictionary.save('data/wiki_200_new.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CONTINUE HERE TO LOAD THE DICTIONARY__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dictionary\n",
    "dictionary = Dictionary.load('data/wiki_200_new.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(20308 unique tokens: ['abc', 'abkehr', 'ablehnen', 'abrufen', 'abschluss']...)\n"
     ]
    }
   ],
   "source": [
    "# check if the dictionary has been loaded \n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity with LDA (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "* corpus: the corpus\n",
    "* num_topics: topics to be extracted from the training corpus\n",
    "* id2word: id to word mapping, the dictionary\n",
    "* workers: number of cpu cores used\n",
    "\n",
    "The trained model can be stored and loaded, as same as the dictionary before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 309. MiB for an array with shape (79077, 8, 64, 2) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32me:\\python3_8\\lib\\site-packages\\gensim\\models\\ldamulticore.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics, dtype)\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"auto-tuning alpha not implemented in multicore LDA; use plain LdaModel.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         super(LdaMulticore, self).__init__(\n\u001b[0m\u001b[0;32m    180\u001b[0m             \u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\python3_8\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\python3_8\\lib\\site-packages\\gensim\\models\\ldamulticore.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, corpus, chunks_as_numpy)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[0mchunk_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunks_as_numpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mchunk_no\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_stream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m                 \u001b[0mreallen\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# keep track of how many documents we've processed so far\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\python3_8\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mchunkize_serial\u001b[1;34m(iterable, chunksize, as_numpy, dtype)\u001b[0m\n\u001b[0;32m   1177\u001b[0m             \u001b[0mwrapped_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1179\u001b[1;33m             \u001b[0mwrapped_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1180\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mwrapped_chunk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-638fec04bcaf>\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"text\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                     \u001b[1;31m# Transfom the corpus to vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                     \u001b[1;32myield\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m                     \u001b[0mindex\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                     \u001b[1;31m# clear the node\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-9cda3b37c685>\u001b[0m in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# load and tokenize text with the spacy language model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mprep_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m# list for tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprep_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\python3_8\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__call__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.predict\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.greedy_parse\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32me:\\python3_8\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\python3_8\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserModel.begin_update\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserStepModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.precompute_hiddens.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32me:\\python3_8\\lib\\site-packages\\spacy\\_ml.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m    181\u001b[0m         )\n\u001b[0;32m    182\u001b[0m         \u001b[0mYf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mYf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mYf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[0mYf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_padding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mYf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdY_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\python3_8\\lib\\site-packages\\spacy\\_ml.py\u001b[0m in \u001b[0;36m_add_padding\u001b[1;34m(self, Yf)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_add_padding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m         \u001b[0mYf_padded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mYf_padded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32me:\\python3_8\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 309. MiB for an array with shape (79077, 8, 64, 2) and data type float32"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda = LdaMulticore(corpus_small, num_topics=100, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First experiments have shown that a topic number of 10 (default) is too low. 100 resulted in better disctinction between the different articles.\n",
    "__Further fine tuning needed here__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the trained model\n",
    "lda.save(\"data/lda_model_200.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the trained model\n",
    "lda = LdaModel.load(\"data/lda_model_200.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index the corpus with the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 309. MiB for an array with shape (79076, 1024) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32me:\\python3_8\\lib\\site-packages\\gensim\\interfaces.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    178\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mtransformed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-638fec04bcaf>\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"text\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                     \u001b[1;31m# Transfom the corpus to vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                     \u001b[1;32myield\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m                     \u001b[0mindex\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                     \u001b[1;31m# clear the node\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-9cda3b37c685>\u001b[0m in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# load and tokenize text with the spacy language model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mprep_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m# list for tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprep_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\python3_8\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__call__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.predict\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.greedy_parse\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32me:\\python3_8\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\python3_8\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserModel.begin_update\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserStepModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.precompute_hiddens.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32me:\\python3_8\\lib\\site-packages\\spacy\\_ml.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         Yf = self.ops.gemm(\n\u001b[0m\u001b[0;32m    180\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnF\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnO\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrans2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         )\n",
      "\u001b[1;32me:\\python3_8\\lib\\site-packages\\thinc\\neural\\ops.pyx\u001b[0m in \u001b[0;36mthinc.neural.ops.NumpyOps.gemm\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32me:\\python3_8\\lib\\site-packages\\thinc\\neural\\ops.pyx\u001b[0m in \u001b[0;36mthinc.neural.ops.NumpyOps.allocate\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 309. MiB for an array with shape (79076, 1024) and data type float32"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus_index = similarities.MatrixSimilarity(list(lda[corpus_small]), num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beispiele hier einlesen\n",
    "## Similarity Check\n",
    "\n",
    "Now that we have a LDA model and an index we can check the similarity of an input document against all documents in our corpus.\n",
    "First we have to define an input document, in this case we took a text from our corpus to see if the expected similarity of 1.0 can be computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define document to use in similarity check\n",
    "test_document = texts[136]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das '''archimedische Prinzip''' ist nach dem vor über 2000 Jahren lebenden griechischen Gelehrten [[Archimedes]] benannt, der als erster diesen Sachverhalt formulierte,<ref>{{Literatur |Autor=Acott, CJ |Titel=The diving \"Law-ers\": A brief resume of their lives. |Datum=1999 |Online=http://archive.rubicon-foundation.org/5990 |Abruf=2020-03-03}}</ref> und zwar als 16. Proposition in seinem Werk ''Über die schwimmenden Körper''<ref>{{Literatur |Autor=Károly Simonyi |Titel=Kulturgeschichte der Physik |Verlag=Harri Deutsch, Thun |Ort=Frankfurt a. M. |Datum=1995 |ISBN=3-8171-1379-X |Seiten=89f}} Archimedes formulierte gem. Simonyi: „Jeder beliebige Körper, der leichter als das Wasser ist, strebt beim Eintauchen mit einer Kraft nach oben, die sich aus der Differenz zwischen dem gewicht des vom Körper verdrängten Wassers und dem Gewicht des Körpers selbst ergibt. Ist der Körper jedoch schwerer als das Wasser, dann wird er mit einer Kraft nach unten gezogen, die sich aus der Differenz des Körpergewichtes zum Gewicht des von ihm verdrängten Wassers ergibt.“</ref> Es lautet:\n",
      "\n",
      "{{Zitat|Der [[Statischer Auftrieb|statische Auftrieb]] eines Körpers in einem Medium ist genauso groß wie die [[Gewichtskraft]] des vom Körper verdrängten Mediums.}}\n",
      "\n",
      "[[Datei:Auftrieb.svg|miniatur|Bild 1: schematisierter Auftrieb]] \n",
      "Das archimedische Prinzip gilt in allen [[Fluid]]en, d.&nbsp;h. in guter Näherung in [[Flüssigkeit]]en und in [[Gas]]en. [[Schiff]]e verdrängen Wasser und erhalten dadurch Auftrieb. Da die mittlere [[Dichte]] eines Schiffes geringer als die Dichte von Wasser ist, schwimmt es an der Oberfläche. Auch [[Ballon]]e und [[Luftschiff]]e machen sich diese Eigenschaft zunutze. Sie werden mit einem Gas befüllt, dessen Dichte geringer ist als die der umgebenden Luft. Diese Gase (z.&nbsp;B. [[Helium]] oder [[Wasserstoff]]) sind bei vielen Luftschiffen und Ballonen von Natur aus weniger dicht als Luft; in [[Heißluftballon]]s und [[Heißluft-Luftschiff]]en wird die Luftfüllung mit Hilfe von Gasbrennern erwärmt, wodurch ihre Dichte abnimmt.\n",
      "\n",
      "== Erklärung des Phänomens ==\n",
      "[[Datei:Auftrieb Archimedes 1.svg|miniatur|Bild 2: Die Kraft (b) an der Unterseite (der Druck im Wasser) ist größer als die Kraft (a) an der Oberseite. Die seitlichen Kräfte (c und d) sind für den Auftrieb ohne Bedeutung]]\n",
      "[[Datei:Auftrieb2.svg|miniatur|Bild 3: Die Kraft, die auf einen Punkt wirkt (in Flüssigkeiten oder Gasen) ist in alle Richtungen gleich groß.]]\n",
      "\n",
      "In vereinfachter Sichtweise liegt die Ursache für die Auftriebskraft darin, dass der [[Hydrostatischer Druck|hydrostatische Druck]] an der Oberseite bzw. der Unterseite eines eingetauchten Körpers unterschiedlich ist. Aus diesem [[Druck (Physik)|Druckunterschied]] resultieren unterschiedlich große Kräfte auf Unter- und Oberseite des eingetauchten Körpers, auf die Unterseite wirkt eine größere Kraft als auf die weiter oben befindlichen Teile der Oberfläche.\n",
      "\n",
      "=== Beispielrechnung ===\n",
      "Im Beispiel (Bild 1) gehen wir von einem Würfel mit 20&nbsp;cm Kantenlänge aus. Er ist 10&nbsp;cm tief unter die Wasseroberfläche eingetaucht. \n",
      "\n",
      "==== Berechnung über Druckunterschiede ====\n",
      "Der Druck, den 1 m Wassersäule erzeugt, beträgt <math>9810~\\frac{\\mathrm N}{{\\mathrm m}^2}= 9810~\\mathrm{Pa}</math>. An der Oberseite des Körpers mit <math>0{,}1~\\mathrm m</math> Wassersäule herrscht also <math>981~\\mathrm{Pa}</math>, an der Unterseite bei <math>0{,}3~\\mathrm m</math> Wassersäule ergibt sich <math>2943~\\mathrm{Pa}</math>. Der Luftdruck addiert sich zu beiden Werten und muss in der weiteren Rechnung nicht berücksichtigt werden.\n",
      "\n",
      "Auf die untere Fläche (Bild 1) <math>A_\\text{unten}</math> wirkt somit die Kraft \n",
      ":<math>F_\\text{unten} = 2943~\\frac{\\mathrm N}{{\\mathrm m}^2} \\cdot 0{,}04\\,\\mathrm{m}^2 = \\underline{117{,}72\\,\\mathrm{N}}</math>\n",
      "\n",
      "nach oben. Auf die obere Fläche <math>A_\\text{oben}</math> wirkt dagegen die Kraft\n",
      ":<math>F_\\text{oben} = 981~\\frac{\\mathrm N}{{\\mathrm m}^2} \\cdot 0{,}04\\,\\mathrm{m}^2 = \\underline{39{,}24\\,{\\rm N}}</math>\n",
      "\n",
      "nach unten. Die Differenz der beiden Kräfte, also der Auftrieb dieses Körpers berechnet sich also zu\n",
      ":<math>F_\\text{Auftrieb} = F_\\text{unten} - F_\\text{oben} = 117{,}72\\,\\mathrm{N} - 39{,}24\\,\\mathrm{N} =\\underline{78{,}48\\,\\mathrm{N}}</math>.\n",
      "\n",
      "==== Berechnung mit Hilfe des archimedischen Prinzips ====\n",
      "Nach Archimedes gilt Folgendes:\n",
      "<math>F_\\text{Auftrieb} = F_{\\text{Gewicht, Fluid}}</math>.\n",
      "Bezogen auf das Beispiel (Bild 1) können wir schreiben:\n",
      "\n",
      ":<math>\\begin{align} F_\\text{Auftrieb} &= V_\\mathrm{verdr\\ddot angt} \\cdot \\rho_\\text{Fluid} \\cdot g \\\\\n",
      "&= 8000\\,\\mathrm{cm}^3 \\cdot 1 \\, \\frac{g}{\\mathrm{cm}^3} \\cdot 9{,}81 \\cdot10^{-3}\\,\\frac{\\mathrm N}{g} \\\\  &= \\underline{78{,}48\\,\\mathrm{N}}\\end{align}</math>\n",
      "\n",
      "Dabei wurde die Dichte <math>\\rho_\\text{Fluid}</math> des Fluids, die Beziehung <math> \\rho = \\frac{m}{V}</math> zur [[Masse (Physik)|Masse]] <math>m</math> und zum [[Volumen]] <math>V</math>, und der [[Ortsfaktor]] <math>g</math> verwendet. Wir sehen, dass beide Methoden zum selben Ergebnis führen.\n",
      "\n",
      "=== Gedankenexperiment ===\n",
      "Folgendes [[Gedankenexperiment]] veranschaulicht die Richtigkeit des archimedischen Prinzips. Dazu stelle man sich ein ruhendes Fluid vor. Innerhalb des Fluids sei ein beliebiger Teil des Fluids markiert. Die Markierung kann man sich wie eine Art Wasserballon in einem Behälter Wasser vorstellen, nur dass die Haut dieses Wasserballons unendlich dünn und massenlos ist und eine beliebige Form annehmen kann.\n",
      "\n",
      "Man stellt nun fest, dass der so markierte Teil des Fluids innerhalb des Fluids weder steigt noch sinkt, da sich das gesamte Fluid in Ruhe befindet – der markierte Teil schwebt sozusagen schwerelos im ihn umgebenden Fluid. Das bedeutet, dass die Auftriebskraft des markierten Fluidteils exakt sein Gewicht kompensiert. Daraus kann gefolgert werden, dass die Auftriebskraft des markierten Fluidteils genau seiner Gewichtskraft entspricht. Da die Markierung innerhalb des Fluids beliebig ist, ist somit die Richtigkeit des archimedischen Prinzips für homogene Fluide gezeigt.\n",
      "\n",
      "=== Steigen, sinken, schweben ===\n",
      "Damit der Körper die in der Grafik beschriebene Position beibehält, muss seine Gewichtskraft gleich der Gewichtskraft des verdrängten Wassers (78,48&nbsp;N) sein. Dann heben sich alle auf den Körper wirkenden Kräfte auf und dieser kommt zum Stillstand.\n",
      "Nach der Formel <math>m = \\tfrac{F_\\text{Gewicht}}{g}</math> muss der Körper 8.000&nbsp;g schwer sein. Des Weiteren hätte er nach <math> \\rho = \\tfrac{m}{V}</math> eine Dichte von 1&nbsp;kg/dm<sup>3</sup>, also die Dichte von Wasser.\n",
      "\n",
      "Wir können also folgende Regel formulieren:\n",
      "* Wenn <math>\\rho_\\mathrm{K\\ddot orper} = \\rho_\\text{Fluid}</math> ist, dann schwebt der Körper.\n",
      "* Wenn <math>\\rho_\\mathrm{K\\ddot orper} < \\rho_\\text{Fluid}</math> ist, dann steigt der Körper. \n",
      "* Wenn <math>\\rho_\\mathrm{K\\ddot orper} > \\rho_\\text{Fluid}</math> ist, dann sinkt der Körper.\n",
      "Die Körper steigen oder sinken, bis der Gewichtskraft eine betragsmäßig gleich große Kraft entgegenwirkt. Dies kann beim Sinken eine sich ändernde Dichte des Fluids oder auch der Boden des Bechers bewirken. Ein Körper steigt oft so lange, bis er die Oberfläche durchbricht. In diesem Fall gilt: \n",
      ":<math>V_\\text{eingetaucht} \\cdot \\rho_\\text{Fluid} = V_\\mathrm{K\\ddot orper} \\cdot \\rho_\\mathrm{K\\ddot orper}</math>\n",
      "\n",
      "== Entdeckung des archimedischen Prinzips ==\n",
      "[[Datei:Experiment physique principe d Archimede.jpg|mini|Experiment zum Beweis des archimedischen Prinzips, Illustration von 1547]]\n",
      "[[Archimedes]] war von König [[Hieron II. von Syrakus|Hieron II.]] von [[Syrakus]] beauftragt worden, herauszufinden, ob dessen Krone wie bestellt aus reinem [[Gold]] wäre oder ob das Material durch billigeres Metall gestreckt worden sei. Diese Aufgabe stellte Archimedes zunächst vor Probleme, da die Krone natürlich nicht zerstört werden durfte.\n",
      "\n",
      "Der Überlieferung nach hatte Archimedes schließlich den rettenden Einfall, als er zum Baden in eine bis zum Rand gefüllte Wanne stieg und dabei das Wasser überlief. Er erkannte, dass die Menge Wasser, die übergelaufen war, genau seinem Körpervolumen entsprach. Angeblich lief er dann, nackt wie er war, durch die Straßen und rief ''„[[Heureka]]!“ („Ich habe es gefunden“)''.\n",
      "\n",
      "Um die gestellte Aufgabe zu lösen, tauchte er einmal die Krone und dann einen [[Gold]]barren, der genauso viel wog wie die Krone, in einen bis zum Rand gefüllten Wasserbehälter und maß die Menge des überlaufenden Wassers. Da die Krone mehr Wasser verdrängte als der Goldbarren und somit bei gleichem Gewicht voluminöser war, musste sie aus einem Material geringerer Dichte, also nicht aus reinem Gold, gefertigt worden sein.\n",
      "\n",
      "Diese Geschichte wurde vom [[Römisches Reich|römischen]] [[Architekt]]en [[Vitruv]] überliefert.\n",
      "\n",
      "Obwohl der Legende nach auf dieser Geschichte die Entdeckung des archimedischen Prinzips beruht, würde der Versuch von Archimedes auch mit jeder anderen Flüssigkeit funktionieren. Das Interessanteste am archimedischen Prinzip, nämlich die Entstehung des Auftriebs und damit die Berechnung der Dichte des Fluids, spielt in dieser Entdeckungsgeschichte gar keine Rolle.\n",
      "\n",
      "== Physikalische Herleitung ==\n",
      "Ein Körper wird vom Druck <math>p>0</math> belastet, welchen das umgebende Medium (Flüssigkeit oder Gas) auf seine Oberfläche ausübt. Ein betrachtetes Teilstück der Oberfläche mit dem Inhalt <math>A</math> sei so klein gewählt, dass es praktisch eben ist und dass in seinem Bereich der Druck <math>p</math> konstant ist. Der Einheitsvektor der äußeren Flächennormale der Teilfläche sei <math>\\vec e_n</math>. Das Medium übt dann die Kraft\n",
      " \n",
      ":<math>\n",
      "\\vec F_A=-pA\\vec e_n\n",
      "</math>\n",
      "\n",
      "auf das Teilstück aus. Eine Summierung dieser Kräfte über alle Teilstücke liefert die gesamte Auftriebskraft.\n",
      "[[Datei:ArchimedischesPrinzipHerleitung.svg|miniatur|Zur Herleitung des archimedischen Prinzips]]\n",
      "[[Datei:Tragkraft.jpg|miniatur|Tragkraft eines mit [[Wasserstoff]] gefüllten Gummiballons]]\n",
      "\n",
      "Das archimedische Prinzip gilt nur genau dann streng, wenn das verdrängte Medium [[inkompressibel]] (nicht zusammendrückbar) ist. Für Flüssigkeiten wie z.&nbsp;B. Wasser ist dies gut erfüllt, daher soll im Folgenden von einem Körper ausgegangen werden, der in eine Flüssigkeit der (genau genommen von der Temperatur abhängigen) Dichte <math>\\rho</math> eintaucht.\n",
      "\n",
      "In der Flüssigkeit lastet auf einer waagerechten Fläche der Größe <math>A</math> in der Tiefe <math>z</math> das Gewicht einer Flüssigkeitssäule der Masse <math>m=\\rho A z</math>. Der Druck in dieser Tiefe ist deshalb\n",
      "\n",
      ":<math>\n",
      "p(z) = \\frac{m g}{A} = \\rho g z\n",
      "</math>.\n",
      "\n",
      "Ein entsprechender Druckverlauf gilt bei nicht zu großen Höhendifferenzen <math>z</math> auch in der Luft oder anderen Gasen (d.&nbsp;h. die [[Kompressibilität]] fällt nicht ins Gewicht; bei großen Höhenunterschieden müsste eine veränderliche Dichte berücksichtigt werden). Deshalb gelten die folgenden Überlegungen auch für realistisch große Luftschiffe oder Ballone.\n",
      "\n",
      "Für einfache geometrische Formen kann man die Gültigkeit des archimedischen Prinzips mit einfachen Mitteln von Hand nachrechnen. Für einen Quader mit Grundfläche <math>A</math> und Höhe <math>h</math>, der senkrecht in die Flüssigkeit eintaucht, erhält man beispielsweise:\n",
      "* Kraft auf die obere Grundfläche mit der Flächennormalen <math>-\\vec e_z</math>: \n",
      "*:<math>\\vec F_\\text{o} = -p(z_0) A(-\\vec e_z) = \\rho g z_0 A\\vec e_z</math>\n",
      "* Kraft auf die untere Grundfläche mit der Flächennormalen <math>\\vec e_z</math>: \n",
      "*:<math>\\vec F_\\text{u} = -p(z_0+h) A \\vec e_z= -\\rho g (z_0+h) A\\vec e_z</math>\n",
      "* Kräfte auf die Seitenflächen heben sich stets gegenseitig auf.\n",
      "* Die gesamte Auftriebskraft ist also \n",
      "*:<math>\\vec F = \\vec F_\\text{o} + \\vec F_\\text{u} = -\\rho g h A \\vec e_z = -\\rho g V \\vec e_z. </math>\n",
      "\n",
      "Dabei ist <math>V</math> das verdrängte Volumen, also <math>\\rho V</math> die verdrängte Masse und <math>\\rho V g</math> ihre Gewichtskraft. Das archimedische Prinzip ist also erfüllt. Das negative Vorzeichen entfällt, wenn die <math>z</math>-Achse nach oben gewählt wird.\n",
      "\n",
      "Für einen beliebig geformten Körper <math>\\mathcal{V}</math> erhält man die gesamte Auftriebskraft durch das Oberflächenintegral\n",
      "\n",
      ":<math>\\vec F=-\\oint_{\\partial\\mathcal{V}}p\\,\\mathrm{d}\\vec A.</math>\n",
      "\n",
      "Mit dem [[Gau%C3%9Fscher_Integralsatz#Folgerungen|Integralsatz]] \n",
      ": <math>\\oint_{\\partial\\mathcal{V}}p\\,\\mathrm{d}\\vec A=\\int_{\\mathcal{V}}\\operatorname{grad}p\\,\\mathrm{d}V</math> \n",
      "und <math>\\operatorname{grad}p=\\rho g \\vec e_z </math> folgt daraus\n",
      "\n",
      ":<math>\\vec F=-\\rho g V\\vec e_z</math>.\n",
      "\n",
      "== Weblinks ==\n",
      "{{Commonscat|Buoyancy|Auftrieb}}\n",
      "* [http://www.walter-fendt.de/html5/phde/buoyantforce_de.htm Interaktives Experiment zur Größe der Auftriebskraft auf einen Körper, der in eine Flüssigkeit taucht]\n",
      "* [http://www.mister-mueller.de/physik/Ph_unterricht/Ph_Experimente/Mechanik/Experiment_des_Archimedes.html Nachbau des historischen Experimentes mit einfachen Mitteln]\n",
      "\n",
      "== Einzelnachweise ==\n",
      "<references />\n",
      "\n",
      "{{Normdaten|TYP=s|GND=4137273-6}}\n",
      "\n",
      "[[Kategorie:Klassische Mechanik]]\n",
      "[[Kategorie:Archimedes]]\n"
     ]
    }
   ],
   "source": [
    "print(test_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transform the document to vector space\n",
    "test_vec = dictionary.doc2bow(preprocess_text(test_document))\n",
    "# convert to lda space\n",
    "test_vec_lda = lda[test_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the similarities\n",
    "sims = corpus_index[test_vec_lda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0), (9, 0.0), (10, 0.0), (11, 0.0), (12, 0.0), (13, 0.0), (14, 0.0), (15, 0.0), (16, 0.0), (17, 0.0), (18, 0.0), (19, 0.0), (20, 0.0), (21, 0.0), (22, 0.0), (23, 0.0), (24, 0.0), (25, 0.0), (26, 0.0), (27, 0.0), (28, 0.0), (29, 0.0), (30, 0.0), (31, 0.0), (32, 0.0), (33, 0.0), (34, 0.0), (35, 0.0), (36, 0.0), (37, 0.0), (38, 0.05845186), (39, 0.0), (40, 0.0), (41, 0.012841652), (42, 0.0), (43, 0.0), (44, 0.0), (45, 0.0), (46, 0.0), (47, 0.0), (48, 0.0), (49, 0.0), (50, 0.0), (51, 0.0), (52, 0.0), (53, 0.0), (54, 0.0), (55, 0.0), (56, 0.0), (57, 0.0), (58, 0.0), (59, 0.0), (60, 0.0), (61, 0.0), (62, 0.0), (63, 0.0), (64, 0.0), (65, 0.0), (66, 0.0), (67, 0.0), (68, 0.0), (69, 0.0), (70, 0.0), (71, 0.0), (72, 0.0), (73, 0.06601354), (74, 0.0), (75, 0.0), (76, 0.0), (77, 0.0), (78, 0.0), (79, 0.0), (80, 0.0), (81, 0.0), (82, 0.0), (83, 0.0), (84, 0.0), (85, 0.0), (86, 0.0), (87, 0.0), (88, 0.0), (89, 0.0), (90, 0.0), (91, 0.0), (92, 0.0), (93, 0.0), (94, 0.0), (95, 0.0), (96, 0.0), (97, 0.0), (98, 0.0), (99, 0.0), (100, 0.0), (101, 0.0), (102, 0.0), (103, 0.0), (104, 0.0), (105, 0.0), (106, 0.0), (107, 0.0), (108, 0.0), (109, 0.0), (110, 0.0), (111, 0.0), (112, 0.0), (113, 0.0), (114, 0.0), (115, 0.0), (116, 0.0), (117, 0.0), (118, 0.0), (119, 0.0), (120, 0.0), (121, 0.0), (122, 0.24961066), (123, 0.0), (124, 0.0), (125, 0.0), (126, 0.0), (127, 0.0), (128, 0.0), (129, 0.0), (130, 0.0), (131, 0.0), (132, 0.0), (133, 0.0), (134, 0.0), (135, 0.0), (136, 1.0), (137, 0.0), (138, 0.0), (139, 0.0), (140, 0.0), (141, 0.0), (142, 0.0), (143, 0.0), (144, 0.0), (145, 0.0), (146, 0.0), (147, 0.0), (148, 0.0), (149, 0.0), (150, 0.0), (151, 0.0), (152, 0.0), (153, 0.0), (154, 0.0), (155, 0.0), (156, 0.0), (157, 0.0), (158, 0.0), (159, 0.0), (160, 0.0), (161, 0.0), (162, 0.0), (163, 0.0), (164, 0.0), (165, 0.0), (166, 0.0), (167, 0.0), (168, 0.0), (169, 0.0), (170, 0.0), (171, 0.0), (172, 0.0), (173, 0.0), (174, 0.0), (175, 0.0), (176, 0.0), (177, 0.0), (178, 0.0), (179, 0.0), (180, 0.0), (181, 0.0), (182, 0.0), (183, 0.0), (184, 0.0), (185, 0.0), (186, 0.0), (187, 0.0), (188, 0.0), (189, 0.0), (190, 0.0), (191, 0.0), (192, 0.0), (193, 0.0), (194, 0.0), (195, 0.0), (196, 0.0), (197, 0.0), (198, 0.0), (199, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "sims = corpus_index[test_vec_lda]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Übereinstimmung von  100.00 % \n",
      " Document ID: 0 Title: Alan Smithee \n",
      " ------------------------------------\n",
      "Übereinstimmung von  100.00 % \n",
      " Document ID: 136 Title: Archimedisches Prinzip \n",
      " ------------------------------------\n",
      "2 Plagiatsfälle gefunden\n"
     ]
    }
   ],
   "source": [
    "hits = 0\n",
    "for ids in list(enumerate(sims)):\n",
    "    if ids[1] >= 0.75:\n",
    "        hits += 1\n",
    "        title = title_ids.get(ids[0])\n",
    "        print(\"Übereinstimmung von \",\"%.2f\" %(ids[1]*100),\"%\",\"\\n\",\"Document ID:\",ids[0], \"Title:\", title,\"\\n\", \"------------------------------------\")\n",
    "print(hits, \"Plagiatsfälle gefunden\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
