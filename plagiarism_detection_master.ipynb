{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Corpus\n",
    "\n",
    "Corpus from: https://dumps.wikimedia.org/dewiki/20200820/\n",
    "\n",
    "Sentences for comparison from: https://github.com/t-systems-on-site-services-gmbh/german-wikipedia-text-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from xml.etree.ElementTree import *\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "import os\n",
    "import pprint\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import LdaMulticore\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from smart_open import open \n",
    "import spacy\n",
    "import de_core_news_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the XML-file\n",
    "xml_file = \"data/wiki_corpus/dewiki-20200820-pages-articles-multistream.xml\"\n",
    "\n",
    "# number of documents to parse \n",
    "num_documents = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the language model from spacy\n",
    "2. Function preprocess_text(text) transforms text to preprocessed tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the language model from spacy\n",
    "spacy_data = de_core_news_md.load()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # load and tokenize text with the spacy language model\n",
    "    prep_text = spacy_data(text)\n",
    "    # list for tokens\n",
    "    prep_tokens = []\n",
    "    # for every token in text\n",
    "    for token in prep_text:\n",
    "        # remove stopwords and punctuatiuon\n",
    "        if token.pos_ != 'PUNCT' and token.is_stop == False:\n",
    "            # lemmatize and transform to lowercase\n",
    "            lemma_token = token.lemma_.lower()\n",
    "            # remove non-alphabetic tokens\n",
    "            if lemma_token.isalpha() or lemma_token == '-PRON-':\n",
    "                prep_tokens.append(lemma_token)\n",
    "    # return preprocessed text \n",
    "    return prep_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For experimental purposes:\n",
    "\n",
    "Load the texts into a list\n",
    "\n",
    "__NOT MEMORY FRIENDLY__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "texts = []\n",
    "index = 0\n",
    "for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):        \n",
    "    if index < 20:\n",
    "        if event == 'end' and \"text\" in elem.tag:\n",
    "            index += 1  \n",
    "            texts.append(str(elem.text))\n",
    "            elem.clear()\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to return the title of a given article later on, we need to store those in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_ids = {}\n",
    "index = 0\n",
    "for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):        \n",
    "    if index < 200:\n",
    "        if event == 'end' and \"title\" in elem.tag:\n",
    "            title_ids[index]=str(elem.text)\n",
    "            index += 1    \n",
    "            elem.clear()\n",
    "    else:\n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a corpus from the text contents of the XML file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Corpus is defined as a class object, so it can be called when needed.\n",
    "2. Loops through the XML-file, searching for closing \"text\" tags.\n",
    "3. Returns the text contents from these nodes in preprocessed form.\n",
    "4. Then clears the current node from memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the corpus as an object\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        # define the XML tree\n",
    "        for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):            \n",
    "            # Each document is represented as an object between <text> tags in the xml file\n",
    "            if event == 'end' and \"text\" in elem.tag:\n",
    "                # Transfom the corpus to vectors\n",
    "                yield dictionary.doc2bow(preprocess_text(elem.text))\n",
    "                # clear the node\n",
    "                elem.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole corpus is too big for this experiment and takes too long to parse through. For our proof-of-concept approach we therefore propose a function which only loops through the first i documents (text nodes) in the XML tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a smaller corpus, containing only the first i documents:\n",
    "class MyCorpus_small:\n",
    "    def __iter__(self):\n",
    "        index = 0\n",
    "        # define the XML tree\n",
    "        for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):\n",
    "            if index < num_documents:\n",
    "                # Each document is represented as an object between <text> tags in the xml file\n",
    "                if event == 'end' and \"text\" in elem.tag:\n",
    "                    # Transfom the corpus to vectors\n",
    "                    yield dictionary.doc2bow(preprocess_text(elem.text))\n",
    "                    index+=1\n",
    "                    # clear the node\n",
    "                    elem.clear()\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the corpus, without loading it into memory, this is not needed when working with the smaller corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = MyCorpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the smaller corpus, again without loading it into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_small = MyCorpus_small()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Dictionary\n",
    "\n",
    "To further work with the corpus in vector form, we need to build a dictionary. \n",
    "\n",
    "This function needs to be called only once, since we are able to save the dictionary created by it and load it in future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(xml_file):\n",
    "    index = 0\n",
    "    first_elem = True\n",
    "    # loop through all nodes\n",
    "    for event, elem in ET.iterparse(xml_file, events = (\"start\", \"end\")):        \n",
    "        if index < num_documents:\n",
    "            # check if current node contains a document\n",
    "            if event == \"end\" and \"text\" in elem.tag:\n",
    "                # preprocess the text\n",
    "                text = preprocess_text(elem.text)\n",
    "                # if this is the first document found, create a new dictionary with it\n",
    "                if first_elem:\n",
    "                    dictionary = Dictionary([text])\n",
    "                    first_elem = False\n",
    "                    index += 1\n",
    "                # all documents after the first one get appended to the dictionary\n",
    "                else:\n",
    "                    dictionary.add_documents([text])\n",
    "                    index += 1\n",
    "                # clear the node\n",
    "                elem.clear()\n",
    "        else:\n",
    "            break\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DO NOT RUN THE FOLLOWING CODE IF THE DICTIONARY CAN BE LOADED FROM A FILE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build the dictionary:\n",
    "dictionary = build_dictionary(xml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 43 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# remove words that appear only once\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "dictionary.filter_tokens(once_ids)\n",
    "# remove gaps in id sequence after words that were removed\n",
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dictionary\n",
    "dictionary.save('data/wiki_200_new.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CONTINUE HERE TO LOAD THE DICTIONARY__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dictionary\n",
    "dictionary = Dictionary.load('data/wiki_200_new.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(20308 unique tokens: ['abc', 'abkehr', 'ablehnen', 'abrufen', 'abschluss']...)\n"
     ]
    }
   ],
   "source": [
    "# check if the dictionary has been loaded \n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity with LDA (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "* corpus: the corpus\n",
    "* num_topics: topics to be extracted from the training corpus\n",
    "* id2word: id to word mapping, the dictionary\n",
    "* workers: number of cpu cores used\n",
    "\n",
    "The trained model can be stored and loaded, as same as the dictionary before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda = LdaMulticore(corpus_small, num_topics=200, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First experiments have shown that a topic number of 10 (default) is too low. 100 resulted in better disctinction between the different articles.\n",
    "__Further fine tuning needed here__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the trained model\n",
    "lda.save(\"data/lda_model_200_t200.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the trained model\n",
    "lda = LdaModel.load(\"data/lda_model_200_t200.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index the corpus with the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus_index = similarities.MatrixSimilarity(list(lda[corpus_small]), num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the index\n",
    "corpus_index.save(\"data/lda_index_200_t200.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.similarities.docsim.MatrixSimilarity at 0x288a650fc40>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the index from disk\n",
    "corpus_index.load(\"data/lda_index_200_t200.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beispiele hier einlesen\n",
    "## Similarity Check\n",
    "\n",
    "Now that we have a LDA model and an index we can check the similarity of an input document against all documents in our corpus.\n",
    "First we have to define an input document, in this case we took a text from our corpus to see if the expected similarity of 1.0 can be computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define document to use in similarity check\n",
    "test_document = texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'''Alan Smithee''' steht als [[Pseudonym]] für einen fiktiven Regisseur, der Filme verantwortet, bei denen der eigentliche [[Regisseur]] seinen Namen nicht mit dem Werk in Verbindung gebracht haben möchte. Von 1968 bis 2000 wurde es von der [[Directors Guild of America]] (DGA) für solche Situationen empfohlen, seither ist es '''Thomas Lee'''.<ref>[[Los Angeles Times|latimes]].com: [http://articles.latimes.com/2000/jan/15/entertainment/ca-54271 ''Name of Director Smithee Isn't What It Used to Be''], zuletzt geprüft am 2. April 2011</ref> ''Alan Smithee'' ist jedoch weiterhin in Gebrauch.\n",
      "\n",
      "Alternative Schreibweisen sind unter anderem die Ursprungsvariante ''Al'''len''' Smithee'' sowie ''Alan Sm'''y'''thee'' und ''A'''dam''' Smithee''. Auch zwei teilweise asiatisch anmutende Schreibweisen ''Alan Smi Thee'' und ''Sumishii Aran'' gehören – so die [[Internet Movie Database]] – dazu.<ref name=\"IMDb\">[http://www.imdb.com/name/nm0000647/ Eigener Eintrag für ''Alan Smithee'' in der IMDb]</ref>\n",
      "\n",
      "== Geschichte ==\n",
      "=== Entstehung ===\n",
      "Das Pseudonym entstand 1968 infolge der Arbeiten am Western-Film ''Death of a Gunfighter'' (deutscher Titel ''[[Frank Patch – Deine Stunden sind gezählt]]''). Regisseur [[Robert Totten]] und Hauptdarsteller [[Richard Widmark]] gerieten in einen Streit, woraufhin [[Don Siegel]] als neuer Regisseur eingesetzt wurde.\n",
      "\n",
      "Der Film trug nach Abschluss der Arbeiten noch deutlich Tottens [[Manier (Stil)|Handschrift]], der auch mehr Drehtage als Siegel daran gearbeitet hatte, weshalb dieser die Nennung seines Namens als Regisseur ablehnte. Totten selbst lehnte aber ebenfalls ab. Als Lösung wurde  ''Allen Smithee'' als ein möglichst einzigartiger Name gewählt.<ref>[http://www.imdb.com/name/nm0000647/bio ''Biography for Alan Smithee''] in der Internet Movie Database</ref>\n",
      "\n",
      "In den zeitgenössischen Kritiken wurde der Regisseur u.&nbsp;a. von [[Roger Ebert]] mit den Worten gelobt: {{Zitat-en|Director Allen Smithee, a name I’m not familiar with, allows his story to unfold naturally. He never preaches, and he never lingers on the obvious. His characters do what they have to do.<ref>rogerebert.[[Chicago Sun-Times|suntimes]].com: [https://www.rogerebert.com/reviews/death-of-a-gunfighter-1969 ''Death of a Gunfighter''], zuletzt geprüft am 2. April 2011</ref>|Übersetzung=Regisseur Alan Smithee, ein Name, der mir nicht vertraut ist, erlaubt es seiner Handlung, sich natürlich zu entfalten. Er predigt niemals, und er verweilt nie beim Offensichtlichen. Seine Charaktere tun, was sie tun müssen.}}\n",
      "\n",
      "=== Aufdeckung und Abkehr ===\n",
      "1997 kam die Parodie ''An Alan Smithee Film: Burn Hollywood Burn'' (deutscher Titel ''[[Fahr zur Hölle Hollywood]]'') in die Kinos, was das Pseudonym einem größeren Publikum bekannt machte, nicht zuletzt weil [[Arthur Hiller (Regisseur)|Arthur Hiller]], der eigentliche Regisseur des Films, selbst seinen Namen zurückzog und analog zum Filmtitel das Pseudonym ''Alan Smithee'' benutzte. Der Film gilt als einer der schlechtesten Filme der 1990er Jahre und gewann fünf [[Goldene Himbeere]]n.\n",
      "\n",
      "Der Film ''[[Supernova (2000)|Supernova]]'' ist der erste Post-Smithee-Film, dort führte ein gewisser ''Thomas Lee'' alias [[Walter Hill]] die Regie.\n",
      "<!-- fand nur einen für den von 1990, siehe ''[[Das Kindermädchen]]'':\n",
      "„Smithee wurde allerdings auch nach ''Supernova'' gesichtet, in einem Film namens ''The Guardian''.“\n",
      "-->\n",
      "\n",
      "== Verwendung ==\n",
      "Die Verwendung dieses oder eines anderen Pseudonyms ist für Mitglieder der DGA streng reglementiert. Ein Regisseur, der für einen von ihm gedrehten Film seinen Namen nicht hergeben möchte, hat nach Sichtung des fertigen Films drei Tage Zeit, anzuzeigen, dass er ein Pseudonym verwenden möchte. Der Rat der DGA entscheidet binnen zwei Tagen über das Anliegen. Erhebt die Produktionsfirma Einspruch, entscheidet ein Komitee aus Mitgliedern der DGA und der Vereinigung der Film- und Fernsehproduzenten, ob der Regisseur ein Pseudonym angeben darf. Über die Beantragung muss der Regisseur Stillschweigen halten, ebenso darf er den fertigen Film nicht öffentlich kritisieren, wenn die DGA ihm die Verwendung eines Pseudonyms zugesteht.<ref>Siehe zu diesen Regelungen [http://www.dga.org/~/media/Files/Contracts/Agreements/2008%20BA/008ba2008article8.pdf Artikel 8, Abschnitt 8-211 des ''Basic Agreement''] (PDF; 125&nbsp;kB) der DGA von 2008, abgerufen am 25. April 2012</ref> Ein Antrag des Regisseurs auf Pseudonymisierung kann abgelehnt werden, so durfte [[Tony Kaye (Regisseur)|Tony Kaye]] den Namen Smithee bei dem Film ''[[American History X]]'' nicht einsetzen, obwohl er den Antrag stellte.\n",
      "\n",
      "Auch bei nicht-US-amerikanischen Produktionen wird der Name verwendet, wie etwa beim [[Pilotfilm]] der Fernsehserie ''[[Schulmädchen (Fernsehserie)|Schulmädchen]]''. 2007 sendete die ARD am 8. und 9. August den zweiteiligen TV-Film ''Paparazzo''. Auch in diesem Werk erscheint anstatt des eigentlichen Regisseurs [[Stephan Wagner (Regisseur)|Stephan Wagner]] Alan Smithee im Abspann.\n",
      "\n",
      "Regisseure, die das Pseudonym benutzt haben:\n",
      "* [[Don Siegel]] und [[Robert Totten]] (für ''[[Frank Patch – Deine Stunden sind gezählt]]'')\n",
      "* [[David Lynch]] (für die dreistündige Fernsehfassung von ''[[Der Wüstenplanet (Film)|Der Wüstenplanet]]'')\n",
      "* [[Chris Christensen]] (''The Omega Imperative'')\n",
      "* [[Gianni Bozzacchi]] (für ''I Love N.Y.'')\n",
      "* [[Stuart Rosenberg]] (für ''Let’s Get Harry'')\n",
      "* [[Richard C. Sarafian]] (für ''[[Starfire]]'')\n",
      "* [[Dennis Hopper]] (für ''[[Catchfire]]'')\n",
      "* [[Arthur Hiller (Regisseur)|Arthur Hiller]] (für ''[[Fahr zur Hölle Hollywood]]'')\n",
      "* [[Rick Rosenthal]] (''Die Vögel II – Die Rückkehr'')\n",
      "* [[Kevin Yagher]] (''[[Hellraiser IV – Bloodline]]'')\n",
      "\n",
      "Der Pilotfilm der Serie ''[[MacGyver]]'' und die fünfte Folge der ersten Staffel führen einen Alan Smithee als Regisseur. Auf der TV-Serien-Seite ''TV Rage'' wird Jerrold Freedman als Regisseur des Pilotfilms angegeben. Der Regisseur der fünften Folge ist unbekannt.\n",
      "\n",
      "Zu den Drehbuchautoren, die das Pseudonym benutzt haben, gehören [[Sam Raimi]] und [[Ivan Raimi]], die das Drehbuch zu ''Die total beknackte Nuß'' als ''Alan Smithee, Jr.'' und ''Alan Smithee, Sr.'' schrieben.\n",
      "\n",
      "Auch in Computerspielen wird dieses Pseudonym angegeben: Im Abspann des Ego-Shooters ''Marine Sharpshooter IV'' aus dem Jahr 2008 wird als Art Director des Spiels ''Alan Smithee'' genannt.<ref>http://einestages.spiegel.de/external/ShowTopicAlbumBackground/a13641/l26/l0/F.html#featuredEntry</ref>\n",
      "\n",
      "2014 produzierte die [[New York City|New Yorker]] Performance-Kompanie [[Big Dance Theater]] ''Alan Smithee Directed this Play'', das im August des Jahres auch in Berlin bei [[Tanz im August]] aufgeführt wurde.<ref>''Alan Smithee ist schuld!'' in [[Frankfurter Allgemeine Sonntagszeitung]] vom 17. August 2014, Seite 36</ref>\n",
      "\n",
      "== Literatur ==\n",
      "* Jeremy Braddock, Stephen Hock (Hrsg.): ''Directed by Allen Smithee.'' Foreword by Andrew Sarris. University of Minnesota Press, Minneapolis, London 2001, ISBN 0-8166-3534-X.\n",
      "\n",
      "== Weblinks ==\n",
      "* {{IMDb|nm0000647}}\n",
      "* [http://www.abc.net.au/rn/arts/atoday/stories/s353584.htm Artikel über Smithee von ABC Online (englisch)]\n",
      "* [http://einestages.spiegel.de/static/topicalbumbackground/13641/der_mann_der_niemals_lebte.html Der Mann, der niemals lebte, Spiegel Online einestages]\n",
      "* [http://dradiowissen.de/beitrag/alan-smithee-die-film-legende-lebt Alan Smithee lebt!, DRadio Wissen]\n",
      "\n",
      "== Einzelnachweise ==\n",
      "<references />\n",
      "\n",
      "{{Normdaten|TYP=p|GND=123396956|VIAF=86737339}}\n",
      "\n",
      "[[Kategorie:Fiktive Person|Smithee, Alan]]\n",
      "[[Kategorie:Pseudonym]]\n",
      "[[Kategorie:Sammelpseudonym|Smithee, Alan]]\n",
      "[[Kategorie:Werk von Alan Smithee]]\n"
     ]
    }
   ],
   "source": [
    "print(test_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transform the document to vector space\n",
    "test_vec = dictionary.doc2bow(preprocess_text(test_document))\n",
    "# convert to lda space\n",
    "test_vec_lda = lda[test_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the similarities\n",
    "sims = corpus_index[test_vec_lda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0), (9, 0.0), (10, 0.0), (11, 0.09635989), (12, 0.22857422), (13, 0.0), (14, 0.0), (15, 0.0), (16, 0.0), (17, 0.0), (18, 0.0), (19, 0.0), (20, 0.0), (21, 0.0), (22, 0.0), (23, 0.0), (24, 0.0), (25, 0.0), (26, 0.0), (27, 0.0), (28, 0.0), (29, 0.0), (30, 0.0), (31, 0.0), (32, 0.10481228), (33, 0.030247318), (34, 0.0), (35, 0.036757674), (36, 0.0), (37, 0.0), (38, 0.0), (39, 0.012771295), (40, 0.0), (41, 0.0015398836), (42, 0.0), (43, 0.0), (44, 0.0), (45, 0.028419478), (46, 0.0), (47, 0.0), (48, 0.0020706612), (49, 0.0), (50, 0.0), (51, 0.0), (52, 0.0), (53, 0.0026773836), (54, 0.0), (55, 0.0), (56, 0.0), (57, 0.0), (58, 0.0), (59, 0.068634436), (60, 0.776972), (61, 0.0), (62, 0.027996365), (63, 0.030247318), (64, 0.0), (65, 0.0), (66, 0.0), (67, 0.0), (68, 0.0), (69, 0.0), (70, 0.0), (71, 0.0), (72, 0.0), (73, 0.00035092066), (74, 0.079071365), (75, 0.0), (76, 0.0), (77, 0.0), (78, 0.0), (79, 0.0006174115), (80, 0.0007639988), (81, 0.0), (82, 0.0007905102), (83, 0.008023579), (84, 0.0), (85, 0.0), (86, 0.0), (87, 0.0), (88, 0.0031944318), (89, 0.0), (90, 0.0), (91, 0.0), (92, 0.0), (93, 0.070076264), (94, 0.0006614264), (95, 0.0), (96, 0.0), (97, 0.0), (98, 0.0), (99, 0.0), (100, 0.00061758776), (101, 0.0), (102, 0.0), (103, 0.0), (104, 0.0), (105, 0.0), (106, 0.0), (107, 0.0), (108, 0.0), (109, 0.0), (110, 0.0), (111, 0.0), (112, 0.0), (113, 0.0), (114, 0.6414491), (115, 0.0), (116, 0.0), (117, 0.0), (118, 0.0), (119, 0.0), (120, 0.008153961), (121, 0.0), (122, 0.0), (123, 0.0), (124, 0.0), (125, 0.0), (126, 0.0), (127, 0.0), (128, 0.0), (129, 0.0011598936), (130, 0.020361701), (131, 0.0), (132, 0.0), (133, 0.0), (134, 0.0), (135, 0.0), (136, 0.037306648), (137, 0.0), (138, 0.0009448675), (139, 0.0), (140, 0.0), (141, 0.0), (142, 0.0), (143, 0.0), (144, 0.0), (145, 0.0), (146, 0.030020213), (147, 0.0), (148, 0.0), (149, 0.0), (150, 0.0034421692), (151, 0.0), (152, 0.0), (153, 0.0), (154, 0.0), (155, 0.0), (156, 0.0), (157, 0.0), (158, 0.0), (159, 0.0), (160, 0.01920466), (161, 0.0), (162, 0.0), (163, 0.0), (164, 0.0), (165, 0.0), (166, 0.0), (167, 0.0), (168, 0.0), (169, 0.0), (170, 0.0), (171, 0.0), (172, 0.0), (173, 0.0), (174, 0.0), (175, 0.0), (176, 0.0), (177, 0.0), (178, 0.0), (179, 0.0), (180, 0.002117438), (181, 0.0), (182, 0.0), (183, 0.0), (184, 0.0), (185, 0.0), (186, 0.0), (187, 0.0), (188, 0.0), (189, 0.0), (190, 0.030247318), (191, 0.0), (192, 0.0), (193, 0.0), (194, 0.0), (195, 0.0), (196, 0.0), (197, 0.0), (198, 0.0), (199, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "sims = corpus_index[test_vec_lda]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score:  1.0 \n",
      " Document ID: 0 \n",
      " Title: Alan Smithee \n",
      " ------------------------------------\n",
      "Similarity Score:  0.776972 \n",
      " Document ID: 60 \n",
      " Title: Anthropologie \n",
      " ------------------------------------\n",
      "2 cases of possible plagiarism detected.\n"
     ]
    }
   ],
   "source": [
    "hits = 0\n",
    "for ids in list(enumerate(sims)):\n",
    "    if ids[1] >= 0.75:\n",
    "        hits += 1\n",
    "        title = title_ids.get(ids[0])\n",
    "        print(\"Similarity Score: \",ids[1],\"\\n\",\"Document ID:\",ids[0],\"\\n\",\"Title:\", title,\"\\n\", \"------------------------------------\")\n",
    "print(hits, \"cases of possible plagiarism detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
